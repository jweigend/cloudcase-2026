#cloud-config
#
# Spark Master Installation (node1)
# Idempotent: Download nur wenn nicht vorhanden
#

write_files:
  # spark-env.sh
  - path: /opt/spark/conf/spark-env.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
      export SPARK_HOME=/opt/spark
      export SPARK_LOCAL_DIRS=/data/spark
      export SPARK_DAEMON_MEMORY=2g
      
      # JMX f√ºr Monitoring
      export SPARK_DAEMON_JAVA_OPTS="-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9405 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false"

  # spark-defaults.conf
  - path: /opt/spark/conf/spark-defaults.conf
    content: |
      spark.master                     spark://node1.cloud.local:7077
      spark.local.dir                  /data/spark
      spark.executor.memory            8g
      spark.executor.cores             2
      spark.driver.memory              2g
      spark.serializer                 org.apache.spark.serializer.KryoSerializer
      spark.sql.shuffle.partitions     12

  # Systemd Service
  - path: /etc/systemd/system/spark-master.service
    content: |
      [Unit]
      Description=Apache Spark Master
      After=network.target
      
      [Service]
      Type=forking
      User=cloudadmin
      Environment="SPARK_HOME=/opt/spark"
      ExecStart=/opt/spark/sbin/start-master.sh
      ExecStop=/opt/spark/sbin/stop-master.sh
      Restart=on-failure
      RestartSec=10
      
      [Install]
      WantedBy=multi-user.target

runcmd:
  # Verzeichnisse erstellen (idempotent)
  - mkdir -p /opt/spark/conf /data/spark
  - chown -R cloudadmin:cloudadmin /data/spark
  
  # Spark installieren (nur wenn nicht vorhanden)
  - |
    if [ ! -f /opt/spark/bin/spark-submit ]; then
      SPARK_VERSION="3.5.0"
      cd /tmp
      wget -q "https://downloads.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz"
      tar -xzf spark-${SPARK_VERSION}-bin-hadoop3.tgz
      cp -r spark-${SPARK_VERSION}-bin-hadoop3/* /opt/spark/
      chown -R cloudadmin:cloudadmin /opt/spark
      rm -rf spark-${SPARK_VERSION}-bin-hadoop3*
    fi
  
  # Service aktivieren (idempotent)
  - systemctl daemon-reload
  - systemctl enable spark-master
