---
# Spark Cluster Installation
#
# Verwendung:
#   ansible-playbook -i inventory.yml spark.yml
#

- name: Install Spark on all nodes
  hosts: spark
  vars:
    spark_home: /opt/spark
    spark_user: cloudadmin
    spark_master_host: node1.cloud.local
    spark_master_port: 7077
    spark_worker_memory: 24g
    spark_worker_cores: 6
    
  tasks:
    - name: Create directories
      file:
        path: "{{ item }}"
        state: directory
        owner: "{{ spark_user }}"
        group: "{{ spark_user }}"
        mode: '0755'
      loop:
        - "{{ spark_home }}"
        - /data/spark
        - /data/spark/logs
        - /data/spark/work

    - name: Check if Spark is already installed
      stat:
        path: "{{ spark_home }}/bin/spark-submit"
      register: spark_installed

    - name: Download Spark
      get_url:
        url: "https://dlcdn.apache.org/spark/spark-{{ spark_version }}/spark-{{ spark_version }}-bin-hadoop3-scala2.13.tgz"
        dest: /tmp/spark.tgz
        mode: '0644'
        timeout: 300
      when: not spark_installed.stat.exists

    - name: Extract Spark
      unarchive:
        src: /tmp/spark.tgz
        dest: /tmp
        remote_src: yes
      when: not spark_installed.stat.exists

    - name: Install Spark
      shell: |
        cp -r /tmp/spark-{{ spark_version }}-bin-hadoop3-scala2.13/* {{ spark_home }}/
        chown -R {{ spark_user }}:{{ spark_user }} {{ spark_home }}
      when: not spark_installed.stat.exists

    - name: Clean up download
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - /tmp/spark.tgz
        - /tmp/spark-{{ spark_version }}-bin-hadoop3-scala2.13
      when: not spark_installed.stat.exists

    - name: Configure spark-env.sh
      copy:
        dest: "{{ spark_home }}/conf/spark-env.sh"
        owner: "{{ spark_user }}"
        group: "{{ spark_user }}"
        mode: '0755'
        content: |
          #!/bin/bash
          
          export JAVA_HOME="{{ java_home }}"
          export SPARK_HOME="{{ spark_home }}"
          export SPARK_LOG_DIR=/data/spark/logs
          export SPARK_WORKER_DIR=/data/spark/work
          
          export SPARK_MASTER_HOST={{ spark_master_host }}
          export SPARK_MASTER_PORT={{ spark_master_port }}
          
          export SPARK_WORKER_MEMORY={{ spark_worker_memory }}
          export SPARK_WORKER_CORES={{ spark_worker_cores }}

    - name: Configure spark-defaults.conf
      copy:
        dest: "{{ spark_home }}/conf/spark-defaults.conf"
        owner: "{{ spark_user }}"
        group: "{{ spark_user }}"
        content: |
          # Cluster Connection
          spark.master                     spark://{{ spark_master_host }}:{{ spark_master_port }}
          
          # Executor Configuration (optimiert für 4 Worker mit je 6 Cores, 24 GB)
          # 3 Executors pro Worker = 12 Executors total
          # Jeder Executor: 6 GB RAM, 2 Cores
          spark.executor.memory            6g
          spark.executor.cores             2
          spark.executor.memoryOverhead    1g
          
          # Driver Configuration
          spark.driver.memory              4g
          spark.driver.cores               2
          spark.driver.maxResultSize       2g
          
          # Default Parallelism (2-3x Cores = 48-72)
          spark.default.parallelism        48
          spark.sql.shuffle.partitions     48
          
          # Event Logging (für Spark History Server)
          spark.eventLog.enabled           true
          spark.eventLog.dir               file:///data/spark/logs
          spark.history.fs.logDirectory    file:///data/spark/logs
          
          # Serialization (bessere Performance)
          spark.serializer                 org.apache.spark.serializer.KryoSerializer
          spark.kryoserializer.buffer.max  512m
          
          # SQL/DataFrame Optimizations
          spark.sql.adaptive.enabled       true
          spark.sql.adaptive.coalescePartitions.enabled   true
          
          # Memory Management
          spark.memory.fraction            0.6
          spark.memory.storageFraction     0.5
          
          # Network/Shuffle
          spark.network.timeout            300s
          spark.rpc.askTimeout             300s

- name: Configure Spark Master
  hosts: spark_master
  vars:
    spark_home: /opt/spark
    spark_user: cloudadmin
    
  tasks:
    - name: Create Spark Master systemd service
      copy:
        dest: /etc/systemd/system/spark-master.service
        content: |
          [Unit]
          Description=Apache Spark Master
          After=network.target
          
          [Service]
          Type=forking
          User={{ spark_user }}
          Environment="JAVA_HOME={{ java_home }}"
          Environment="SPARK_HOME={{ spark_home }}"
          Environment="SPARK_MASTER_WEBUI_PORT=8081"
          ExecStart={{ spark_home }}/sbin/start-master.sh
          ExecStop={{ spark_home }}/sbin/stop-master.sh
          Restart=on-failure
          RestartSec=10
          
          [Install]
          WantedBy=multi-user.target

    - name: Reload systemd
      systemd:
        daemon_reload: yes

    - name: Enable and start Spark Master
      systemd:
        name: spark-master
        enabled: yes
        state: started

    - name: Wait for Spark Master to start
      wait_for:
        host: "{{ ansible_host }}"
        port: 7077
        timeout: 30

    - name: Show Spark Master status
      debug:
        msg: "Spark Master running on {{ inventory_hostname }}:7077 (UI: http://{{ inventory_hostname }}:8081)"

- name: Configure Spark Workers
  hosts: spark_workers
  vars:
    spark_home: /opt/spark
    spark_user: cloudadmin
    spark_master_host: node1.cloud.local
    
  tasks:
    - name: Create Spark Worker systemd service
      copy:
        dest: /etc/systemd/system/spark-worker.service
        content: |
          [Unit]
          Description=Apache Spark Worker
          After=network.target spark-master.service
          Wants=spark-master.service
          
          [Service]
          Type=forking
          User={{ spark_user }}
          Environment="JAVA_HOME={{ java_home }}"
          Environment="SPARK_HOME={{ spark_home }}"
          Environment="SPARK_WORKER_WEBUI_PORT=8082"
          ExecStartPre=/bin/sleep 5
          ExecStart={{ spark_home }}/sbin/start-worker.sh spark://{{ spark_master_host }}:7077
          ExecStop={{ spark_home }}/sbin/stop-worker.sh
          Restart=on-failure
          RestartSec=10
          
          [Install]
          WantedBy=multi-user.target

    - name: Reload systemd
      systemd:
        daemon_reload: yes

    - name: Enable and start Spark Worker
      systemd:
        name: spark-worker
        enabled: yes
        state: started

    - name: Wait for Spark Worker to start
      wait_for:
        port: 8082
        timeout: 30

    - name: Show Spark Worker status
      debug:
        msg: "Spark Worker running on {{ inventory_hostname }} (UI: http://{{ inventory_hostname }}:8082)"
