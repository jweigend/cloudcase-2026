{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Drill-Down Architektur: Solr Facetten + Spark\n",
        "\n",
        "Dieses Notebook demonstriert eine **interaktive Analyse-Architektur** mit Drill-Down:\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                  Drill-Down Architecture                             â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                      â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\n",
        "â”‚  â”‚   Parquet    â”‚ â”€â”€â”€â–º â”‚    Spark     â”‚ â”€â”€â”€â–º â”‚    Solr      â”‚       â”‚\n",
        "â”‚  â”‚  (Raw Data)  â”‚      â”‚  (Import)    â”‚      â”‚ (Full Data)  â”‚       â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
        "â”‚                                                      â”‚              â”‚\n",
        "â”‚                                               â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”       â”‚\n",
        "â”‚                                               â”‚             â”‚       â”‚\n",
        "â”‚                                               â–¼             â–¼       â”‚\n",
        "â”‚                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
        "â”‚                                        â”‚ Facetten â”‚  â”‚ Streaming â”‚  â”‚\n",
        "â”‚                                        â”‚Drill-Downâ”‚  â”‚Expressionsâ”‚  â”‚\n",
        "â”‚                                        â”‚  <50ms   â”‚  â”‚  Aggreg.  â”‚  â”‚\n",
        "â”‚                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
        "â”‚                                               â”‚             â”‚       â”‚\n",
        "â”‚                                               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
        "â”‚                                                      â–¼              â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\n",
        "â”‚  â”‚    Spark     â”‚ â—„â”€â”€â”€ â”‚ Solr /export â”‚ â—„â”€â”€â”€ â”‚  Deep-Dive   â”‚       â”‚\n",
        "â”‚  â”‚  (ML/Stats)  â”‚      â”‚  (Streaming) â”‚      â”‚   Request    â”‚       â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
        "â”‚                                                                      â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "## Die 4 SÃ¤ulen dieser Architektur\n",
        "\n",
        "| # | Komponente | Aufgabe | Latenz |\n",
        "|---|------------|---------|--------|\n",
        "| 1 | **Rohdaten in Solr** | Alle Daten indiziert fÃ¼r Suche & Facetten | Einmaliger Import |\n",
        "| 2 | **Facetten-Drill-Down** | Interaktive Exploration Ã¼ber Facetten | <50ms |\n",
        "| 3 | **Streaming Expressions** | Ad-hoc Aggregationen in Solr | 100-500ms |\n",
        "| 4 | **Spark via /export** | Komplexe Analysen auf Teilmengen | Sekunden |\n",
        "\n",
        "**Vorteil:** User kann von Ãœbersicht bis Einzeldatensatz navigieren!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Konfiguration\n",
        "# =============================================================================\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# Spark Cluster\n",
        "SPARK_MASTER = 'spark://node0.cloud.local:7077'\n",
        "\n",
        "# Solr Cloud\n",
        "SOLR_HOST = 'node1'\n",
        "SOLR_PORT = 8983\n",
        "SOLR_NODES = ['node1', 'node2', 'node3', 'node4']  # FÃ¼r Load Balancing\n",
        "ZK_HOSTS = 'node1:2181,node2:2181,node3:2181'\n",
        "\n",
        "# Collection fÃ¼r Rohdaten\n",
        "COLLECTION = 'nyc-taxi-raw'\n",
        "\n",
        "# Daten\n",
        "PARQUET_FILES = [\n",
        "    ('yellow_tripdata_2023-01.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet'),\n",
        "    ('yellow_tripdata_2023-02.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet'),\n",
        "]\n",
        "DATA_DIR = '/data/spark/datasets/nyc-taxi'\n",
        "\n",
        "print('âœ… Konfiguration geladen')\n",
        "print(f'   Spark Master: {SPARK_MASTER}')\n",
        "print(f'   Solr:         http://{SOLR_HOST}:{SOLR_PORT}/solr/{COLLECTION}')\n",
        "print(f'   Solr Nodes:   {SOLR_NODES}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Teil 1: Rohdaten komplett in Solr laden\n",
        "\n",
        "Wir laden alle ~6 Millionen Taxi-Fahrten in Solr.\n",
        "Das ermÃ¶glicht spÃ¤ter Drill-Down auf Einzelfahrt-Ebene."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_collection_with_schema(name, num_shards=4, replication_factor=1):\n",
        "    \"\"\"\n",
        "    Erstellt eine Solr Collection mit optimiertem Schema fÃ¼r Drill-Down.\n",
        "    \"\"\"\n",
        "    base_url = f'http://{SOLR_HOST}:{SOLR_PORT}/solr'\n",
        "    \n",
        "    # 1. Collection erstellen\n",
        "    print(f'ğŸ“¦ Erstelle Collection {name}...')\n",
        "    resp = requests.get(f'{base_url}/admin/collections', params={\n",
        "        'action': 'CREATE',\n",
        "        'name': name,\n",
        "        'numShards': num_shards,\n",
        "        'replicationFactor': replication_factor,\n",
        "        'collection.configName': '_default'\n",
        "    }, timeout=60)\n",
        "    \n",
        "    if 'already exists' in resp.text:\n",
        "        print(f'   â„¹ï¸  Collection existiert bereits')\n",
        "    elif resp.status_code == 200:\n",
        "        print(f'   âœ… Collection erstellt')\n",
        "    else:\n",
        "        print(f'   âš ï¸  {resp.text[:200]}')\n",
        "    \n",
        "    # 2. Schema-Felder fÃ¼r Facetten hinzufÃ¼gen\n",
        "    print(f'ğŸ“ Konfiguriere Schema fÃ¼r Facetten...')\n",
        "    \n",
        "    # Felder die als Facetten dienen (indexed, docValues fÃ¼r schnelle Aggregation)\n",
        "    facet_fields = [\n",
        "        {'name': 'pickup_date', 'type': 'string', 'indexed': True, 'stored': True, 'docValues': True},\n",
        "        {'name': 'pickup_hour', 'type': 'pint', 'indexed': True, 'stored': True, 'docValues': True},\n",
        "        {'name': 'pickup_dayofweek', 'type': 'pint', 'indexed': True, 'stored': True, 'docValues': True},\n",
        "        {'name': 'PULocationID', 'type': 'pint', 'indexed': True, 'stored': True, 'docValues': True},\n",
        "        {'name': 'DOLocationID', 'type': 'pint', 'indexed': True, 'stored': True, 'docValues': True},\n",
        "        {'name': 'payment_type', 'type': 'pint', 'indexed': True, 'stored': True, 'docValues': True},\n",
        "        {'name': 'passenger_count', 'type': 'pint', 'indexed': True, 'stored': True, 'docValues': True},\n",
        "        {'name': 'trip_distance', 'type': 'pdouble', 'indexed': True, 'stored': True, 'docValues': True},\n",
        "        {'name': 'total_amount', 'type': 'pdouble', 'indexed': True, 'stored': True, 'docValues': True},\n",
        "        {'name': 'tip_amount', 'type': 'pdouble', 'indexed': True, 'stored': True, 'docValues': True},\n",
        "        {'name': 'fare_amount', 'type': 'pdouble', 'indexed': True, 'stored': True, 'docValues': True},\n",
        "    ]\n",
        "    \n",
        "    for field in facet_fields:\n",
        "        resp = requests.post(\n",
        "            f'{base_url}/{name}/schema',\n",
        "            json={'add-field': field},\n",
        "            headers={'Content-Type': 'application/json'},\n",
        "            timeout=30\n",
        "        )\n",
        "        if resp.status_code == 200 or 'already exists' in resp.text:\n",
        "            print(f'   âœ… Feld {field[\"name\"]}')\n",
        "        else:\n",
        "            print(f'   âš ï¸  Feld {field[\"name\"]}: {resp.text[:100]}')\n",
        "    \n",
        "    print('âœ… Schema konfiguriert')\n",
        "    return True\n",
        "\n",
        "# Collection erstellen\n",
        "create_collection_with_schema(COLLECTION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parquet-Daten herunterladen (falls nicht vorhanden)\n",
        "import urllib.request\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "for filename, url in PARQUET_FILES:\n",
        "    filepath = os.path.join(DATA_DIR, filename)\n",
        "    if os.path.exists(filepath):\n",
        "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "        print(f'âœ… {filename} existiert ({size_mb:.1f} MB)')\n",
        "    else:\n",
        "        print(f'â¬‡ï¸  Lade {filename}...')\n",
        "        urllib.request.urlretrieve(url, filepath)\n",
        "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "        print(f'âœ… {filename} heruntergeladen ({size_mb:.1f} MB)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "print('ğŸš€ Starte Spark Session...')\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('Drill-Down Architecture - Parallel Import') \\\n",
        "    .master(SPARK_MASTER) \\\n",
        "    .config('spark.executor.cores', '2') \\\n",
        "    .config('spark.executor.memory', '6g') \\\n",
        "    .config('spark.driver.memory', '4g') \\\n",
        "    .config('spark.sql.shuffle.partitions', '48') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f'âœ… Spark Session gestartet')\n",
        "print(f'   Master: {spark.sparkContext.master}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def solr_delete_all(collection):\n",
        "    \"\"\"LÃ¶scht alle Dokumente einer Collection.\"\"\"\n",
        "    url = f'http://{SOLR_HOST}:{SOLR_PORT}/solr/{collection}/update'\n",
        "    resp = requests.post(url, json={'delete': {'query': '*:*'}}, params={'commit': 'true'}, timeout=60)\n",
        "    resp.raise_for_status()\n",
        "    return resp.json()\n",
        "\n",
        "def import_parquet_to_solr_parallel(spark, parquet_path, collection, batch_size=2000):\n",
        "    \"\"\"\n",
        "    Importiert Parquet-Daten PARALLEL via Spark Executors nach Solr.\n",
        "    Jede Partition sendet direkt an Solr - kein Driver-Bottleneck!\n",
        "    \"\"\"\n",
        "    print(f'\\nğŸ“– Lade {parquet_path}...')\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Parquet laden\n",
        "    df = spark.read.parquet(parquet_path)\n",
        "    \n",
        "    # Spalten normalisieren\n",
        "    if 'Airport_fee' in df.columns:\n",
        "        df = df.withColumnRenamed('Airport_fee', 'airport_fee')\n",
        "    \n",
        "    # Datum-Spalten zu Strings konvertieren (fÃ¼r JSON-Serialisierung)\n",
        "    # Dies muss VOR foreachPartition passieren!\n",
        "    df = df.withColumn('pickup_date', F.date_format('tpep_pickup_datetime', 'yyyy-MM-dd')) \\\n",
        "           .withColumn('pickup_hour', F.hour('tpep_pickup_datetime')) \\\n",
        "           .withColumn('pickup_dayofweek', F.dayofweek('tpep_pickup_datetime')) \\\n",
        "           .withColumn('tpep_pickup_datetime', F.date_format('tpep_pickup_datetime', \"yyyy-MM-dd'T'HH:mm:ss'Z'\")) \\\n",
        "           .withColumn('tpep_dropoff_datetime', F.date_format('tpep_dropoff_datetime', \"yyyy-MM-dd'T'HH:mm:ss'Z'\")) \\\n",
        "           .withColumn('id', F.expr('uuid()'))\n",
        "    \n",
        "    # Repartition fÃ¼r optimale ParallelitÃ¤t (4 Solr Nodes = mind. 8-16 Partitions)\n",
        "    num_partitions = 16\n",
        "    df = df.repartition(num_partitions)\n",
        "    \n",
        "    total_rows = df.count()\n",
        "    print(f'   {total_rows:,} Zeilen geladen in {time.time() - start_time:.1f}s')\n",
        "    print(f'   Partitionen: {num_partitions}')\n",
        "    \n",
        "    print(f'ğŸ“¤ Paralleler Push nach Solr {collection}...')\n",
        "    start_push = time.time()\n",
        "    \n",
        "    # Broadcast der Konfiguration zu allen Executors\n",
        "    solr_nodes_bc = spark.sparkContext.broadcast(SOLR_NODES)\n",
        "    solr_port_bc = spark.sparkContext.broadcast(SOLR_PORT)\n",
        "    collection_bc = spark.sparkContext.broadcast(collection)\n",
        "    batch_size_bc = spark.sparkContext.broadcast(batch_size)\n",
        "    \n",
        "    def send_partition_to_solr(partition_index, iterator):\n",
        "        \"\"\"Wird auf jedem Executor ausgefÃ¼hrt - sendet Partition an Solr.\"\"\"\n",
        "        import requests\n",
        "        \n",
        "        # Round-Robin Ã¼ber Solr Nodes basierend auf Partition-Index\n",
        "        nodes = solr_nodes_bc.value\n",
        "        node = nodes[partition_index % len(nodes)]\n",
        "        port = solr_port_bc.value\n",
        "        coll = collection_bc.value\n",
        "        batch_sz = batch_size_bc.value\n",
        "        \n",
        "        url = f'http://{node}:{port}/solr/{coll}/update'\n",
        "        \n",
        "        batch = []\n",
        "        count = 0\n",
        "        \n",
        "        for row in iterator:\n",
        "            # Row zu Dict konvertieren, None fÃ¼r NaN\n",
        "            doc = row.asDict()\n",
        "            # NaN/None handling\n",
        "            doc = {k: (None if v != v else v) for k, v in doc.items()}  # NaN check: v != v\n",
        "            batch.append(doc)\n",
        "            \n",
        "            if len(batch) >= batch_sz:\n",
        "                resp = requests.post(url, json=batch, timeout=60)\n",
        "                resp.raise_for_status()\n",
        "                count += len(batch)\n",
        "                batch = []\n",
        "        \n",
        "        # Rest senden\n",
        "        if batch:\n",
        "            resp = requests.post(url, json=batch, timeout=60)\n",
        "            resp.raise_for_status()\n",
        "            count += len(batch)\n",
        "        \n",
        "        yield (partition_index, node, count)\n",
        "    \n",
        "    # Parallel auf allen Executors ausfÃ¼hren\n",
        "    results = df.rdd.mapPartitionsWithIndex(send_partition_to_solr).collect()\n",
        "    \n",
        "    # Commit\n",
        "    commit_url = f'http://{SOLR_HOST}:{SOLR_PORT}/solr/{collection}/update?commit=true'\n",
        "    requests.post(commit_url, json=[], timeout=30)\n",
        "    \n",
        "    elapsed = time.time() - start_push\n",
        "    total_sent = sum(r[2] for r in results)\n",
        "    \n",
        "    print(f'   Partitions-Statistik:')\n",
        "    for part_id, node, count in sorted(results):\n",
        "        print(f'      Partition {part_id:2d} â†’ {node}: {count:,} docs')\n",
        "    \n",
        "    print(f'   âœ… {total_sent:,} Dokumente in {elapsed:.1f}s ({total_sent/elapsed:,.0f} docs/s)')\n",
        "    \n",
        "    return total_sent\n",
        "\n",
        "print('âœ… Import-Funktionen definiert')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Daten importieren\n",
        "print('=' * 70)\n",
        "print('ğŸ“¦ PARALLELER ROHDATEN-IMPORT NACH SOLR')\n",
        "print('=' * 70)\n",
        "\n",
        "# Alte Daten lÃ¶schen\n",
        "print('ğŸ—‘ï¸  LÃ¶sche alte Daten...')\n",
        "solr_delete_all(COLLECTION)\n",
        "\n",
        "total_imported = 0\n",
        "for filename, _ in PARQUET_FILES:\n",
        "    filepath = os.path.join(DATA_DIR, filename)\n",
        "    count = import_parquet_to_solr_parallel(spark, filepath, COLLECTION)\n",
        "    total_imported += count\n",
        "\n",
        "print(f'\\n{\"=\" * 70}')\n",
        "print(f'âœ… IMPORT ABGESCHLOSSEN: {total_imported:,} Dokumente in Solr')\n",
        "print(f'{\"=\" * 70}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Teil 2: Facetten-basierter Drill-Down\n",
        "\n",
        "Jetzt demonstrieren wir interaktiven Drill-Down Ã¼ber Solr Facetten.\n",
        "\n",
        "**Use Case:** User startet mit Ãœbersicht â†’ klickt auf interessanten Wert â†’ sieht Details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "class DrillDownExplorer:\n",
        "    \"\"\"\n",
        "    Interaktiver Drill-Down Explorer mit Solr Facetten.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, collection, solr_host=SOLR_HOST, solr_port=SOLR_PORT):\n",
        "        self.base_url = f'http://{solr_host}:{solr_port}/solr/{collection}'\n",
        "        self.current_filters = []\n",
        "        \n",
        "    def query_with_facets(self, q='*:*', facet_fields=None, fq=None, rows=0):\n",
        "        \"\"\"\n",
        "        FÃ¼hrt Query mit Facetten aus.\n",
        "        \"\"\"\n",
        "        if facet_fields is None:\n",
        "            facet_fields = ['pickup_hour', 'pickup_dayofweek', 'PULocationID', 'payment_type']\n",
        "        \n",
        "        params = {\n",
        "            'q': q,\n",
        "            'rows': rows,\n",
        "            'facet': 'true',\n",
        "            'facet.field': facet_fields,\n",
        "            'facet.limit': 20,\n",
        "            'facet.mincount': 1,\n",
        "            'wt': 'json'\n",
        "        }\n",
        "        \n",
        "        if fq:\n",
        "            params['fq'] = fq\n",
        "        \n",
        "        start = time.time()\n",
        "        resp = requests.get(f'{self.base_url}/select', params=params, timeout=30)\n",
        "        elapsed_ms = (time.time() - start) * 1000\n",
        "        \n",
        "        result = resp.json()\n",
        "        \n",
        "        return {\n",
        "            'numFound': result['response']['numFound'],\n",
        "            'facets': result.get('facet_counts', {}).get('facet_fields', {}),\n",
        "            'qtime_ms': elapsed_ms,\n",
        "            'docs': result['response'].get('docs', [])\n",
        "        }\n",
        "    \n",
        "    def show_facet_distribution(self, field, fq=None, title=None):\n",
        "        \"\"\"\n",
        "        Zeigt Verteilung eines Facetten-Feldes.\n",
        "        \"\"\"\n",
        "        result = self.query_with_facets(facet_fields=[field], fq=fq)\n",
        "        \n",
        "        # Facetten-Daten parsen (Solr gibt [value, count, value, count, ...] zurÃ¼ck)\n",
        "        facet_data = result['facets'].get(field, [])\n",
        "        pairs = [(facet_data[i], facet_data[i+1]) for i in range(0, len(facet_data), 2)]\n",
        "        \n",
        "        df = pd.DataFrame(pairs, columns=[field, 'count'])\n",
        "        \n",
        "        print(f'\\n{\"â”€\" * 50}')\n",
        "        print(f'ğŸ“Š {title or field.upper()}')\n",
        "        print(f'   Gefilterte Dokumente: {result[\"numFound\"]:,}')\n",
        "        print(f'   Query-Zeit: {result[\"qtime_ms\"]:.0f}ms')\n",
        "        if fq:\n",
        "            print(f'   Filter: {fq}')\n",
        "        print(f'{\"â”€\" * 50}')\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def drill_down(self, fq, fields=None, limit=10):\n",
        "        \"\"\"\n",
        "        Drill-Down: Zeigt Details fÃ¼r einen Filter.\n",
        "        \"\"\"\n",
        "        if fields is None:\n",
        "            fields = '*'\n",
        "        \n",
        "        params = {\n",
        "            'q': '*:*',\n",
        "            'fq': fq,\n",
        "            'rows': limit,\n",
        "            'fl': fields,\n",
        "            'wt': 'json'\n",
        "        }\n",
        "        \n",
        "        start = time.time()\n",
        "        resp = requests.get(f'{self.base_url}/select', params=params, timeout=30)\n",
        "        elapsed_ms = (time.time() - start) * 1000\n",
        "        \n",
        "        result = resp.json()\n",
        "        \n",
        "        print(f'\\n{\"â”€\" * 50}')\n",
        "        print(f'ğŸ” DRILL-DOWN: {fq}')\n",
        "        print(f'   Treffer: {result[\"response\"][\"numFound\"]:,}')\n",
        "        print(f'   Query-Zeit: {elapsed_ms:.0f}ms')\n",
        "        print(f'{\"â”€\" * 50}')\n",
        "        \n",
        "        return pd.DataFrame(result['response']['docs'])\n",
        "\n",
        "# Explorer initialisieren\n",
        "explorer = DrillDownExplorer(COLLECTION)\n",
        "print('âœ… Drill-Down Explorer initialisiert')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Level 1: Ãœbersicht - Verteilung nach Stunde\n",
        "# ============================================================\n",
        "print('=' * 70)\n",
        "print('ğŸ¯ DRILL-DOWN DEMO: Von Ãœbersicht zu Einzelfahrt')\n",
        "print('=' * 70)\n",
        "\n",
        "df_hours = explorer.show_facet_distribution('pickup_hour', title='Fahrten pro Stunde')\n",
        "display(df_hours.head(10))\n",
        "\n",
        "# Visualisierung\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.bar(df_hours['pickup_hour'].astype(int), df_hours['count'])\n",
        "plt.xlabel('Stunde')\n",
        "plt.ylabel('Anzahl Fahrten')\n",
        "plt.title('Taxi-Fahrten pro Stunde (alle Daten)')\n",
        "plt.xticks(range(0, 24))\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\nğŸ‘† Beobachtung: Stunde 18 (18:00 Uhr) hat viele Fahrten. Drill-Down!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Level 2: Drill-Down auf Stunde 18 - Welche Zonen?\n",
        "# ============================================================\n",
        "\n",
        "df_zones = explorer.show_facet_distribution(\n",
        "    'PULocationID', \n",
        "    fq='pickup_hour:18',\n",
        "    title='Top Pickup-Zonen um 18:00 Uhr'\n",
        ")\n",
        "display(df_zones.head(10))\n",
        "\n",
        "print('\\nğŸ‘† Zone 237 und 236 sind Hotspots um 18 Uhr. Weiter Drill-Down!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Level 3: Drill-Down auf Zone 237 um 18 Uhr - Zahlungsarten?\n",
        "# ============================================================\n",
        "\n",
        "df_payment = explorer.show_facet_distribution(\n",
        "    'payment_type',\n",
        "    fq=['pickup_hour:18', 'PULocationID:237'],\n",
        "    title='Zahlungsarten in Zone 237 um 18:00'\n",
        ")\n",
        "\n",
        "# Payment Type Mapping\n",
        "payment_map = {1: 'Kreditkarte', 2: 'Bargeld', 3: 'Keine Zahlung', 4: 'Streit'}\n",
        "df_payment['payment_name'] = df_payment['payment_type'].astype(int).map(payment_map)\n",
        "display(df_payment)\n",
        "\n",
        "print('\\nğŸ‘† Interessant: Mehr Kreditkarte als Bargeld. Einzelfahrten ansehen!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Level 4: Drill-Down auf Einzelfahrten\n",
        "# ============================================================\n",
        "\n",
        "df_trips = explorer.drill_down(\n",
        "    fq=['pickup_hour:18', 'PULocationID:237', 'payment_type:1'],\n",
        "    fields='id,tpep_pickup_datetime,PULocationID,DOLocationID,trip_distance,total_amount,tip_amount,passenger_count',\n",
        "    limit=15\n",
        ")\n",
        "\n",
        "display(df_trips)\n",
        "\n",
        "print('\\nâœ… Drill-Down komplett: Von 6M Fahrten â†’ spezifische Einzelfahrten')\n",
        "print('   Jede Query < 50ms!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Teil 3: Solr Streaming Expressions fÃ¼r Ad-hoc Aggregationen\n",
        "\n",
        "Streaming Expressions sind Solrs eingebaute Aggregations-Engine.\n",
        "Sie ermÃ¶glichen komplexe Aggregationen **ohne Spark** - direkt in Solr."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_streaming_expression(expr, collection=COLLECTION):\n",
        "    \"\"\"\n",
        "    FÃ¼hrt eine Solr Streaming Expression aus.\n",
        "    \"\"\"\n",
        "    url = f'http://{SOLR_HOST}:{SOLR_PORT}/solr/{collection}/stream'\n",
        "    \n",
        "    start = time.time()\n",
        "    resp = requests.post(\n",
        "        url,\n",
        "        data={'expr': expr},\n",
        "        headers={'Content-Type': 'application/x-www-form-urlencoded'},\n",
        "        timeout=60\n",
        "    )\n",
        "    elapsed_ms = (time.time() - start) * 1000\n",
        "    \n",
        "    result = resp.json()\n",
        "    \n",
        "    # Ergebnis extrahieren (Stream-Format)\n",
        "    docs = []\n",
        "    if 'result-set' in result:\n",
        "        for item in result['result-set'].get('docs', []):\n",
        "            if 'EOF' not in item:  # End-of-stream marker ignorieren\n",
        "                docs.append(item)\n",
        "    \n",
        "    print(f'   â±ï¸  Query-Zeit: {elapsed_ms:.0f}ms')\n",
        "    return docs\n",
        "\n",
        "print('âœ… Streaming Expression Helper geladen')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Streaming Expression 1: Statistiken nach Stunde\n",
        "# ============================================================\n",
        "print('\\n' + '=' * 70)\n",
        "print('ğŸ“Š STREAMING EXPRESSION: Umsatz-Statistik pro Stunde')\n",
        "print('=' * 70)\n",
        "\n",
        "expr = f'''\n",
        "rollup(\n",
        "  search({COLLECTION},\n",
        "    q=\"*:*\",\n",
        "    fl=\"pickup_hour,total_amount,tip_amount\",\n",
        "    sort=\"pickup_hour asc\",\n",
        "    qt=\"/export\"\n",
        "  ),\n",
        "  over=\"pickup_hour\",\n",
        "  sum(total_amount),\n",
        "  avg(total_amount),\n",
        "  avg(tip_amount),\n",
        "  count(*)\n",
        ")\n",
        "'''\n",
        "\n",
        "results = execute_streaming_expression(expr)\n",
        "df_stats = pd.DataFrame(results)\n",
        "display(df_stats.head(15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Streaming Expression 2: Top Routen nach Umsatz\n",
        "# ============================================================\n",
        "print('\\n' + '=' * 70)\n",
        "print('ğŸ“Š STREAMING EXPRESSION: Top 20 Routen nach Umsatz')\n",
        "print('=' * 70)\n",
        "\n",
        "expr = f'''\n",
        "top(\n",
        "  n=20,\n",
        "  rollup(\n",
        "    search({COLLECTION},\n",
        "      q=\"*:*\",\n",
        "      fl=\"PULocationID,DOLocationID,total_amount\",\n",
        "      sort=\"PULocationID asc\",\n",
        "      qt=\"/export\"\n",
        "    ),\n",
        "    over=\"PULocationID,DOLocationID\",\n",
        "    sum(total_amount),\n",
        "    count(*)\n",
        "  ),\n",
        "  sort=\"sum(total_amount) desc\"\n",
        ")\n",
        "'''\n",
        "\n",
        "results = execute_streaming_expression(expr)\n",
        "df_routes = pd.DataFrame(results)\n",
        "df_routes.columns = [c.replace('(', '_').replace(')', '').replace('*', 'all') for c in df_routes.columns]\n",
        "display(df_routes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Streaming Expression 3: Statistiken mit Filter\n",
        "# ============================================================\n",
        "print('\\n' + '=' * 70)\n",
        "print('ğŸ“Š STREAMING EXPRESSION: Wochenend-Nachtfahrten (Fr/Sa 22:00-04:00)')\n",
        "print('=' * 70)\n",
        "\n",
        "# Freitag = 6, Samstag = 7 in Spark's dayofweek\n",
        "expr = f'''\n",
        "rollup(\n",
        "  search({COLLECTION},\n",
        "    q=\"pickup_dayofweek:(6 OR 7) AND (pickup_hour:[22 TO 23] OR pickup_hour:[0 TO 4])\",\n",
        "    fl=\"pickup_hour,total_amount,tip_amount,trip_distance\",\n",
        "    sort=\"pickup_hour asc\",\n",
        "    qt=\"/export\"\n",
        "  ),\n",
        "  over=\"pickup_hour\",\n",
        "  sum(total_amount),\n",
        "  avg(total_amount),\n",
        "  avg(tip_amount),\n",
        "  avg(trip_distance),\n",
        "  count(*)\n",
        ")\n",
        "'''\n",
        "\n",
        "results = execute_streaming_expression(expr)\n",
        "if results:\n",
        "    df_night = pd.DataFrame(results)\n",
        "    df_night.columns = [c.replace('(', '_').replace(')', '').replace('*', 'all') for c in df_night.columns]\n",
        "    display(df_night)\n",
        "else:\n",
        "    print('Keine Ergebnisse - Filter anpassen?')\n",
        "\n",
        "print('\\nâœ… Streaming Expressions: Aggregationen ohne Spark in 100-500ms!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Teil 4: Spark fÃ¼r komplexe Analysen via Solr /export\n",
        "\n",
        "Wenn Streaming Expressions nicht ausreichen (ML, komplexe Statistik),\n",
        "kann Spark gefilterte Daten Ã¼ber den `/export` Handler laden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def solr_to_spark_via_export(spark, query, fields, collection=COLLECTION, max_rows=100000):\n",
        "    \"\"\"\n",
        "    LÃ¤dt gefilterte Daten aus Solr nach Spark via /export Handler.\n",
        "    \n",
        "    Der /export Handler streamt alle Ergebnisse effizient.\n",
        "    \"\"\"\n",
        "    print(f'\\nğŸ“¥ Lade Daten von Solr nach Spark...')\n",
        "    print(f'   Query: {query}')\n",
        "    print(f'   Felder: {fields}')\n",
        "    \n",
        "    start = time.time()\n",
        "    \n",
        "    # Zuerst: Wie viele Dokumente?\n",
        "    count_url = f'http://{SOLR_HOST}:{SOLR_PORT}/solr/{collection}/select'\n",
        "    count_resp = requests.get(count_url, params={'q': query, 'rows': 0, 'wt': 'json'})\n",
        "    total_docs = count_resp.json()['response']['numFound']\n",
        "    print(f'   Gefundene Dokumente: {total_docs:,}')\n",
        "    \n",
        "    if total_docs > max_rows:\n",
        "        print(f'   âš ï¸  Limitiere auf {max_rows:,} Zeilen')\n",
        "    \n",
        "    # Export URL bauen (sort ist Pflicht fÃ¼r /export)\n",
        "    export_url = f'http://{SOLR_HOST}:{SOLR_PORT}/solr/{collection}/export'\n",
        "    params = {\n",
        "        'q': query,\n",
        "        'fl': fields,\n",
        "        'sort': fields.split(',')[0] + ' asc',\n",
        "        'wt': 'json'\n",
        "    }\n",
        "    \n",
        "    # Daten abrufen\n",
        "    resp = requests.get(export_url, params=params, timeout=120, stream=True)\n",
        "    result = resp.json()\n",
        "    \n",
        "    docs = result.get('response', {}).get('docs', [])\n",
        "    if max_rows and len(docs) > max_rows:\n",
        "        docs = docs[:max_rows]\n",
        "    \n",
        "    fetch_time = time.time() - start\n",
        "    print(f'   Geladen: {len(docs):,} Dokumente in {fetch_time:.1f}s')\n",
        "    \n",
        "    # Zu Spark DataFrame\n",
        "    if docs:\n",
        "        pdf = pd.DataFrame(docs)\n",
        "        sdf = spark.createDataFrame(pdf)\n",
        "        print(f'   âœ… Spark DataFrame erstellt: {sdf.count():,} Zeilen')\n",
        "        return sdf\n",
        "    else:\n",
        "        print('   âš ï¸  Keine Daten gefunden')\n",
        "        return None\n",
        "\n",
        "print('âœ… Solr-to-Spark Export Helper geladen')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Beispiel: ML-Analyse auf gefilterten Daten\n",
        "# ============================================================\n",
        "print('=' * 70)\n",
        "print('ğŸ§  SPARK ANALYSE: Tipp-Verhalten nach Distanz (Zone 237)')\n",
        "print('=' * 70)\n",
        "\n",
        "# Gefilterte Daten aus Solr laden\n",
        "df_analysis = solr_to_spark_via_export(\n",
        "    spark,\n",
        "    query='PULocationID:237 AND trip_distance:[1 TO 20]',\n",
        "    fields='trip_distance,total_amount,tip_amount,fare_amount,passenger_count',\n",
        "    max_rows=50000\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if df_analysis:\n",
        "    # Spark ML: Korrelationsanalyse\n",
        "    from pyspark.ml.stat import Correlation\n",
        "    from pyspark.ml.feature import VectorAssembler\n",
        "    \n",
        "    # Numerische Spalten zu Vector\n",
        "    numeric_cols = ['trip_distance', 'total_amount', 'tip_amount', 'fare_amount']\n",
        "    \n",
        "    # NaN/None entfernen\n",
        "    df_clean = df_analysis.dropna(subset=numeric_cols)\n",
        "    \n",
        "    assembler = VectorAssembler(inputCols=numeric_cols, outputCol='features')\n",
        "    df_vector = assembler.transform(df_clean)\n",
        "    \n",
        "    # Korrelationsmatrix\n",
        "    corr_matrix = Correlation.corr(df_vector, 'features').head()[0]\n",
        "    \n",
        "    print('\\nğŸ“Š Korrelationsmatrix:')\n",
        "    corr_df = pd.DataFrame(\n",
        "        corr_matrix.toArray(),\n",
        "        columns=numeric_cols,\n",
        "        index=numeric_cols\n",
        "    )\n",
        "    display(corr_df.round(3))\n",
        "    \n",
        "    # Statistiken\n",
        "    print('\\nğŸ“Š Deskriptive Statistik:')\n",
        "    df_analysis.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if df_analysis:\n",
        "    # Tipp-Rate Analyse\n",
        "    df_tip = df_analysis.withColumn(\n",
        "        'tip_rate', \n",
        "        F.when(F.col('fare_amount') > 0, \n",
        "               F.col('tip_amount') / F.col('fare_amount') * 100\n",
        "        ).otherwise(0)\n",
        "    )\n",
        "    \n",
        "    # Distanz-Buckets\n",
        "    df_buckets = df_tip.withColumn(\n",
        "        'distance_bucket',\n",
        "        F.when(F.col('trip_distance') < 2, '0-2 mi')\n",
        "         .when(F.col('trip_distance') < 5, '2-5 mi')\n",
        "         .when(F.col('trip_distance') < 10, '5-10 mi')\n",
        "         .otherwise('10+ mi')\n",
        "    )\n",
        "    \n",
        "    # Aggregation\n",
        "    print('\\nğŸ“Š Tipp-Rate nach Distanz:')\n",
        "    df_buckets.groupBy('distance_bucket').agg(\n",
        "        F.count('*').alias('fahrten'),\n",
        "        F.avg('tip_rate').alias('avg_tip_rate'),\n",
        "        F.avg('tip_amount').alias('avg_tip'),\n",
        "        F.avg('total_amount').alias('avg_total')\n",
        "    ).orderBy('distance_bucket').show()\n",
        "    \n",
        "    print('\\nâœ… Komplexe Spark-Analyse auf Solr-gefilterten Daten!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Zusammenfassung: Drill-Down Architektur\n",
        "\n",
        "| Komponente | Einsatz | Latenz | Beispiel |\n",
        "|------------|---------|--------|----------|\n",
        "| **Solr Facetten** | Interaktive Navigation, Drill-Down | <50ms | Stunde â†’ Zone â†’ Einzelfahrt |\n",
        "| **Streaming Expressions** | Ad-hoc Aggregationen | 100-500ms | Umsatz pro Stunde, Top-Routen |\n",
        "| **Spark via /export** | ML, komplexe Statistik | Sekunden | Korrelationen, Clustering |\n",
        "\n",
        "**Vorteile dieser Architektur:**\n",
        "- âœ… VollstÃ¤ndiger Drill-Down bis zur Einzelfahrt\n",
        "- âœ… Keine Pre-Aggregation nÃ¶tig\n",
        "- âœ… Facetten fÃ¼r intuitive Navigation\n",
        "- âœ… Spark nur wenn wirklich nÃ¶tig\n",
        "- âœ… Kein Spark-Solr Connector notwendig\n",
        "\n",
        "**Trade-offs:**\n",
        "- âš ï¸ Mehr Speicher in Solr (Rohdaten statt Aggregate)\n",
        "- âš ï¸ Import dauert lÃ¤nger (6M statt 1000 Dokumente)\n",
        "- âš ï¸ Streaming Expressions haben Lernkurve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AufrÃ¤umen\n",
        "spark.stop()\n",
        "print('\\nâœ… Spark Session beendet')\n",
        "print('\\nğŸ¯ Notebook abgeschlossen!')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
