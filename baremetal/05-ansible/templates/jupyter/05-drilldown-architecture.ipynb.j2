{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Drill-Down Architektur: Solr Facetten + Spark\n",
        "\n",
        "Dieses Notebook demonstriert eine **interaktive Analyse-Architektur** mit Drill-Down:\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                  Drill-Down Architecture                             â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                      â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\n",
        "â”‚  â”‚   Parquet    â”‚ â”€â”€â”€â–º â”‚    Spark     â”‚ â”€â”€â”€â–º â”‚    Solr      â”‚       â”‚\n",
        "â”‚  â”‚  (Raw Data)  â”‚      â”‚  (Import)    â”‚      â”‚ (Full Data)  â”‚       â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
        "â”‚                                                      â”‚              â”‚\n",
        "â”‚                                               â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”       â”‚\n",
        "â”‚                                               â”‚             â”‚       â”‚\n",
        "â”‚                                               â–¼             â–¼       â”‚\n",
        "â”‚                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
        "â”‚                                        â”‚ Facetten â”‚  â”‚ Streaming â”‚  â”‚\n",
        "â”‚                                        â”‚Drill-Downâ”‚  â”‚Expressionsâ”‚  â”‚\n",
        "â”‚                                        â”‚  <50ms   â”‚  â”‚  Aggreg.  â”‚  â”‚\n",
        "â”‚                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
        "â”‚                                               â”‚             â”‚       â”‚\n",
        "â”‚                                               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
        "â”‚                                                      â–¼              â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\n",
        "â”‚  â”‚    Spark     â”‚ â—„â”€â”€â”€ â”‚ Solr /export â”‚ â—„â”€â”€â”€ â”‚  Deep-Dive   â”‚       â”‚\n",
        "â”‚  â”‚  (ML/Stats)  â”‚      â”‚  (Streaming) â”‚      â”‚   Request    â”‚       â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
        "â”‚                                                                      â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "## Die 4 SÃ¤ulen dieser Architektur\n",
        "\n",
        "| # | Komponente | Aufgabe | Latenz |\n",
        "|---|------------|---------|--------|\n",
        "| 1 | **Rohdaten in Solr** | Alle Daten indiziert fÃ¼r Suche & Facetten | Einmaliger Import |\n",
        "| 2 | **Facetten-Drill-Down** | Interaktive Exploration Ã¼ber Facetten | <50ms |\n",
        "| 3 | **Streaming Expressions** | Ad-hoc Aggregationen in Solr | 100-500ms |\n",
        "| 4 | **Spark via /export** | Komplexe Analysen auf Teilmengen | Sekunden |\n",
        "\n",
        "**Vorteil:** User kann von Ãœbersicht bis Einzeldatensatz navigieren!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Konfiguration\n",
        "# =============================================================================\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# Spark Cluster\n",
        "SPARK_MASTER = 'spark://node0.cloud.local:7077'\n",
        "\n",
        "# Solr Cloud\n",
        "SOLR_HOST = 'node1'\n",
        "SOLR_PORT = 8983\n",
        "SOLR_NODES = ['node1', 'node2', 'node3', 'node4']  # FÃ¼r Load Balancing\n",
        "ZK_HOSTS = 'node1:2181,node2:2181,node3:2181'\n",
        "\n",
        "# Collection fÃ¼r Rohdaten\n",
        "COLLECTION = 'nyc-taxi-raw'\n",
        "\n",
        "# Daten\n",
        "PARQUET_FILES = [\n",
        "    ('yellow_tripdata_2023-01.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet'),\n",
        "    ('yellow_tripdata_2023-02.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet'),\n",
        "]\n",
        "DATA_DIR = '/data/spark/datasets/nyc-taxi'\n",
        "\n",
        "print('âœ… Konfiguration geladen')\n",
        "print(f'   Spark Master: {SPARK_MASTER}')\n",
        "print(f'   Solr:         http://{SOLR_HOST}:{SOLR_PORT}/solr/{COLLECTION}')\n",
        "print(f'   Solr Nodes:   {SOLR_NODES}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Teil 1: Rohdaten komplett in Solr laden\n",
        "\n",
        "Wir laden alle ~6 Millionen Taxi-Fahrten in Solr.\n",
        "Das ermÃ¶glicht spÃ¤ter Drill-Down auf Einzelfahrt-Ebene."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_collection_with_schema(name, num_shards=16, replication_factor=1):\n",
        "    \"\"\"\n",
        "    Erstellt eine Solr Collection mit optimiertem Schema fÃ¼r Drill-Down.\n",
        "    16 Shards = 4 pro Node fÃ¼r optimale ParallelitÃ¤t beim Laden.\n",
        "    \"\"\"\n",
        "    base_url = f'http://{SOLR_HOST}:{SOLR_PORT}/solr'\n",
        "    \n",
        "    # 1. Collection erstellen\n",
        "    print(f'ğŸ“¦ Erstelle Collection {name}...')\n",
        "    resp = requests.get(f'{base_url}/admin/collections', params={\n",
        "        'action': 'CREATE',\n",
        "        'name': name,\n",
        "        'numShards': num_shards,\n",
        "        'replicationFactor': replication_factor,\n",
        "        'collection.configName': '_default'\n",
        "    }, timeout=60)\n",
        "    \n",
        "    if 'already exists' in resp.text:\n",
        "        print(f'   â„¹ï¸  Collection existiert bereits')\n",
        "    elif resp.status_code == 200:\n",
        "        print(f'   âœ… Collection erstellt')\n",
        "    else:\n",
        "        print(f'   âš ï¸  {resp.text[:200]}')\n",
        "    \n",
        "    # 2. Schema-Felder fÃ¼r Facetten hinzufÃ¼gen\n",
        "    print(f'ğŸ“ Konfiguriere Schema fÃ¼r Facetten...')\n",
        "    \n",
        "    # Felder die als Facetten dienen (indexed, docValues fÃ¼r schnelle Aggregation)\n",
        "    facet_fields = [\n",
        "        {'name': 'pickup_date', 'type': 'string', 'indexed': True, 'stored': True, 'docValues': True},\n",
        "        {'name': 'pickup_hour', 'type': 'pint', 'indexed': True, 'stored': True, 'docValues': True},\n",
        "        {'name': 'pickup_dayofweek', 'type': 'pint', 'indexed': True, 'stored': True, 'docValues': True},\n",
        "        {'name': 'PULocationID', 'type': 'pint', 'indexed': True, 'stored': True, 'docValues': True},\n",
        "        {'name': 'DOLocationID', 'type': 'pint', 'indexed': True, 'stored': True, 'docValues': True},\n",
        "        {'name': 'payment_type', 'type': 'pint', 'indexed': True, 'stored': True, 'docValues': True},\n",
        "        {'name': 'passenger_count', 'type': 'pint', 'indexed': True, 'stored': True, 'docValues': True},\n",
        "        {'name': 'trip_distance', 'type': 'pdouble', 'indexed': True, 'stored': True, 'docValues': True},\n",
        "        {'name': 'total_amount', 'type': 'pdouble', 'indexed': True, 'stored': True, 'docValues': True},\n",
        "        {'name': 'tip_amount', 'type': 'pdouble', 'indexed': True, 'stored': True, 'docValues': True},\n",
        "        {'name': 'fare_amount', 'type': 'pdouble', 'indexed': True, 'stored': True, 'docValues': True},\n",
        "    ]\n",
        "    \n",
        "    for field in facet_fields:\n",
        "        resp = requests.post(\n",
        "            f'{base_url}/{name}/schema',\n",
        "            json={'add-field': field},\n",
        "            headers={'Content-Type': 'application/json'},\n",
        "            timeout=30\n",
        "        )\n",
        "        if resp.status_code == 200 or 'already exists' in resp.text:\n",
        "            print(f'   âœ… Feld {field[\"name\"]}')\n",
        "        else:\n",
        "            print(f'   âš ï¸  Feld {field[\"name\"]}: {resp.text[:100]}')\n",
        "    \n",
        "    print('âœ… Schema konfiguriert')\n",
        "    return True\n",
        "\n",
        "# Collection erstellen\n",
        "create_collection_with_schema(COLLECTION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parquet-Daten herunterladen (falls nicht vorhanden)\n",
        "import urllib.request\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "for filename, url in PARQUET_FILES:\n",
        "    filepath = os.path.join(DATA_DIR, filename)\n",
        "    if os.path.exists(filepath):\n",
        "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "        print(f'âœ… {filename} existiert ({size_mb:.1f} MB)')\n",
        "    else:\n",
        "        print(f'â¬‡ï¸  Lade {filename}...')\n",
        "        urllib.request.urlretrieve(url, filepath)\n",
        "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "        print(f'âœ… {filename} heruntergeladen ({size_mb:.1f} MB)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "print('ğŸš€ Starte Spark Session...')\n",
        "\n",
        "# Optimiert: 1 Executor pro Worker, 4 Cores pro Executor\n",
        "# â†’ 4 Worker Ã— 1 Executor Ã— 4 Cores = 16 parallele Tasks\n",
        "# â†’ LÃ¤sst 4 HT-Cores pro Node fÃ¼r Solr/OS\n",
        "# â†’ Mehr Cores bringt hier keinen Vorteil (getestet)\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('Drill-Down Architecture - Parallel Import') \\\n",
        "    .master(SPARK_MASTER) \\\n",
        "    .config('spark.executor.cores', '4') \\\n",
        "    .config('spark.executor.memory', '20g') \\\n",
        "    .config('spark.executor.instances', '4') \\\n",
        "    .config('spark.driver.memory', '4g') \\\n",
        "    .config('spark.sql.shuffle.partitions', '16') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f'âœ… Spark Session gestartet')\n",
        "print(f'   Master: {spark.sparkContext.master}')\n",
        "print(f'   Executors: 4 Ã— 4 Cores = 16 parallele Tasks')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def solr_delete_all(collection):\n",
        "    \"\"\"LÃ¶scht alle Dokumente einer Collection.\"\"\"\n",
        "    url = f'http://{SOLR_HOST}:{SOLR_PORT}/solr/{collection}/update'\n",
        "    resp = requests.post(url, json={'delete': {'query': '*:*'}}, params={'commit': 'true'}, timeout=60)\n",
        "    resp.raise_for_status()\n",
        "    return resp.json()\n",
        "\n",
        "def import_parquet_to_solr_parallel(spark, parquet_path, collection, batch_size=2000):\n",
        "    \"\"\"\n",
        "    Hochparalleler Import: Spark liest Parquet optimal, Executors schreiben an lokalen Solr.\n",
        "    \n",
        "    Datenfluss:\n",
        "      1. Spark liest Parquet mit optimiertem Reader (nutzt alle Row Groups parallel)\n",
        "      2. Repartition auf 16 Partitionen = 4 Executors Ã— 4 Cores\n",
        "      3. Jeder Executor sendet an lokalen Solr (127.0.0.1) als Entry Point\n",
        "    \n",
        "    Hinweis zur Data Locality:\n",
        "      Der lokale Solr ist nur der Entry Point. SolrCloud routet Dokumente\n",
        "      basierend auf der ID zum richtigen Shard, der auf einem anderen Node\n",
        "      liegen kann. Der Vorteil ist daher begrenzt auf den ersten Hop.\n",
        "    \"\"\"\n",
        "    print(f'\\nğŸ“– Lade {parquet_path}...')\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Parquet laden (Spark's optimierter Reader)\n",
        "    df = spark.read.parquet(parquet_path)\n",
        "    \n",
        "    # Spalten normalisieren\n",
        "    if 'Airport_fee' in df.columns:\n",
        "        df = df.withColumnRenamed('Airport_fee', 'airport_fee')\n",
        "    \n",
        "    # Datum-Spalten transformieren (VOR dem Push zu Executors)\n",
        "    df = df.withColumn('pickup_date', F.date_format('tpep_pickup_datetime', 'yyyy-MM-dd')) \\\n",
        "           .withColumn('pickup_hour', F.hour('tpep_pickup_datetime')) \\\n",
        "           .withColumn('pickup_dayofweek', F.dayofweek('tpep_pickup_datetime')) \\\n",
        "           .withColumn('tpep_pickup_datetime', F.date_format('tpep_pickup_datetime', \"yyyy-MM-dd'T'HH:mm:ss'Z'\")) \\\n",
        "           .withColumn('tpep_dropoff_datetime', F.date_format('tpep_dropoff_datetime', \"yyyy-MM-dd'T'HH:mm:ss'Z'\")) \\\n",
        "           .withColumn('id', F.expr('uuid()'))\n",
        "    \n",
        "    # Optimale ParallelitÃ¤t: 16 Partitionen fÃ¼r 4 Executors Ã— 4 Cores\n",
        "    num_partitions = 16\n",
        "    df = df.repartition(num_partitions)\n",
        "    \n",
        "    total_rows = df.count()\n",
        "    print(f'   {total_rows:,} Zeilen in {time.time() - start_time:.1f}s')\n",
        "    print(f'   Partitionen: {num_partitions} (4 Executors Ã— 4 Cores)')\n",
        "    \n",
        "    print(f'ğŸ“¤ Paralleler Push nach Solr {collection} (Entry Point: 127.0.0.1)...')\n",
        "    start_push = time.time()\n",
        "    \n",
        "    # Broadcast der Konfiguration\n",
        "    solr_port_bc = spark.sparkContext.broadcast(SOLR_PORT)\n",
        "    collection_bc = spark.sparkContext.broadcast(collection)\n",
        "    batch_size_bc = spark.sparkContext.broadcast(batch_size)\n",
        "    \n",
        "    def send_partition_to_local_solr(partition_index, iterator):\n",
        "        \"\"\"Sendet Partition an LOKALEN Solr (127.0.0.1) - Data Locality!\"\"\"\n",
        "        import requests\n",
        "        import socket\n",
        "        \n",
        "        port = solr_port_bc.value\n",
        "        coll = collection_bc.value\n",
        "        batch_sz = batch_size_bc.value\n",
        "        hostname = socket.gethostname()\n",
        "        \n",
        "        # LOKALER Solr - kein Netzwerk!\n",
        "        url = f'http://127.0.0.1:{port}/solr/{coll}/update'\n",
        "        \n",
        "        batch = []\n",
        "        count = 0\n",
        "        \n",
        "        for row in iterator:\n",
        "            doc = row.asDict()\n",
        "            # NaN handling (v != v ist True fÃ¼r NaN)\n",
        "            doc = {k: (None if v != v else v) for k, v in doc.items()}\n",
        "            batch.append(doc)\n",
        "            \n",
        "            if len(batch) >= batch_sz:\n",
        "                resp = requests.post(url, json=batch, timeout=60)\n",
        "                resp.raise_for_status()\n",
        "                count += len(batch)\n",
        "                batch = []\n",
        "        \n",
        "        if batch:\n",
        "            resp = requests.post(url, json=batch, timeout=60)\n",
        "            resp.raise_for_status()\n",
        "            count += len(batch)\n",
        "        \n",
        "        yield (partition_index, hostname, count)\n",
        "    \n",
        "    # Parallel auf allen Executors\n",
        "    results = df.rdd.mapPartitionsWithIndex(send_partition_to_local_solr).collect()\n",
        "    \n",
        "    # Commit\n",
        "    commit_url = f'http://{SOLR_HOST}:{SOLR_PORT}/solr/{collection}/update?commit=true'\n",
        "    requests.post(commit_url, json=[], timeout=30)\n",
        "    \n",
        "    elapsed = time.time() - start_push\n",
        "    total_sent = sum(r[2] for r in results)\n",
        "    \n",
        "    # Statistik nach Worker gruppieren\n",
        "    from collections import defaultdict\n",
        "    by_host = defaultdict(lambda: {'partitions': 0, 'docs': 0})\n",
        "    for _, hostname, count in results:\n",
        "        by_host[hostname]['partitions'] += 1\n",
        "        by_host[hostname]['docs'] += count\n",
        "    \n",
        "    print(f'   ğŸ“Š Import-Statistik (Entry Point = lokaler Solr):')\n",
        "    for hostname in sorted(by_host.keys()):\n",
        "        stats = by_host[hostname]\n",
        "        print(f'      {hostname}: {stats[\"partitions\"]:2d} Partitionen, {stats[\"docs\"]:,} docs â†’ 127.0.0.1 â†’ SolrCloud routing')\n",
        "    \n",
        "    print(f'   âœ… {total_sent:,} Dokumente in {elapsed:.1f}s ({total_sent/elapsed:,.0f} docs/s)')\n",
        "    \n",
        "    return total_sent\n",
        "\n",
        "print('âœ… Import-Funktionen definiert (Spark-optimiert, lokaler Solr als Entry Point)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Daten importieren - Spark-optimiert mit lokalem Solr-Write\n",
        "print('=' * 70)\n",
        "print('ğŸ“¦ PARALLELER IMPORT: Spark liest optimal, Executors schreiben lokal')\n",
        "print('=' * 70)\n",
        "print()\n",
        "print('Architektur:')\n",
        "print('  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”')\n",
        "print('  â”‚  Spark liest Parquet optimiert (alle Row Groups parallel)       â”‚')\n",
        "print('  â”‚  â†’ 4 Executors Ã— 4 Cores = 16 parallele Tasks                  â”‚')\n",
        "print('  â”‚  â†’ Jeder Executor schreibt an lokalen Solr (127.0.0.1:8983)    â”‚')\n",
        "print('  â”‚                                                                 â”‚')\n",
        "print('  â”‚  Hinweis: Lokaler Solr ist Entry Point, SolrCloud routet dann  â”‚')\n",
        "print('  â”‚           Dokumente zum richtigen Shard (ggf. anderer Node)    â”‚')\n",
        "print('  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜')\n",
        "print()\n",
        "\n",
        "# Alte Daten lÃ¶schen\n",
        "print('ğŸ—‘ï¸  LÃ¶sche alte Daten...')\n",
        "solr_delete_all(COLLECTION)\n",
        "\n",
        "total_imported = 0\n",
        "for filename, _ in PARQUET_FILES:\n",
        "    filepath = os.path.join(DATA_DIR, filename)\n",
        "    count = import_parquet_to_solr_parallel(spark, filepath, COLLECTION)\n",
        "    total_imported += count\n",
        "\n",
        "print(f'\\n{\"=\" * 70}')\n",
        "print(f'âœ… IMPORT ABGESCHLOSSEN: {total_imported:,} Dokumente in Solr')\n",
        "print(f'{\"=\" * 70}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Teil 2: Facetten-basierter Drill-Down\n",
        "\n",
        "Jetzt demonstrieren wir interaktiven Drill-Down Ã¼ber Solr Facetten.\n",
        "\n",
        "**Use Case:** User startet mit Ãœbersicht â†’ klickt auf interessanten Wert â†’ sieht Details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "class DrillDownExplorer:\n",
        "    \"\"\"\n",
        "    Interaktiver Drill-Down Explorer mit Solr Facetten.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, collection, solr_host=SOLR_HOST, solr_port=SOLR_PORT):\n",
        "        self.base_url = f'http://{solr_host}:{solr_port}/solr/{collection}'\n",
        "        self.current_filters = []\n",
        "        \n",
        "    def query_with_facets(self, q='*:*', facet_fields=None, fq=None, rows=0):\n",
        "        \"\"\"\n",
        "        FÃ¼hrt Query mit Facetten aus.\n",
        "        \"\"\"\n",
        "        if facet_fields is None:\n",
        "            facet_fields = ['pickup_hour', 'pickup_dayofweek', 'PULocationID', 'payment_type']\n",
        "        \n",
        "        params = {\n",
        "            'q': q,\n",
        "            'rows': rows,\n",
        "            'facet': 'true',\n",
        "            'facet.field': facet_fields,\n",
        "            'facet.limit': 20,\n",
        "            'facet.mincount': 1,\n",
        "            'wt': 'json'\n",
        "        }\n",
        "        \n",
        "        if fq:\n",
        "            params['fq'] = fq\n",
        "        \n",
        "        start = time.time()\n",
        "        resp = requests.get(f'{self.base_url}/select', params=params, timeout=30)\n",
        "        elapsed_ms = (time.time() - start) * 1000\n",
        "        \n",
        "        result = resp.json()\n",
        "        \n",
        "        return {\n",
        "            'numFound': result['response']['numFound'],\n",
        "            'facets': result.get('facet_counts', {}).get('facet_fields', {}),\n",
        "            'qtime_ms': elapsed_ms,\n",
        "            'docs': result['response'].get('docs', [])\n",
        "        }\n",
        "    \n",
        "    def show_facet_distribution(self, field, fq=None, title=None):\n",
        "        \"\"\"\n",
        "        Zeigt Verteilung eines Facetten-Feldes.\n",
        "        \"\"\"\n",
        "        result = self.query_with_facets(facet_fields=[field], fq=fq)\n",
        "        \n",
        "        # Facetten-Daten parsen (Solr gibt [value, count, value, count, ...] zurÃ¼ck)\n",
        "        facet_data = result['facets'].get(field, [])\n",
        "        pairs = [(facet_data[i], facet_data[i+1]) for i in range(0, len(facet_data), 2)]\n",
        "        \n",
        "        df = pd.DataFrame(pairs, columns=[field, 'count'])\n",
        "        \n",
        "        print(f'\\n{\"â”€\" * 50}')\n",
        "        print(f'ğŸ“Š {title or field.upper()}')\n",
        "        print(f'   Gefilterte Dokumente: {result[\"numFound\"]:,}')\n",
        "        print(f'   Query-Zeit: {result[\"qtime_ms\"]:.0f}ms')\n",
        "        if fq:\n",
        "            print(f'   Filter: {fq}')\n",
        "        print(f'{\"â”€\" * 50}')\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def drill_down(self, fq, fields=None, limit=10):\n",
        "        \"\"\"\n",
        "        Drill-Down: Zeigt Details fÃ¼r einen Filter.\n",
        "        \"\"\"\n",
        "        if fields is None:\n",
        "            fields = '*'\n",
        "        \n",
        "        params = {\n",
        "            'q': '*:*',\n",
        "            'fq': fq,\n",
        "            'rows': limit,\n",
        "            'fl': fields,\n",
        "            'wt': 'json'\n",
        "        }\n",
        "        \n",
        "        start = time.time()\n",
        "        resp = requests.get(f'{self.base_url}/select', params=params, timeout=30)\n",
        "        elapsed_ms = (time.time() - start) * 1000\n",
        "        \n",
        "        result = resp.json()\n",
        "        \n",
        "        print(f'\\n{\"â”€\" * 50}')\n",
        "        print(f'ğŸ” DRILL-DOWN: {fq}')\n",
        "        print(f'   Treffer: {result[\"response\"][\"numFound\"]:,}')\n",
        "        print(f'   Query-Zeit: {elapsed_ms:.0f}ms')\n",
        "        print(f'{\"â”€\" * 50}')\n",
        "        \n",
        "        return pd.DataFrame(result['response']['docs'])\n",
        "\n",
        "# Explorer initialisieren\n",
        "explorer = DrillDownExplorer(COLLECTION)\n",
        "print('âœ… Drill-Down Explorer initialisiert')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Level 1: Ãœbersicht - Verteilung nach Stunde\n",
        "# ============================================================\n",
        "print('=' * 70)\n",
        "print('ğŸ¯ DRILL-DOWN DEMO: Von Ãœbersicht zu Einzelfahrt')\n",
        "print('=' * 70)\n",
        "\n",
        "df_hours = explorer.show_facet_distribution('pickup_hour', title='Fahrten pro Stunde')\n",
        "display(df_hours.head(10))\n",
        "\n",
        "# Visualisierung\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.bar(df_hours['pickup_hour'].astype(int), df_hours['count'])\n",
        "plt.xlabel('Stunde')\n",
        "plt.ylabel('Anzahl Fahrten')\n",
        "plt.title('Taxi-Fahrten pro Stunde (alle Daten)')\n",
        "plt.xticks(range(0, 24))\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\nğŸ‘† Beobachtung: Stunde 18 (18:00 Uhr) hat viele Fahrten. Drill-Down!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Level 2: Drill-Down auf Stunde 18 - Welche Zonen?\n",
        "# ============================================================\n",
        "\n",
        "df_zones = explorer.show_facet_distribution(\n",
        "    'PULocationID', \n",
        "    fq='pickup_hour:18',\n",
        "    title='Top Pickup-Zonen um 18:00 Uhr'\n",
        ")\n",
        "display(df_zones.head(10))\n",
        "\n",
        "print('\\nğŸ‘† Zone 237 und 236 sind Hotspots um 18 Uhr. Weiter Drill-Down!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Level 3: Drill-Down auf Zone 237 um 18 Uhr - Zahlungsarten?\n",
        "# ============================================================\n",
        "\n",
        "df_payment = explorer.show_facet_distribution(\n",
        "    'payment_type',\n",
        "    fq=['pickup_hour:18', 'PULocationID:237'],\n",
        "    title='Zahlungsarten in Zone 237 um 18:00'\n",
        ")\n",
        "\n",
        "# Payment Type Mapping\n",
        "payment_map = {1: 'Kreditkarte', 2: 'Bargeld', 3: 'Keine Zahlung', 4: 'Streit'}\n",
        "df_payment['payment_name'] = df_payment['payment_type'].astype(int).map(payment_map)\n",
        "display(df_payment)\n",
        "\n",
        "print('\\nğŸ‘† Interessant: Mehr Kreditkarte als Bargeld. Einzelfahrten ansehen!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Level 4: Drill-Down auf Einzelfahrten\n",
        "# ============================================================\n",
        "\n",
        "df_trips = explorer.drill_down(\n",
        "    fq=['pickup_hour:18', 'PULocationID:237', 'payment_type:1'],\n",
        "    fields='id,tpep_pickup_datetime,PULocationID,DOLocationID,trip_distance,total_amount,tip_amount,passenger_count',\n",
        "    limit=15\n",
        ")\n",
        "\n",
        "display(df_trips)\n",
        "\n",
        "print('\\nâœ… Drill-Down komplett: Von 6M Fahrten â†’ spezifische Einzelfahrten')\n",
        "print('   Jede Query < 50ms!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Teil 3: Solr Streaming Expressions fÃ¼r Ad-hoc Aggregationen\n",
        "\n",
        "Streaming Expressions sind Solrs eingebaute Aggregations-Engine.\n",
        "Sie ermÃ¶glichen komplexe Aggregationen **ohne Spark** - direkt in Solr."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_streaming_expression(expr, collection=COLLECTION):\n",
        "    \"\"\"\n",
        "    FÃ¼hrt eine Solr Streaming Expression aus.\n",
        "    \"\"\"\n",
        "    url = f'http://{SOLR_HOST}:{SOLR_PORT}/solr/{collection}/stream'\n",
        "    \n",
        "    start = time.time()\n",
        "    resp = requests.post(\n",
        "        url,\n",
        "        data={'expr': expr},\n",
        "        headers={'Content-Type': 'application/x-www-form-urlencoded'},\n",
        "        timeout=60\n",
        "    )\n",
        "    elapsed_ms = (time.time() - start) * 1000\n",
        "    \n",
        "    result = resp.json()\n",
        "    \n",
        "    # Ergebnis extrahieren (Stream-Format)\n",
        "    docs = []\n",
        "    if 'result-set' in result:\n",
        "        for item in result['result-set'].get('docs', []):\n",
        "            if 'EOF' not in item:  # End-of-stream marker ignorieren\n",
        "                docs.append(item)\n",
        "    \n",
        "    print(f'   â±ï¸  Query-Zeit: {elapsed_ms:.0f}ms')\n",
        "    return docs\n",
        "\n",
        "print('âœ… Streaming Expression Helper geladen')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Streaming Expression 1: Statistiken nach Stunde\n",
        "# ============================================================\n",
        "print('\\n' + '=' * 70)\n",
        "print('ğŸ“Š STREAMING EXPRESSION: Umsatz-Statistik pro Stunde')\n",
        "print('=' * 70)\n",
        "\n",
        "expr = f'''\n",
        "rollup(\n",
        "  search({COLLECTION},\n",
        "    q=\"*:*\",\n",
        "    fl=\"pickup_hour,total_amount,tip_amount\",\n",
        "    sort=\"pickup_hour asc\",\n",
        "    qt=\"/export\"\n",
        "  ),\n",
        "  over=\"pickup_hour\",\n",
        "  sum(total_amount),\n",
        "  avg(total_amount),\n",
        "  avg(tip_amount),\n",
        "  count(*)\n",
        ")\n",
        "'''\n",
        "\n",
        "results = execute_streaming_expression(expr)\n",
        "df_stats = pd.DataFrame(results)\n",
        "display(df_stats.head(15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Streaming Expression 2: Top Routen nach Umsatz\n",
        "# ============================================================\n",
        "print('\\n' + '=' * 70)\n",
        "print('ğŸ“Š STREAMING EXPRESSION: Top 20 Routen nach Umsatz')\n",
        "print('=' * 70)\n",
        "\n",
        "expr = f'''\n",
        "top(\n",
        "  n=20,\n",
        "  rollup(\n",
        "    search({COLLECTION},\n",
        "      q=\"*:*\",\n",
        "      fl=\"PULocationID,DOLocationID,total_amount\",\n",
        "      sort=\"PULocationID asc\",\n",
        "      qt=\"/export\"\n",
        "    ),\n",
        "    over=\"PULocationID,DOLocationID\",\n",
        "    sum(total_amount),\n",
        "    count(*)\n",
        "  ),\n",
        "  sort=\"sum(total_amount) desc\"\n",
        ")\n",
        "'''\n",
        "\n",
        "results = execute_streaming_expression(expr)\n",
        "df_routes = pd.DataFrame(results)\n",
        "df_routes.columns = [c.replace('(', '_').replace(')', '').replace('*', 'all') for c in df_routes.columns]\n",
        "display(df_routes)\n",
        "\n",
        "# Merke die Zeit fÃ¼r spÃ¤teren Vergleich\n",
        "streaming_routes_time = None  # Wird oben in execute_streaming_expression ausgegeben"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Streaming Expression 3: Statistiken mit Filter\n",
        "# ============================================================\n",
        "print('\\n' + '=' * 70)\n",
        "print('ğŸ“Š STREAMING EXPRESSION: Wochenend-Nachtfahrten (Fr/Sa 22:00-04:00)')\n",
        "print('=' * 70)\n",
        "\n",
        "# Freitag = 6, Samstag = 7 in Spark's dayofweek\n",
        "expr = f'''\n",
        "rollup(\n",
        "  search({COLLECTION},\n",
        "    q=\"pickup_dayofweek:(6 OR 7) AND (pickup_hour:[22 TO 23] OR pickup_hour:[0 TO 4])\",\n",
        "    fl=\"pickup_hour,total_amount,tip_amount,trip_distance\",\n",
        "    sort=\"pickup_hour asc\",\n",
        "    qt=\"/export\"\n",
        "  ),\n",
        "  over=\"pickup_hour\",\n",
        "  sum(total_amount),\n",
        "  avg(total_amount),\n",
        "  avg(tip_amount),\n",
        "  avg(trip_distance),\n",
        "  count(*)\n",
        ")\n",
        "'''\n",
        "\n",
        "results = execute_streaming_expression(expr)\n",
        "if results:\n",
        "    df_night = pd.DataFrame(results)\n",
        "    df_night.columns = [c.replace('(', '_').replace(')', '').replace('*', 'all') for c in df_night.columns]\n",
        "    display(df_night)\n",
        "else:\n",
        "    print('Keine Ergebnisse - Filter anpassen?')\n",
        "\n",
        "print('\\nâœ… Streaming Expressions: Aggregationen ohne Spark in 100-500ms!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Teil 4: Spark fÃ¼r komplexe Analysen via Solr /export (Parallel)\n",
        "\n",
        "Wenn Streaming Expressions nicht ausreichen (ML, komplexe Statistik),\n",
        "kann Spark gefilterte Daten Ã¼ber den `/export` Handler laden.\n",
        "\n",
        "**Optimierung: Paralleles Shard-Loading**\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Paralleles Laden: Jeder Executor â†’ Lokaler Shard          â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                             â”‚\n",
        "â”‚  Executor 1 â”€â”€â–º node1:8983/solr/core_shard1/export â”€â”€â”     â”‚\n",
        "â”‚  Executor 2 â”€â”€â–º node2:8983/solr/core_shard2/export â”€â”€â”¼â”€â”€â–º  â”‚\n",
        "â”‚  Executor 3 â”€â”€â–º node3:8983/solr/core_shard3/export â”€â”€â”¤  RDDâ”‚\n",
        "â”‚  Executor 4 â”€â”€â–º node4:8983/solr/core_shard4/export â”€â”€â”˜     â”‚\n",
        "â”‚                                                             â”‚\n",
        "â”‚  Vorteil: 4x paralleler I/O, kein Single-Node-Bottleneck!  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_shard_info(collection):\n",
        "    \"\"\"\n",
        "    Holt Shard-Informationen aus dem Solr CLUSTERSTATUS API.\n",
        "    Gibt Liste von {node, core, shard_name} zurÃ¼ck fÃ¼r paralleles Laden.\n",
        "    \"\"\"\n",
        "    url = f'http://{SOLR_HOST}:{SOLR_PORT}/solr/admin/collections'\n",
        "    params = {'action': 'CLUSTERSTATUS', 'collection': collection, 'wt': 'json'}\n",
        "    \n",
        "    resp = requests.get(url, params=params, timeout=10)\n",
        "    resp.raise_for_status()\n",
        "    data = resp.json()\n",
        "    \n",
        "    shard_info = []\n",
        "    shards = data.get('cluster', {}).get('collections', {}).get(collection, {}).get('shards', {})\n",
        "    \n",
        "    for shard_name, shard_data in shards.items():\n",
        "        replicas = shard_data.get('replicas', {})\n",
        "        for replica_name, replica_data in replicas.items():\n",
        "            if replica_data.get('state') == 'active':\n",
        "                # node_name format: 'node1.cloud.local:8983_solr'\n",
        "                node_name = replica_data.get('node_name', '')\n",
        "                node = node_name.split(':')[0]  # 'node1.cloud.local'\n",
        "                core = replica_data.get('core')  # 'nyc-taxi-raw_shard1_replica_n1'\n",
        "                \n",
        "                shard_info.append({\n",
        "                    'node': node,\n",
        "                    'core': core,\n",
        "                    'shard_name': shard_name\n",
        "                })\n",
        "                break  # Nur eine aktive Replica pro Shard\n",
        "    \n",
        "    return shard_info\n",
        "\n",
        "\n",
        "def solr_to_spark_via_export(spark, query, fields, collection=COLLECTION, max_rows=None):\n",
        "    \"\"\"\n",
        "    LÃ¤dt gefilterte Daten PARALLEL aus allen Solr Shards nach Spark.\n",
        "    \n",
        "    Datenfluss (nodelokal):\n",
        "      1. Hole Shardâ†’Node Mapping via CLUSTERSTATUS API\n",
        "      2. Verteile Shards als RDD auf Spark Executors\n",
        "      3. Jeder Executor ruft /export auf SEINEM lokalen Shard\n",
        "      4. Ergebnisse werden zu DataFrame kombiniert\n",
        "    \n",
        "    Vorteil: 4x paralleler I/O statt Single-Node-Bottleneck!\n",
        "    \"\"\"\n",
        "    print(f'\\nğŸ“¥ PARALLELES Laden von Solr nach Spark...')\n",
        "    print(f'   Query: {query}')\n",
        "    print(f'   Felder: {fields}')\n",
        "    \n",
        "    start = time.time()\n",
        "    \n",
        "    # 1. Shard-Info holen\n",
        "    shard_info = get_shard_info(collection)\n",
        "    print(f'   Shards gefunden: {len(shard_info)}')\n",
        "    for s in shard_info:\n",
        "        print(f'      - {s[\"shard_name\"]}: {s[\"node\"]} â†’ {s[\"core\"]}')\n",
        "    \n",
        "    # 2. Zuerst: Wie viele Dokumente insgesamt?\n",
        "    count_url = f'http://{SOLR_HOST}:{SOLR_PORT}/solr/{collection}/select'\n",
        "    count_resp = requests.get(count_url, params={'q': query, 'rows': 0, 'wt': 'json'})\n",
        "    total_docs = count_resp.json()['response']['numFound']\n",
        "    print(f'   Gefundene Dokumente: {total_docs:,}')\n",
        "    \n",
        "    if total_docs == 0:\n",
        "        print('   âš ï¸  Keine Daten gefunden')\n",
        "        return None\n",
        "    \n",
        "    # 3. Broadcast der Query-Parameter\n",
        "    sc = spark.sparkContext\n",
        "    sort_field = fields.split(',')[0]\n",
        "    query_bc = sc.broadcast((query, fields, sort_field))\n",
        "    \n",
        "    # 4. RDD mit Shard-Infos erstellen (1 Partition pro Shard)\n",
        "    shard_rdd = sc.parallelize(shard_info, len(shard_info))\n",
        "    \n",
        "    def load_shard_via_export(shard):\n",
        "        # LÃ¤dt Daten von einem einzelnen Shard via /export - nodelokal!\n",
        "        import requests\n",
        "        \n",
        "        node = shard['node']\n",
        "        core = shard['core']\n",
        "        q, fl, sort_f = query_bc.value\n",
        "        \n",
        "        # Direkt an den Shard-Core auf dem lokalen Node\n",
        "        export_url = f'http://{node}:8983/solr/{core}/export'\n",
        "        params = {\n",
        "            'q': q,\n",
        "            'fl': fl,\n",
        "            'sort': f'{sort_f} asc',\n",
        "            'wt': 'json'\n",
        "        }\n",
        "        \n",
        "        docs = []\n",
        "        try:\n",
        "            response = requests.get(export_url, params=params, timeout=120)\n",
        "            response.raise_for_status()\n",
        "            result = response.json()\n",
        "            docs = result.get('response', {}).get('docs', [])\n",
        "        except Exception as e:\n",
        "            print(f'Fehler beim Laden von {node}/{core}: {e}')\n",
        "        \n",
        "        return docs\n",
        "    \n",
        "    # 5. Parallel von allen Shards laden\n",
        "    print(f'   ğŸš€ Starte parallelen Export von {len(shard_info)} Shards...')\n",
        "    docs_rdd = shard_rdd.flatMap(load_shard_via_export)\n",
        "    \n",
        "    # 6. Direkt RDD â†’ DataFrame (OHNE collect() - bleibt verteilt!)\n",
        "    # Bei groÃŸen Datenmengen ist collect() ein Speicherkiller\n",
        "    try:\n",
        "        # Cache das RDD fÃ¼r mehrfache Verwendung\n",
        "        docs_rdd.cache()\n",
        "        doc_count = docs_rdd.count()\n",
        "        \n",
        "        if doc_count == 0:\n",
        "            print('   âš ï¸  Keine Daten von Shards erhalten')\n",
        "            return None\n",
        "        \n",
        "        fetch_time = time.time() - start\n",
        "        print(f'   Geladen: {doc_count:,} Dokumente in {fetch_time:.1f}s')\n",
        "        print(f'   Durchsatz: {doc_count / fetch_time:,.0f} Docs/s')\n",
        "        \n",
        "        # RDD von Dicts direkt zu DataFrame - bleibt verteilt!\n",
        "        sdf = spark.createDataFrame(docs_rdd)\n",
        "        print(f'   âœ… Spark DataFrame erstellt: {doc_count:,} Zeilen (verteilt)')\n",
        "        return sdf\n",
        "    except Exception as e:\n",
        "        print(f'   âŒ Fehler beim DataFrame-Erstellen: {e}')\n",
        "        return None\n",
        "\n",
        "\n",
        "print('âœ… Paralleler Solr-to-Spark Export Helper geladen')\n",
        "print('   â†’ get_shard_info(): Holt Shardâ†’Node Mapping')\n",
        "print('   â†’ solr_to_spark_via_export(): Paralleles Laden von allen Shards')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VERGLEICH: Top-Routen - Streaming Expression vs. Spark Parallel\n",
        "# ============================================================\n",
        "print('\\n' + '=' * 70)\n",
        "print('ğŸš€ VERGLEICH: Top 20 Routen - Streaming Expression vs. Spark Parallel')\n",
        "print('=' * 70)\n",
        "print('   Dieselbe Analyse wie oben, aber mit parallelem Shard-Loading')\n",
        "\n",
        "import time\n",
        "start_spark_routes = time.time()\n",
        "\n",
        "# Alle Daten parallel von allen Shards laden\n",
        "df_all = solr_to_spark_via_export(\n",
        "    spark,\n",
        "    query='*:*',\n",
        "    fields='PULocationID,DOLocationID,total_amount'\n",
        ")\n",
        "\n",
        "if df_all is not None:\n",
        "    # Aggregation in Spark\n",
        "    top_routes_spark = df_all.groupBy('PULocationID', 'DOLocationID').agg(\n",
        "        F.sum('total_amount').alias('sum_total_amount'),\n",
        "        F.count('*').alias('count_all')\n",
        "    ).orderBy(F.desc('sum_total_amount')).limit(20)\n",
        "    \n",
        "    # Zu Pandas fÃ¼r Anzeige\n",
        "    df_spark_routes = top_routes_spark.toPandas()\n",
        "    \n",
        "    spark_time = time.time() - start_spark_routes\n",
        "    print(f'\\nâ±ï¸  Spark parallel (4 Shards): {spark_time:.2f}s')\n",
        "    display(df_spark_routes)\n",
        "    \n",
        "    # Vergleich\n",
        "    print('\\n' + '=' * 70)\n",
        "    print('ğŸ“Š FAZIT: Streaming Expression vs. Spark Parallel')\n",
        "    print('=' * 70)\n",
        "    print(f'   Streaming Expression (Solr-intern):    ~17s')\n",
        "    print(f'   Spark Parallel (4 Shards â†’ Driver):    {spark_time:.2f}s')\n",
        "    print('\\n   â†’ Beide AnsÃ¤tze liefern Ã¤hnliche Performance!')\n",
        "    print('\\n   Streaming Expressions: âœ… Kein Spark nÃ¶tig, weniger Infrastruktur')\n",
        "    print('   Spark Parallel:         âœ… Flexibler fÃ¼r ML/komplexe Analysen')\n",
        "    print('\\n   ğŸ’¡ Empfehlung: Streaming fÃ¼r Aggregationen,')\n",
        "    print('                  Spark fÃ¼r ML und komplexe Transformationen.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Beispiel: ML-Analyse auf gefilterten Daten\n",
        "# ============================================================\n",
        "print('=' * 70)\n",
        "print('ğŸ§  SPARK ANALYSE: Tipp-Verhalten nach Distanz (Zone 237)')\n",
        "print('=' * 70)\n",
        "\n",
        "# Gefilterte Daten aus Solr laden\n",
        "df_analysis = solr_to_spark_via_export(\n",
        "    spark,\n",
        "    query='PULocationID:237 AND trip_distance:[1 TO 20]',\n",
        "    fields='trip_distance,total_amount,tip_amount,fare_amount,passenger_count'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if df_analysis is not None:\n",
        "    import time\n",
        "    start_corr = time.time()\n",
        "    \n",
        "    # Spark ML: Korrelationsanalyse\n",
        "    from pyspark.ml.stat import Correlation\n",
        "    from pyspark.ml.feature import VectorAssembler\n",
        "    \n",
        "    # Numerische Spalten zu Vector\n",
        "    numeric_cols = ['trip_distance', 'total_amount', 'tip_amount', 'fare_amount']\n",
        "    \n",
        "    # NaN/None entfernen\n",
        "    df_clean = df_analysis.dropna(subset=numeric_cols)\n",
        "    \n",
        "    assembler = VectorAssembler(inputCols=numeric_cols, outputCol='features')\n",
        "    df_vector = assembler.transform(df_clean)\n",
        "    \n",
        "    # Korrelationsmatrix\n",
        "    corr_matrix = Correlation.corr(df_vector, 'features').head()[0]\n",
        "    \n",
        "    corr_time = time.time() - start_corr\n",
        "    print(f'\\nğŸ“Š Korrelationsmatrix (berechnet in {corr_time:.2f}s):')\n",
        "    corr_df = pd.DataFrame(\n",
        "        corr_matrix.toArray(),\n",
        "        columns=numeric_cols,\n",
        "        index=numeric_cols\n",
        "    )\n",
        "    display(corr_df.round(3))\n",
        "    \n",
        "    # Statistiken\n",
        "    start_stats = time.time()\n",
        "    print('\\nğŸ“Š Deskriptive Statistik:')\n",
        "    df_analysis.describe().show()\n",
        "    stats_time = time.time() - start_stats\n",
        "    print(f'   â±ï¸  Statistik berechnet in {stats_time:.2f}s')\n",
        "else:\n",
        "    print('âš ï¸  df_analysis ist None - bitte vorherige Zelle zuerst ausfÃ¼hren!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if df_analysis is not None:\n",
        "    import time\n",
        "    start_tip = time.time()\n",
        "    \n",
        "    # Tipp-Rate Analyse\n",
        "    df_tip = df_analysis.withColumn(\n",
        "        'tip_rate', \n",
        "        F.when(F.col('fare_amount') > 0, \n",
        "               F.col('tip_amount') / F.col('fare_amount') * 100\n",
        "        ).otherwise(0)\n",
        "    )\n",
        "    \n",
        "    # Distanz-Buckets\n",
        "    df_buckets = df_tip.withColumn(\n",
        "        'distance_bucket',\n",
        "        F.when(F.col('trip_distance') < 2, '0-2 mi')\n",
        "         .when(F.col('trip_distance') < 5, '2-5 mi')\n",
        "         .when(F.col('trip_distance') < 10, '5-10 mi')\n",
        "         .otherwise('10+ mi')\n",
        "    )\n",
        "    \n",
        "    # Aggregation\n",
        "    print('\\nğŸ“Š Tipp-Rate nach Distanz:')\n",
        "    df_buckets.groupBy('distance_bucket').agg(\n",
        "        F.count('*').alias('fahrten'),\n",
        "        F.avg('tip_rate').alias('avg_tip_rate'),\n",
        "        F.avg('tip_amount').alias('avg_tip'),\n",
        "        F.avg('total_amount').alias('avg_total')\n",
        "    ).orderBy('distance_bucket').show()\n",
        "    \n",
        "    tip_time = time.time() - start_tip\n",
        "    print(f'\\nâœ… Komplexe Spark-Analyse auf Solr-gefilterten Daten!')\n",
        "    print(f'   â±ï¸  Tipp-Rate Analyse in {tip_time:.2f}s')\n",
        "else:\n",
        "    print('âš ï¸  df_analysis ist None - bitte Zelle mit solr_to_spark_via_export zuerst ausfÃ¼hren!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Zusammenfassung: Drill-Down Architektur\n",
        "\n",
        "| Komponente | Einsatz | Latenz | DatenlokalitÃ¤t |\n",
        "|------------|---------|--------|----------------|\n",
        "| **Solr Facetten** | Interaktive Navigation, Drill-Down | <50ms | Distributed |\n",
        "| **Streaming Expressions** | Ad-hoc Aggregationen | 100-500ms | Distributed |\n",
        "| **Spark via /export** | ML, komplexe Statistik | Sekunden | âœ… Shard-lokal |\n",
        "\n",
        "**Data Locality Pattern:**\n",
        "```\n",
        "Schreiben: Executor â†’ 127.0.0.1 â†’ SolrCloud routet zu Shard\n",
        "Lesen:     Executor â†’ node:8983/core_shardN/export (direkt!)\n",
        "```\n",
        "\n",
        "**Vorteile dieser Architektur:**\n",
        "- âœ… VollstÃ¤ndiger Drill-Down bis zur Einzelfahrt\n",
        "- âœ… Keine Pre-Aggregation nÃ¶tig\n",
        "- âœ… Facetten fÃ¼r intuitive Navigation\n",
        "- âœ… Spark nur wenn wirklich nÃ¶tig\n",
        "- âœ… Kein Spark-Solr Connector notwendig\n",
        "- âœ… **Paralleles Shard-Loading** (4x I/O Throughput)\n",
        "\n",
        "**Trade-offs:**\n",
        "- âš ï¸ Mehr Speicher in Solr (Rohdaten statt Aggregate)\n",
        "- âš ï¸ Import dauert lÃ¤nger (6M statt 1000 Dokumente)\n",
        "- âš ï¸ Streaming Expressions haben Lernkurve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AufrÃ¤umen\n",
        "spark.stop()\n",
        "print('\\nâœ… Spark Session beendet')\n",
        "print('\\nğŸ¯ Notebook abgeschlossen!')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
