{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NYC Taxi Data Analysis - Solr Cloud + Spark\n",
        "\n",
        "Dieses Notebook demonstriert **echte verteilte Datenverarbeitung** mit Data Locality.\n",
        "\n",
        "## Architektur\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                     Solr Cloud + Spark                          â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n",
        "â”‚   node1     â”‚   node2     â”‚   node3     â”‚   node4     â”‚         â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚\n",
        "â”‚  â”‚Shard 1â”‚  â”‚  â”‚Shard 2â”‚  â”‚  â”‚Shard 3â”‚  â”‚  â”‚Shard 4â”‚  â”‚         â”‚\n",
        "â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â”‚         â”‚\n",
        "â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚         â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”‚         â”‚\n",
        "â”‚  â”‚Spark  â”‚  â”‚  â”‚Spark  â”‚  â”‚  â”‚Spark  â”‚  â”‚  â”‚Spark  â”‚  â”‚         â”‚\n",
        "â”‚  â”‚Executorâ”‚ â”‚  â”‚Executorâ”‚ â”‚  â”‚Executorâ”‚ â”‚  â”‚Executorâ”‚ â”‚         â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚\n",
        "â”‚   LOKAL!    â”‚   LOKAL!    â”‚   LOKAL!    â”‚   LOKAL!    â”‚         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "## Parquet (Replikation) vs. Solr Cloud (Sharding)\n",
        "\n",
        "```\n",
        "Parquet (Replikation):                 Solr Cloud (Sharding):\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ node1  â”‚ node2  â”‚ node3  â”‚ node4  â”‚  â”‚ node1  â”‚ node2  â”‚ node3  â”‚ node4  â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚ 100%   â”‚ 100%   â”‚ 100%   â”‚ 100%   â”‚  â”‚  25%   â”‚  25%   â”‚  25%   â”‚  25%   â”‚\n",
        "â”‚ Daten  â”‚ Daten  â”‚ Daten  â”‚ Daten  â”‚  â”‚Shard 1 â”‚Shard 2 â”‚Shard 3 â”‚Shard 4 â”‚\n",
        "â”‚ (A+B)  â”‚ (A+B)  â”‚ (A+B)  â”‚ (A+B)  â”‚  â”‚  (A)   â”‚  (B)   â”‚  (C)   â”‚  (D)   â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "   400 MB total                           100 MB total\n",
        "```\n",
        "\n",
        "| Aspekt | Parquet (repliziert) | Solr Cloud (sharded) |\n",
        "|--------|---------------------|----------------------|\n",
        "| **Speicherung** | 100% auf jedem Node | 25% pro Node (4 Shards) |\n",
        "| **Speicherbedarf** | 4x Datenmenge | 1x Datenmenge |\n",
        "| **Beispiel 100 MB** | 400 MB gesamt | 100 MB gesamt |\n",
        "\n",
        "**Vorteile von Solr Cloud:**\n",
        "- âœ… **Echte Partitionierung**: Jeder Node speichert und verarbeitet nur seinen Teil\n",
        "- âœ… **Skalierbarkeit**: Bei 10 GB Daten brauchst du nicht 40 GB Platz\n",
        "- âœ… **Data Locality**: Spark Executors lesen von lokalem Solr Shard\n",
        "- âœ… **Filter Pushdown**: Solr filtert VOR der Uebertragung zu Spark\n",
        "- âœ… **Intelligentes Routing**: Queries werden zu den richtigen Shards geleitet\n",
        "- âœ… **Replikation optional**: `replicationFactor=2` fuer Ausfallsicherheit\n",
        "- âœ… **Inkrementelles Update**: Neue Daten werden automatisch auf Shards verteilt\n",
        "\n",
        "**Datenquelle:** [NYC Taxi & Limousine Commission](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Konfiguration\n",
        "# =============================================================================\n",
        "\n",
        "SOLR_NODES = ['node1', 'node2', 'node3', 'node4']\n",
        "SOLR_PORT = 8983\n",
        "ZK_HOSTS = 'node1:2181,node2:2181,node3:2181'\n",
        "COLLECTION = 'nyc-taxi'\n",
        "\n",
        "# Parquet-Dateien zum Laden\n",
        "PARQUET_FILES = [\n",
        "    ('yellow_tripdata_2023-01.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet'),\n",
        "    ('yellow_tripdata_2023-02.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet'),\n",
        "]\n",
        "\n",
        "DATA_DIR = '/data/spark/datasets/nyc-taxi'\n",
        "\n",
        "print('Konfiguration geladen.')\n",
        "print(f'Solr Collection: {COLLECTION}')\n",
        "print(f'ZooKeeper: {ZK_HOSTS}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schritt 1: Solr Collection erstellen\n",
        "\n",
        "Wir erstellen eine Collection mit **4 Shards** (ein Shard pro Node) fuer optimale Data Locality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "SOLR_URL = f'http://node1:{SOLR_PORT}/solr'\n",
        "\n",
        "def check_solr():\n",
        "    \"\"\"Pruefe ob Solr erreichbar ist.\"\"\"\n",
        "    try:\n",
        "        r = requests.get(f'{SOLR_URL}/admin/info/system', timeout=5)\n",
        "        return r.status_code == 200\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def collection_exists(name):\n",
        "    \"\"\"Pruefe ob Collection existiert.\"\"\"\n",
        "    r = requests.get(f'{SOLR_URL}/admin/collections?action=LIST')\n",
        "    data = r.json()\n",
        "    return name in data.get('collections', [])\n",
        "\n",
        "def create_collection(name, num_shards=4, replication_factor=1):\n",
        "    \"\"\"Erstelle Solr Collection.\"\"\"\n",
        "    params = {\n",
        "        'action': 'CREATE',\n",
        "        'name': name,\n",
        "        'numShards': num_shards,\n",
        "        'replicationFactor': replication_factor,\n",
        "        'collection.configName': '_default'\n",
        "    }\n",
        "    r = requests.get(f'{SOLR_URL}/admin/collections', params=params)\n",
        "    return r.json()\n",
        "\n",
        "def delete_collection(name):\n",
        "    \"\"\"Loesche Collection.\"\"\"\n",
        "    r = requests.get(f'{SOLR_URL}/admin/collections?action=DELETE&name={name}')\n",
        "    return r.json()\n",
        "\n",
        "# Verbindung pruefen\n",
        "if not check_solr():\n",
        "    print('âŒ FEHLER: Solr nicht erreichbar!')\n",
        "else:\n",
        "    print('âœ… Solr erreichbar')\n",
        "    \n",
        "    # Collection erstellen\n",
        "    if collection_exists(COLLECTION):\n",
        "        print(f'âš ï¸  Collection \"{COLLECTION}\" existiert bereits')\n",
        "        user_input = input('Loeschen und neu erstellen? (j/N): ')\n",
        "        if user_input.lower() == 'j':\n",
        "            print(f'ğŸ—‘ï¸  Loesche {COLLECTION}...')\n",
        "            delete_collection(COLLECTION)\n",
        "            print(f'ğŸ“¦ Erstelle {COLLECTION} mit 4 Shards...')\n",
        "            result = create_collection(COLLECTION, num_shards=4, replication_factor=1)\n",
        "            print('âœ… Collection erstellt')\n",
        "    else:\n",
        "        print(f'ğŸ“¦ Erstelle {COLLECTION} mit 4 Shards...')\n",
        "        result = create_collection(COLLECTION, num_shards=4, replication_factor=1)\n",
        "        print('âœ… Collection erstellt')\n",
        "\n",
        "# Shard-Verteilung anzeigen\n",
        "print('\\nğŸ“Š Shard-Verteilung:')\n",
        "r = requests.get(f'{SOLR_URL}/admin/collections?action=CLUSTERSTATUS&collection={COLLECTION}')\n",
        "cluster_state = r.json()\n",
        "if 'cluster' in cluster_state:\n",
        "    shards = cluster_state['cluster']['collections'].get(COLLECTION, {}).get('shards', {})\n",
        "    for shard_name, shard_info in shards.items():\n",
        "        replicas = shard_info.get('replicas', {})\n",
        "        for replica_name, replica_info in replicas.items():\n",
        "            node = replica_info.get('node_name', 'unknown')\n",
        "            state = replica_info.get('state', 'unknown')\n",
        "            print(f'  {shard_name}: {node} ({state})')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schritt 2: Schema definieren\n",
        "\n",
        "Wir definieren das Schema fuer die NYC Taxi Daten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Schema fuer NYC Taxi Daten\n",
        "schema_fields = [\n",
        "    {'name': 'VendorID', 'type': 'pint'},\n",
        "    {'name': 'tpep_pickup_datetime', 'type': 'pdate'},\n",
        "    {'name': 'tpep_dropoff_datetime', 'type': 'pdate'},\n",
        "    {'name': 'passenger_count', 'type': 'pint'},\n",
        "    {'name': 'trip_distance', 'type': 'pdouble'},\n",
        "    {'name': 'RatecodeID', 'type': 'pint'},\n",
        "    {'name': 'store_and_fwd_flag', 'type': 'string'},\n",
        "    {'name': 'PULocationID', 'type': 'pint'},\n",
        "    {'name': 'DOLocationID', 'type': 'pint'},\n",
        "    {'name': 'payment_type', 'type': 'pint'},\n",
        "    {'name': 'fare_amount', 'type': 'pdouble'},\n",
        "    {'name': 'extra', 'type': 'pdouble'},\n",
        "    {'name': 'mta_tax', 'type': 'pdouble'},\n",
        "    {'name': 'tip_amount', 'type': 'pdouble'},\n",
        "    {'name': 'tolls_amount', 'type': 'pdouble'},\n",
        "    {'name': 'improvement_surcharge', 'type': 'pdouble'},\n",
        "    {'name': 'total_amount', 'type': 'pdouble'},\n",
        "    {'name': 'congestion_surcharge', 'type': 'pdouble'},\n",
        "    {'name': 'airport_fee', 'type': 'pdouble'},\n",
        "    # Berechnete Felder fuer Analysen\n",
        "    {'name': 'pickup_hour', 'type': 'pint'},\n",
        "    {'name': 'pickup_weekday', 'type': 'pint'},\n",
        "]\n",
        "\n",
        "schema_url = f'{SOLR_URL}/{COLLECTION}/schema'\n",
        "\n",
        "print('ğŸ“‹ Definiere Schema...')\n",
        "for field in schema_fields:\n",
        "    payload = {'add-field': field}\n",
        "    r = requests.post(schema_url, json=payload)\n",
        "    if r.status_code == 200:\n",
        "        result = r.json()\n",
        "        if 'errors' not in result:\n",
        "            print(f'  âœ… {field[\"name\"]} ({field[\"type\"]})')\n",
        "        else:\n",
        "            # Feld existiert wahrscheinlich schon\n",
        "            print(f'  âš ï¸  {field[\"name\"]} (existiert bereits)')\n",
        "    else:\n",
        "        print(f'  âŒ {field[\"name\"]}: {r.text}')\n",
        "\n",
        "print('\\nâœ… Schema definiert')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schritt 3: Daten herunterladen und in Solr laden\n",
        "\n",
        "Wir laden die Parquet-Dateien und indexieren sie in Solr."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import pyarrow.parquet as pq\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "def download_if_missing(filename, url, dest_dir):\n",
        "    \"\"\"Download Datei falls nicht vorhanden.\"\"\"\n",
        "    filepath = os.path.join(dest_dir, filename)\n",
        "    if os.path.exists(filepath):\n",
        "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "        return filepath, size_mb, False\n",
        "    \n",
        "    os.makedirs(dest_dir, exist_ok=True)\n",
        "    print(f'  â¬‡ï¸  Downloading {filename}...')\n",
        "    urllib.request.urlretrieve(url, filepath)\n",
        "    size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "    return filepath, size_mb, True\n",
        "\n",
        "# Download\n",
        "print('=' * 60)\n",
        "print('DOWNLOAD NYC TAXI DATEN')\n",
        "print('=' * 60)\n",
        "\n",
        "local_files = []\n",
        "total_size = 0\n",
        "for filename, url in PARQUET_FILES:\n",
        "    filepath, size_mb, was_downloaded = download_if_missing(filename, url, DATA_DIR)\n",
        "    status = 'â¬‡ï¸  Downloaded' if was_downloaded else 'âœ… Vorhanden'\n",
        "    print(f'{status}: {filename} ({size_mb:.1f} MB)')\n",
        "    local_files.append(filepath)\n",
        "    total_size += size_mb\n",
        "\n",
        "print(f'\\nğŸ“Š Gesamt: {total_size:.1f} MB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Daten in Solr laden (Batch-Import)\n",
        "# =============================================================================\n",
        "\n",
        "BATCH_SIZE = 10000  # Dokumente pro Batch\n",
        "\n",
        "def convert_to_solr_doc(row, doc_id):\n",
        "    \"\"\"Konvertiere Parquet-Zeile zu Solr-Dokument.\"\"\"\n",
        "    doc = {'id': str(doc_id)}\n",
        "    \n",
        "    for col in row.index:\n",
        "        val = row[col]\n",
        "        if pd.isna(val):\n",
        "            continue\n",
        "            \n",
        "        # Spaltennamen normalisieren\n",
        "        col_name = col.replace('Airport_fee', 'airport_fee')\n",
        "        \n",
        "        # Datetime-Felder konvertieren\n",
        "        if 'datetime' in col_name.lower():\n",
        "            if hasattr(val, 'isoformat'):\n",
        "                doc[col_name] = val.isoformat() + 'Z'\n",
        "            else:\n",
        "                doc[col_name] = str(val)\n",
        "            \n",
        "            # Berechnete Felder hinzufuegen\n",
        "            if 'pickup' in col_name.lower() and hasattr(val, 'hour'):\n",
        "                doc['pickup_hour'] = val.hour\n",
        "                doc['pickup_weekday'] = val.weekday()\n",
        "        else:\n",
        "            # Numerische Werte\n",
        "            if isinstance(val, (int, float)):\n",
        "                doc[col_name] = float(val) if isinstance(val, float) else int(val)\n",
        "            else:\n",
        "                doc[col_name] = str(val)\n",
        "    \n",
        "    return doc\n",
        "\n",
        "def index_parquet_to_solr(filepath, collection, start_id=0):\n",
        "    \"\"\"Indexiere Parquet-Datei in Solr.\"\"\"\n",
        "    filename = os.path.basename(filepath)\n",
        "    print(f'\\nğŸ“– Lade {filename}...')\n",
        "    \n",
        "    # Parquet lesen\n",
        "    df = pd.read_parquet(filepath)\n",
        "    total_rows = len(df)\n",
        "    print(f'   Zeilen: {total_rows:,}')\n",
        "    \n",
        "    # Batch-Upload\n",
        "    update_url = f'{SOLR_URL}/{collection}/update?commit=true'\n",
        "    indexed = 0\n",
        "    errors = 0\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for batch_start in range(0, total_rows, BATCH_SIZE):\n",
        "        batch_end = min(batch_start + BATCH_SIZE, total_rows)\n",
        "        batch_df = df.iloc[batch_start:batch_end]\n",
        "        \n",
        "        # Konvertiere zu Solr-Dokumenten\n",
        "        docs = []\n",
        "        for idx, row in batch_df.iterrows():\n",
        "            doc_id = start_id + batch_start + len(docs)\n",
        "            doc = convert_to_solr_doc(row, doc_id)\n",
        "            docs.append(doc)\n",
        "        \n",
        "        # Upload zu Solr\n",
        "        try:\n",
        "            r = requests.post(update_url, json=docs, timeout=120)\n",
        "            if r.status_code == 200:\n",
        "                indexed += len(docs)\n",
        "            else:\n",
        "                errors += len(docs)\n",
        "                print(f'   âŒ Batch-Fehler: {r.text[:100]}')\n",
        "        except Exception as e:\n",
        "            errors += len(docs)\n",
        "            print(f'   âŒ Exception: {e}')\n",
        "        \n",
        "        # Fortschritt\n",
        "        pct = (batch_end / total_rows) * 100\n",
        "        elapsed = time.time() - start_time\n",
        "        rate = indexed / elapsed if elapsed > 0 else 0\n",
        "        print(f'   â³ {batch_end:,}/{total_rows:,} ({pct:.1f}%) - {rate:,.0f} docs/sec', end='\\r')\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    print(f'\\n   âœ… {indexed:,} indexiert in {elapsed:.1f}s ({indexed/elapsed:,.0f} docs/sec)')\n",
        "    \n",
        "    return indexed, start_id + total_rows\n",
        "\n",
        "# Alle Dateien indexieren\n",
        "print('=' * 60)\n",
        "print('INDEXIERUNG IN SOLR')\n",
        "print('=' * 60)\n",
        "\n",
        "total_indexed = 0\n",
        "next_id = 0\n",
        "total_start = time.time()\n",
        "\n",
        "for filepath in local_files:\n",
        "    indexed, next_id = index_parquet_to_solr(filepath, COLLECTION, next_id)\n",
        "    total_indexed += indexed\n",
        "\n",
        "total_time = time.time() - total_start\n",
        "\n",
        "print('\\n' + '=' * 60)\n",
        "print(f'âœ… FERTIG!')\n",
        "print(f'   Dokumente indexiert: {total_indexed:,}')\n",
        "print(f'   Gesamtzeit: {total_time:.1f}s')\n",
        "print(f'   Durchsatz: {total_indexed/total_time:,.0f} docs/sec')\n",
        "print('=' * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schritt 4: Solr Collection verifizieren"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collection-Statistiken\n",
        "print('ğŸ“Š COLLECTION STATISTIKEN')\n",
        "print('=' * 60)\n",
        "\n",
        "# Dokument-Anzahl\n",
        "r = requests.get(f'{SOLR_URL}/{COLLECTION}/select?q=*:*&rows=0')\n",
        "data = r.json()\n",
        "num_docs = data['response']['numFound']\n",
        "print(f'Dokumente: {num_docs:,}')\n",
        "\n",
        "# Shard-Statistiken\n",
        "print('\\nDokumente pro Shard:')\n",
        "for i, node in enumerate(SOLR_NODES):\n",
        "    shard_name = f'shard{i+1}'\n",
        "    try:\n",
        "        r = requests.get(f'http://{node}:{SOLR_PORT}/solr/{COLLECTION}/select?q=*:*&rows=0&shards={shard_name}')\n",
        "        if r.status_code == 200:\n",
        "            shard_docs = r.json()['response']['numFound']\n",
        "            pct = (shard_docs / num_docs) * 100 if num_docs > 0 else 0\n",
        "            print(f'  {shard_name} ({node}): {shard_docs:,} ({pct:.1f}%)')\n",
        "    except Exception as e:\n",
        "        print(f'  {shard_name} ({node}): Fehler - {e}')\n",
        "\n",
        "# Beispiel-Dokument\n",
        "print('\\nBeispiel-Dokument:')\n",
        "r = requests.get(f'{SOLR_URL}/{COLLECTION}/select?q=*:*&rows=1&fl=*')\n",
        "if r.status_code == 200:\n",
        "    docs = r.json()['response']['docs']\n",
        "    if docs:\n",
        "        for key, val in list(docs[0].items())[:10]:\n",
        "            print(f'  {key}: {val}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Schritt 5: Spark Session mit Solr-Connector starten\n",
        "\n",
        "Jetzt kommt der Vorteil: Spark liest direkt von den lokalen Solr Shards!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "import time\n",
        "\n",
        "print('ğŸš€ Starte Spark Session mit Solr-Connector...')\n",
        "start_time = time.time()\n",
        "\n",
        "# Spark-Solr Connector Konfiguration\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('NYC Taxi - Solr Cloud Integration') \\\n",
        "    .master('spark://node1.cloud.local:7077') \\\n",
        "    .config('spark.jars.packages', 'com.lucidworks.spark:spark-solr:4.0.4') \\\n",
        "    .config('spark.executor.cores', '2') \\\n",
        "    .config('spark.executor.memory', '6g') \\\n",
        "    .config('spark.executor.memoryOverhead', '1g') \\\n",
        "    .config('spark.driver.memory', '4g') \\\n",
        "    .config('spark.sql.shuffle.partitions', '48') \\\n",
        "    .config('spark.default.parallelism', '48') \\\n",
        "    .config('spark.sql.adaptive.enabled', 'true') \\\n",
        "    .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "sc = spark.sparkContext\n",
        "\n",
        "print(f'\\n{\"=\" * 60}')\n",
        "print('SPARK CLUSTER INFO')\n",
        "print(f'{\"=\" * 60}')\n",
        "print(f'Spark Version:      {spark.version}')\n",
        "print(f'Application ID:     {sc.applicationId}')\n",
        "print(f'Master:             {sc.master}')\n",
        "print(f'Default Parallelism: {sc.defaultParallelism}')\n",
        "print(f'Startup Zeit:       {elapsed:.2f}s')\n",
        "print(f'{\"=\" * 60}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schritt 6: Daten von Solr laden (Node-lokal!)\n",
        "\n",
        "Der Spark-Solr Connector liest **parallel von jedem Shard** - mit Data Locality!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Daten von Solr Cloud laden\n",
        "# =============================================================================\n",
        "\n",
        "print('ğŸ“– Lade Daten von Solr Cloud (parallel von allen Shards)...')\n",
        "start_time = time.time()\n",
        "\n",
        "# Spark-Solr Connector nutzt Data Locality!\n",
        "# Jeder Executor liest von seinem lokalen Shard\n",
        "df = spark.read.format('solr') \\\n",
        "    .option('zkhost', ZK_HOSTS) \\\n",
        "    .option('collection', COLLECTION) \\\n",
        "    .option('splits', 'true') \\\n",
        "    .option('split_field', '_route_') \\\n",
        "    .option('rows', '10000') \\\n",
        "    .load()\n",
        "\n",
        "# Cache fuer wiederholte Abfragen\n",
        "df.cache()\n",
        "\n",
        "# Triggere Laden und zaehle\n",
        "load_start = time.time()\n",
        "total_rows = df.count()\n",
        "load_time = time.time() - load_start\n",
        "\n",
        "num_partitions = df.rdd.getNumPartitions()\n",
        "\n",
        "print(f'\\n{\"=\" * 60}')\n",
        "print('ğŸ“Š LADE-STATISTIKEN (Solr Cloud)')\n",
        "print(f'{\"=\" * 60}')\n",
        "print(f'Geladene Datensaetze:  {total_rows:,}')\n",
        "print(f'Partitionen:           {num_partitions}')\n",
        "print(f'Ladezeit:              {load_time:.2f}s')\n",
        "print(f'Throughput:            {total_rows / load_time:,.0f} Zeilen/Sekunde')\n",
        "print(f'{\"=\" * 60}')\n",
        "print('\\nâœ… Vergleiche mit Parquet-Ansatz - Data Locality macht den Unterschied!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Schema anzeigen\n",
        "print('ğŸ“‹ SCHEMA')\n",
        "print('=' * 60)\n",
        "df.printSchema()\n",
        "\n",
        "print('\\nğŸ“‹ ERSTE 5 ZEILEN')\n",
        "df.select('id', 'tpep_pickup_datetime', 'trip_distance', 'fare_amount', 'tip_amount', 'total_amount').show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schritt 7: Analysen mit Filter-Pushdown\n",
        "\n",
        "Bei Solr werden Filter **VOR** der Datenuebertragung angewendet - noch effizienter!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Aggregation 1: Gesamtstatistiken\n",
        "# =============================================================================\n",
        "\n",
        "print('ğŸ“Š GESAMTSTATISTIKEN')\n",
        "print('=' * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Filter: Nur gueltige Fahrten\n",
        "df_clean = df.filter(\n",
        "    (F.col('trip_distance') > 0) &\n",
        "    (F.col('trip_distance') < 100) &\n",
        "    (F.col('fare_amount') > 0) &\n",
        "    (F.col('fare_amount') < 500) &\n",
        "    (F.col('passenger_count') > 0) &\n",
        "    (F.col('passenger_count') <= 6)\n",
        ")\n",
        "\n",
        "stats = df_clean.agg(\n",
        "    F.count('*').alias('total_trips'),\n",
        "    F.sum('fare_amount').alias('total_fares'),\n",
        "    F.sum('tip_amount').alias('total_tips'),\n",
        "    F.sum('total_amount').alias('total_revenue'),\n",
        "    F.avg('trip_distance').alias('avg_distance'),\n",
        "    F.avg('fare_amount').alias('avg_fare'),\n",
        "    F.avg('tip_amount').alias('avg_tip'),\n",
        "    F.avg('total_amount').alias('avg_total'),\n",
        "    F.avg('passenger_count').alias('avg_passengers')\n",
        ").collect()[0]\n",
        "\n",
        "query_time = time.time() - start_time\n",
        "\n",
        "print(f'\\nğŸš• Anzahl Fahrten:           {stats.total_trips:>15,}')\n",
        "print(f'ğŸ’° Gesamtumsatz:             ${stats.total_revenue:>14,.2f}')\n",
        "print(f'   - Fahrpreise:             ${stats.total_fares:>14,.2f}')\n",
        "print(f'   - Trinkgeld:              ${stats.total_tips:>14,.2f}')\n",
        "print(f'\\nğŸ“Š Durchschnittswerte pro Fahrt:')\n",
        "print(f'   - Distanz:                {stats.avg_distance:>15.2f} Meilen')\n",
        "print(f'   - Fahrpreis:              ${stats.avg_fare:>14.2f}')\n",
        "print(f'   - Trinkgeld:              ${stats.avg_tip:>14.2f}')\n",
        "print(f'   - Gesamtbetrag:           ${stats.avg_total:>14.2f}')\n",
        "print(f'   - Passagiere:             {stats.avg_passengers:>15.1f}')\n",
        "print(f'\\nâ±ï¸  Query-Zeit: {query_time:.2f}s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Aggregation 2: Analyse nach Stunde (nutzt berechnetes Feld aus Solr)\n",
        "# =============================================================================\n",
        "\n",
        "print('ğŸ• FAHRTEN PRO STUNDE')\n",
        "print('=' * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Nutze das in Solr vorberechnete pickup_hour Feld\n",
        "trips_by_hour = df_clean \\\n",
        "    .groupBy('pickup_hour') \\\n",
        "    .agg(\n",
        "        F.count('*').alias('trips'),\n",
        "        F.sum('total_amount').alias('revenue'),\n",
        "        F.avg('total_amount').alias('avg_amount'),\n",
        "        F.avg('tip_amount').alias('avg_tip')\n",
        "    ) \\\n",
        "    .orderBy('pickup_hour') \\\n",
        "    .collect()\n",
        "\n",
        "query_time = time.time() - start_time\n",
        "\n",
        "print(f'\\n{\"Std\":>4} {\"Fahrten\":>12} {\"Umsatz\":>14} {\"Ã˜ Betrag\":>10} {\"Ã˜ Tip\":>8}')\n",
        "print('-' * 52)\n",
        "\n",
        "for row in trips_by_hour:\n",
        "    if row.pickup_hour is not None:\n",
        "        print(f'{row.pickup_hour:>4} {row.trips:>12,} ${row.revenue:>12,.0f} ${row.avg_amount:>9.2f} ${row.avg_tip:>6.2f}')\n",
        "\n",
        "print(f'\\nâ±ï¸  Query-Zeit: {query_time:.3f}s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Aggregation 3: Top Locations\n",
        "# =============================================================================\n",
        "\n",
        "print('ğŸ“ TOP 10 PICKUP LOCATIONS')\n",
        "print('=' * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "top_locations = df_clean \\\n",
        "    .groupBy('PULocationID') \\\n",
        "    .agg(\n",
        "        F.count('*').alias('trips'),\n",
        "        F.sum('total_amount').alias('revenue'),\n",
        "        F.avg('trip_distance').alias('avg_distance')\n",
        "    ) \\\n",
        "    .orderBy(F.desc('trips')) \\\n",
        "    .limit(10) \\\n",
        "    .collect()\n",
        "\n",
        "query_time = time.time() - start_time\n",
        "\n",
        "print(f'\\n{\"Location\":>10} {\"Fahrten\":>12} {\"Umsatz\":>14} {\"Ã˜ Distanz\":>12}')\n",
        "print('-' * 52)\n",
        "\n",
        "for row in top_locations:\n",
        "    print(f'{int(row.PULocationID):>10} {row.trips:>12,} ${row.revenue:>12,.0f} {row.avg_distance:>11.2f}mi')\n",
        "\n",
        "print(f'\\nâ±ï¸  Query-Zeit: {query_time:.3f}s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Bonus: Solr Filter-Pushdown Demo\n",
        "# =============================================================================\n",
        "\n",
        "print('ğŸ” FILTER-PUSHDOWN DEMO')\n",
        "print('=' * 60)\n",
        "print('\\nDirekte Solr-Query (wird VOR Spark gefiltert):\\n')\n",
        "\n",
        "# Lade nur Daten mit hohem Trinkgeld - Solr filtert, nicht Spark!\n",
        "start_time = time.time()\n",
        "\n",
        "df_high_tips = spark.read.format('solr') \\\n",
        "    .option('zkhost', ZK_HOSTS) \\\n",
        "    .option('collection', COLLECTION) \\\n",
        "    .option('solr.params', 'fq=tip_amount:[20 TO *]&fq=trip_distance:[0 TO 10]') \\\n",
        "    .load()\n",
        "\n",
        "# Nur gefilterte Daten werden uebertragen!\n",
        "count = df_high_tips.count()\n",
        "query_time = time.time() - start_time\n",
        "\n",
        "print(f'Fahrten mit Tip >= $20 und Distanz <= 10mi: {count:,}')\n",
        "print(f'Query-Zeit: {query_time:.2f}s')\n",
        "print('\\nâœ… Nur diese Zeilen wurden ueber das Netzwerk uebertragen!')\n",
        "\n",
        "if count > 0:\n",
        "    print('\\nBeispiele:')\n",
        "    df_high_tips.select('trip_distance', 'fare_amount', 'tip_amount', 'total_amount').show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Spark Session beenden\n",
        "spark.stop()\n",
        "print('âœ… Spark Session beendet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Zusammenfassung\n",
        "\n",
        "| Metrik | Parquet (Driver-Read) | Solr Cloud |\n",
        "|--------|----------------------|------------|\n",
        "| Data Locality | âŒ Nein | âœ… Ja |\n",
        "| Paralleles Lesen | âŒ Nur Driver | âœ… Alle Nodes |\n",
        "| Filter-Pushdown | âŒ Spark-seitig | âœ… Solr-seitig |\n",
        "| Netzwerk-Traffic | ğŸ”´ Hoch | ğŸŸ¢ Minimal |\n",
        "| Skalierbarkeit | ğŸŸ¡ Limitiert | ğŸŸ¢ Linear |\n",
        "\n",
        "**Fazit:** Fuer verteilte Verarbeitung ohne HDFS ist Solr Cloud eine exzellente Alternative!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
