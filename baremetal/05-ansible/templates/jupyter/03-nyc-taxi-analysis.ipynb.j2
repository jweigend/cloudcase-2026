{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NYC Taxi Data Analysis - Distributed Processing\n",
        "\n",
        "Dieses Notebook demonstriert **echte verteilte Datenverarbeitung** mit Apache Spark.\n",
        "\n",
        "## Architektur\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                     Spark Cluster                               â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n",
        "â”‚   node1     â”‚   node2     â”‚   node3     â”‚   node4     â”‚         â”‚\n",
        "â”‚   Worker    â”‚   Worker    â”‚   Worker    â”‚   Worker    â”‚         â”‚\n",
        "â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚\n",
        "â”‚ â”‚Parquet 1â”‚ â”‚ â”‚Parquet 2â”‚ â”‚ â”‚Parquet 1â”‚ â”‚ â”‚Parquet 2â”‚ â”‚         â”‚\n",
        "â”‚ â”‚ (lokal) â”‚ â”‚ â”‚ (lokal) â”‚ â”‚ â”‚ (lokal) â”‚ â”‚ â”‚ (lokal) â”‚ â”‚         â”‚\n",
        "â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                              â”‚\n",
        "                    spark.read.parquet()\n",
        "                    (parallel, data locality)\n",
        "```\n",
        "\n",
        "**Datenquelle:** [NYC Taxi & Limousine Commission](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n",
        "\n",
        "**Cluster:**\n",
        "- Spark Master: `spark://node1.cloud.local:7077`\n",
        "- Workers: node1, node2, node3, node4\n",
        "- Daten: Parquet-Dateien auf jedem Node unter `/data/spark/datasets/nyc-taxi/`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Schritt 1: Daten auf alle Nodes verteilen\n",
        "# =============================================================================\n",
        "# Dieser Schritt muss nur EINMAL ausgefuehrt werden!\n",
        "# Die Daten werden auf node0 heruntergeladen und per SSH auf alle Worker kopiert.\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "DATA_DIR = '/data/spark/datasets/nyc-taxi'\n",
        "NODES = ['node1', 'node2', 'node3', 'node4']\n",
        "\n",
        "# Parquet-Dateien (NYC TLC Yellow Taxi)\n",
        "PARQUET_FILES = [\n",
        "    ('yellow_tripdata_2023-01.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet'),\n",
        "    ('yellow_tripdata_2023-02.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet'),\n",
        "]\n",
        "\n",
        "def download_if_missing(filename, url, dest_dir):\n",
        "    \"\"\"Download Datei falls nicht vorhanden.\"\"\"\n",
        "    filepath = os.path.join(dest_dir, filename)\n",
        "    if os.path.exists(filepath):\n",
        "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "        return filepath, size_mb, False  # already exists\n",
        "    \n",
        "    os.makedirs(dest_dir, exist_ok=True)\n",
        "    urllib.request.urlretrieve(url, filepath)\n",
        "    size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "    return filepath, size_mb, True  # downloaded\n",
        "\n",
        "def copy_to_nodes(files, nodes, remote_dir):\n",
        "    \"\"\"Kopiere Dateien per SSH auf alle Worker-Nodes.\"\"\"\n",
        "    for node in nodes:\n",
        "        print(f'\\nðŸ“¦ Kopiere nach {node}...')\n",
        "        # Verzeichnis erstellen\n",
        "        subprocess.run(['ssh', node, f'mkdir -p {remote_dir}'], check=True)\n",
        "        \n",
        "        for local_file in files:\n",
        "            filename = os.path.basename(local_file)\n",
        "            remote_file = f'{remote_dir}/{filename}'\n",
        "            \n",
        "            # Pruefen ob Datei schon existiert\n",
        "            result = subprocess.run(\n",
        "                ['ssh', node, f'test -f {remote_file} && stat -c%s {remote_file}'],\n",
        "                capture_output=True, text=True\n",
        "            )\n",
        "            \n",
        "            local_size = os.path.getsize(local_file)\n",
        "            if result.returncode == 0 and result.stdout.strip() == str(local_size):\n",
        "                print(f'  âœ“ {filename} bereits vorhanden')\n",
        "            else:\n",
        "                print(f'  â†’ Kopiere {filename}...')\n",
        "                subprocess.run(['scp', local_file, f'{node}:{remote_file}'], check=True)\n",
        "                print(f'  âœ“ {filename} kopiert')\n",
        "\n",
        "# Download\n",
        "print('=' * 60)\n",
        "print('SCHRITT 1: Download der NYC Taxi Daten')\n",
        "print('=' * 60)\n",
        "\n",
        "local_files = []\n",
        "total_size = 0\n",
        "for filename, url in PARQUET_FILES:\n",
        "    filepath, size_mb, was_downloaded = download_if_missing(filename, url, DATA_DIR)\n",
        "    status = 'â¬‡ï¸  Downloaded' if was_downloaded else 'âœ“ Vorhanden'\n",
        "    print(f'{status}: {filename} ({size_mb:.1f} MB)')\n",
        "    local_files.append(filepath)\n",
        "    total_size += size_mb\n",
        "\n",
        "print(f'\\nðŸ“Š Gesamt: {total_size:.1f} MB')\n",
        "\n",
        "# Auf alle Nodes verteilen\n",
        "print('\\n' + '=' * 60)\n",
        "print('SCHRITT 2: Verteilen auf Worker-Nodes')\n",
        "print('=' * 60)\n",
        "\n",
        "copy_to_nodes(local_files, NODES, DATA_DIR)\n",
        "\n",
        "print('\\n' + '=' * 60)\n",
        "print('âœ… Daten auf allen Nodes bereit!')\n",
        "print('=' * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Schritt 2: Spark Session starten\n",
        "# =============================================================================\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "import time\n",
        "\n",
        "print('ðŸš€ Starte Spark Session...')\n",
        "start_time = time.time()\n",
        "\n",
        "# Cluster: 4 Worker x 6 Cores x 24GB\n",
        "# Strategie: 3 Executors pro Worker (6GB RAM, 2 Cores je Executor)\n",
        "# Gesamt: 12 Executors, 72GB RAM, 24 Cores\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('NYC Taxi Analysis - Distributed') \\\n",
        "    .master('spark://node1.cloud.local:7077') \\\n",
        "    .config('spark.executor.cores', '2') \\\n",
        "    .config('spark.executor.memory', '6g') \\\n",
        "    .config('spark.executor.memoryOverhead', '1g') \\\n",
        "    .config('spark.driver.memory', '4g') \\\n",
        "    .config('spark.driver.cores', '2') \\\n",
        "    .config('spark.sql.shuffle.partitions', '48') \\\n",
        "    .config('spark.default.parallelism', '48') \\\n",
        "    .config('spark.memory.fraction', '0.6') \\\n",
        "    .config('spark.memory.storageFraction', '0.5') \\\n",
        "    .config('spark.sql.adaptive.enabled', 'true') \\\n",
        "    .config('spark.sql.adaptive.coalescePartitions.enabled', 'true') \\\n",
        "    .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Cluster-Statistiken\n",
        "sc = spark.sparkContext\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "print(f'\\n{\"=\" * 60}')\n",
        "print('SPARK CLUSTER INFO')\n",
        "print(f'{\"=\" * 60}')\n",
        "print(f'Spark Version:      {spark.version}')\n",
        "print(f'Application ID:     {sc.applicationId}')\n",
        "print(f'Master:             {sc.master}')\n",
        "print(f'Default Parallelism: {sc.defaultParallelism}')\n",
        "print(f'Startup Zeit:       {elapsed:.2f}s')\n",
        "print(f'{\"=\" * 60}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Schritt 3: Parquet-Daten parallel laden\n",
        "# =============================================================================\n",
        "# Spark liest die Parquet-Dateien parallel von allen Nodes!\n",
        "# Jeder Worker liest seine lokale Kopie (Data Locality).\n",
        "\n",
        "DATA_PATH = '/data/spark/datasets/nyc-taxi/*.parquet'\n",
        "\n",
        "print('ðŸ“– Lade Parquet-Daten von allen Nodes parallel...')\n",
        "start_time = time.time()\n",
        "\n",
        "# Explizites Schema (NYC TLC aendert manchmal Datentypen/Spaltennamen zwischen Monaten)\n",
        "taxi_schema = StructType([\n",
        "    StructField('VendorID', LongType(), True),\n",
        "    StructField('tpep_pickup_datetime', TimestampType(), True),\n",
        "    StructField('tpep_dropoff_datetime', TimestampType(), True),\n",
        "    StructField('passenger_count', DoubleType(), True),\n",
        "    StructField('trip_distance', DoubleType(), True),\n",
        "    StructField('RatecodeID', DoubleType(), True),\n",
        "    StructField('store_and_fwd_flag', StringType(), True),\n",
        "    StructField('PULocationID', LongType(), True),\n",
        "    StructField('DOLocationID', LongType(), True),\n",
        "    StructField('payment_type', LongType(), True),\n",
        "    StructField('fare_amount', DoubleType(), True),\n",
        "    StructField('extra', DoubleType(), True),\n",
        "    StructField('mta_tax', DoubleType(), True),\n",
        "    StructField('tip_amount', DoubleType(), True),\n",
        "    StructField('tolls_amount', DoubleType(), True),\n",
        "    StructField('improvement_surcharge', DoubleType(), True),\n",
        "    StructField('total_amount', DoubleType(), True),\n",
        "    StructField('congestion_surcharge', DoubleType(), True),\n",
        "    StructField('airport_fee', DoubleType(), True),\n",
        "])\n",
        "\n",
        "# Lade jede Datei einzeln und vereinige sie (wegen Schema-Unterschieden)\n",
        "import glob\n",
        "from functools import reduce\n",
        "\n",
        "# Lade Dateien mit Schema-Anpassung\n",
        "df1 = spark.read.parquet('/data/spark/datasets/nyc-taxi/yellow_tripdata_2023-01.parquet')\n",
        "df2 = spark.read.parquet('/data/spark/datasets/nyc-taxi/yellow_tripdata_2023-02.parquet')\n",
        "\n",
        "# Spalten normalisieren (Airport_fee -> airport_fee)\n",
        "if 'Airport_fee' in df2.columns:\n",
        "    df2 = df2.withColumnRenamed('Airport_fee', 'airport_fee')\n",
        "\n",
        "# Gemeinsame Spalten finden und casten\n",
        "common_cols = list(set(df1.columns) & set(df2.columns))\n",
        "\n",
        "# Beide DataFrames auf gleiche Spalten und Typen bringen\n",
        "df1_selected = df1.select([F.col(c).cast('double') if c not in ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'store_and_fwd_flag'] else F.col(c) for c in common_cols])\n",
        "df2_selected = df2.select([F.col(c).cast('double') if c not in ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'store_and_fwd_flag'] else F.col(c) for c in common_cols])\n",
        "\n",
        "# Union\n",
        "df = df1_selected.union(df2_selected)\n",
        "\n",
        "# Cache fuer wiederholte Abfragen\n",
        "df.cache()\n",
        "\n",
        "# Statistiken sammeln (triggert das Laden)\n",
        "load_start = time.time()\n",
        "total_rows = df.count()\n",
        "load_time = time.time() - load_start\n",
        "\n",
        "num_partitions = df.rdd.getNumPartitions()\n",
        "num_columns = len(df.columns)\n",
        "\n",
        "print(f'\\n{\"=\" * 60}')\n",
        "print('ðŸ“Š LADE-STATISTIKEN')\n",
        "print(f'{\"=\" * 60}')\n",
        "print(f'Geladene Datensaetze:  {total_rows:,}')\n",
        "print(f'Anzahl Spalten:        {num_columns}')\n",
        "print(f'Partitionen:           {num_partitions}')\n",
        "print(f'Ladezeit:              {load_time:.2f}s')\n",
        "print(f'Throughput:            {total_rows / load_time:,.0f} Zeilen/Sekunde')\n",
        "print(f'{\"=\" * 60}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Schritt 4: Schema und Datenvorschau\n",
        "# =============================================================================\n",
        "\n",
        "print('ðŸ“‹ SCHEMA')\n",
        "print('=' * 60)\n",
        "df.printSchema()\n",
        "\n",
        "print('\\nðŸ“‹ ERSTE 5 ZEILEN')\n",
        "print('=' * 60)\n",
        "df.show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Schritt 5: Datenqualitaet pruefen\n",
        "# =============================================================================\n",
        "\n",
        "print('ðŸ” DATENQUALITAET')\n",
        "print('=' * 60)\n",
        "\n",
        "# NULL-Werte zaehlen\n",
        "null_counts = df.select([\n",
        "    F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c)\n",
        "    for c in ['passenger_count', 'trip_distance', 'fare_amount', 'tip_amount', 'total_amount']\n",
        "]).collect()[0]\n",
        "\n",
        "print('\\nNULL-Werte pro Spalte:')\n",
        "for col in ['passenger_count', 'trip_distance', 'fare_amount', 'tip_amount', 'total_amount']:\n",
        "    null_count = null_counts[col]\n",
        "    pct = (null_count / total_rows) * 100\n",
        "    print(f'  {col:20s}: {null_count:>10,} ({pct:.2f}%)')\n",
        "\n",
        "# Wertebereich pruefen\n",
        "print('\\nWertebereiche:')\n",
        "stats = df.select(\n",
        "    F.min('trip_distance').alias('min_dist'),\n",
        "    F.max('trip_distance').alias('max_dist'),\n",
        "    F.min('fare_amount').alias('min_fare'),\n",
        "    F.max('fare_amount').alias('max_fare'),\n",
        "    F.min('tpep_pickup_datetime').alias('min_date'),\n",
        "    F.max('tpep_pickup_datetime').alias('max_date')\n",
        ").collect()[0]\n",
        "\n",
        "print(f'  Distanz:     {stats.min_dist:.2f} - {stats.max_dist:.2f} Meilen')\n",
        "print(f'  Fahrpreis:   ${stats.min_fare:.2f} - ${stats.max_fare:.2f}')\n",
        "print(f'  Zeitraum:    {stats.min_date} bis {stats.max_date}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Schritt 6: Daten bereinigen\n",
        "# =============================================================================\n",
        "\n",
        "print('ðŸ§¹ DATENBEREINIGUNG')\n",
        "print('=' * 60)\n",
        "\n",
        "rows_before = total_rows\n",
        "\n",
        "# Nur sinnvolle Fahrten behalten\n",
        "df_clean = df.filter(\n",
        "    (F.col('trip_distance') > 0) &\n",
        "    (F.col('trip_distance') < 100) &  # Max 100 Meilen\n",
        "    (F.col('fare_amount') > 0) &\n",
        "    (F.col('fare_amount') < 500) &    # Max $500\n",
        "    (F.col('passenger_count') > 0) &\n",
        "    (F.col('passenger_count') <= 6)   # Max 6 Passagiere\n",
        ")\n",
        "\n",
        "df_clean.cache()\n",
        "rows_after = df_clean.count()\n",
        "rows_removed = rows_before - rows_after\n",
        "\n",
        "print(f'Zeilen vorher:    {rows_before:,}')\n",
        "print(f'Zeilen nachher:   {rows_after:,}')\n",
        "print(f'Entfernt:         {rows_removed:,} ({rows_removed/rows_before*100:.2f}%)')\n",
        "\n",
        "# Ab jetzt mit bereinigten Daten arbeiten\n",
        "df = df_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Aggregation 1: Gesamtstatistiken\n",
        "# =============================================================================\n",
        "\n",
        "print('ðŸ“Š GESAMTSTATISTIKEN')\n",
        "print('=' * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "stats = df.agg(\n",
        "    F.count('*').alias('total_trips'),\n",
        "    F.sum('fare_amount').alias('total_fares'),\n",
        "    F.sum('tip_amount').alias('total_tips'),\n",
        "    F.sum('total_amount').alias('total_revenue'),\n",
        "    F.avg('trip_distance').alias('avg_distance'),\n",
        "    F.avg('fare_amount').alias('avg_fare'),\n",
        "    F.avg('tip_amount').alias('avg_tip'),\n",
        "    F.avg('total_amount').alias('avg_total'),\n",
        "    F.avg('passenger_count').alias('avg_passengers'),\n",
        "    F.sum('trip_distance').alias('total_distance')\n",
        ").collect()[0]\n",
        "\n",
        "query_time = time.time() - start_time\n",
        "\n",
        "print(f'\\nðŸš• Anzahl Fahrten:           {stats.total_trips:>15,}')\n",
        "print(f'ðŸ“ Gesamtdistanz:            {stats.total_distance:>15,.0f} Meilen')\n",
        "print(f'ðŸ’° Gesamtumsatz:             ${stats.total_revenue:>14,.2f}')\n",
        "print(f'   - Fahrpreise:             ${stats.total_fares:>14,.2f}')\n",
        "print(f'   - Trinkgeld:              ${stats.total_tips:>14,.2f}')\n",
        "print(f'\\nðŸ“Š Durchschnittswerte pro Fahrt:')\n",
        "print(f'   - Distanz:                {stats.avg_distance:>15.2f} Meilen')\n",
        "print(f'   - Fahrpreis:              ${stats.avg_fare:>14.2f}')\n",
        "print(f'   - Trinkgeld:              ${stats.avg_tip:>14.2f}')\n",
        "print(f'   - Gesamtbetrag:           ${stats.avg_total:>14.2f}')\n",
        "print(f'   - Passagiere:             {stats.avg_passengers:>15.1f}')\n",
        "print(f'\\nâ±ï¸  Query-Zeit: {query_time:.2f}s')\n",
        "print(f'ðŸ“ˆ Verarbeitete Zeilen/Sek: {stats.total_trips / query_time:,.0f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Aggregation 2: Fahrten pro Wochentag\n",
        "# =============================================================================\n",
        "\n",
        "print('ðŸ“… FAHRTEN PRO WOCHENTAG')\n",
        "print('=' * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "weekdays = ['So', 'Mo', 'Di', 'Mi', 'Do', 'Fr', 'Sa']  # Spark: 1=So, 7=Sa\n",
        "\n",
        "trips_by_weekday = df \\\n",
        "    .withColumn('weekday', F.dayofweek('tpep_pickup_datetime')) \\\n",
        "    .groupBy('weekday') \\\n",
        "    .agg(\n",
        "        F.count('*').alias('trips'),\n",
        "        F.sum('total_amount').alias('revenue'),\n",
        "        F.avg('total_amount').alias('avg_amount'),\n",
        "        F.avg('trip_distance').alias('avg_distance'),\n",
        "        F.avg('tip_amount').alias('avg_tip')\n",
        "    ) \\\n",
        "    .orderBy('weekday') \\\n",
        "    .collect()\n",
        "\n",
        "query_time = time.time() - start_time\n",
        "\n",
        "print(f'\\n{\"Tag\":>4} {\"Fahrten\":>12} {\"Umsatz\":>14} {\"Ã˜ Betrag\":>10} {\"Ã˜ Distanz\":>10} {\"Ã˜ Tip\":>8}')\n",
        "print('-' * 62)\n",
        "\n",
        "total_trips = sum(row.trips for row in trips_by_weekday)\n",
        "for row in trips_by_weekday:\n",
        "    day_name = weekdays[row.weekday - 1]\n",
        "    pct = (row.trips / total_trips) * 100\n",
        "    print(f'{day_name:>4} {row.trips:>12,} ${row.revenue:>12,.0f} ${row.avg_amount:>9.2f} {row.avg_distance:>9.2f}mi ${row.avg_tip:>6.2f}')\n",
        "\n",
        "print(f'\\nâ±ï¸  Query-Zeit: {query_time:.3f}s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Aggregation 3: Fahrten pro Stunde (Heatmap-Daten)\n",
        "# =============================================================================\n",
        "\n",
        "print('ðŸ• FAHRTEN PRO STUNDE')\n",
        "print('=' * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "trips_by_hour = df \\\n",
        "    .withColumn('hour', F.hour('tpep_pickup_datetime')) \\\n",
        "    .groupBy('hour') \\\n",
        "    .agg(\n",
        "        F.count('*').alias('trips'),\n",
        "        F.sum('total_amount').alias('revenue'),\n",
        "        F.avg('total_amount').alias('avg_amount')\n",
        "    ) \\\n",
        "    .orderBy('hour') \\\n",
        "    .collect()\n",
        "\n",
        "query_time = time.time() - start_time\n",
        "\n",
        "print(f'\\n{\"Stunde\":>6} {\"Fahrten\":>12} {\"Umsatz\":>14} {\"Ã˜ Betrag\":>10}  {\"Verteilung\"}')\n",
        "print('-' * 70)\n",
        "\n",
        "max_trips = max(row.trips for row in trips_by_hour)\n",
        "total_trips = sum(row.trips for row in trips_by_hour)\n",
        "\n",
        "for row in trips_by_hour:\n",
        "    bar_len = int((row.trips / max_trips) * 20)\n",
        "    bar = 'â–ˆ' * bar_len\n",
        "    pct = (row.trips / total_trips) * 100\n",
        "    print(f'{row.hour:>5}h {row.trips:>12,} ${row.revenue:>12,.0f} ${row.avg_amount:>9.2f}  {bar} ({pct:.1f}%)')\n",
        "\n",
        "print(f'\\nâ±ï¸  Query-Zeit: {query_time:.3f}s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Aggregation 4: Passagierverteilung\n",
        "# =============================================================================\n",
        "\n",
        "print('ðŸ‘¥ VERTEILUNG NACH PASSAGIERANZAHL')\n",
        "print('=' * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "passenger_stats = df \\\n",
        "    .groupBy('passenger_count') \\\n",
        "    .agg(\n",
        "        F.count('*').alias('trips'),\n",
        "        F.sum('total_amount').alias('revenue'),\n",
        "        F.avg('total_amount').alias('avg_amount'),\n",
        "        F.avg('trip_distance').alias('avg_distance')\n",
        "    ) \\\n",
        "    .orderBy('passenger_count') \\\n",
        "    .collect()\n",
        "\n",
        "query_time = time.time() - start_time\n",
        "\n",
        "print(f'\\n{\"Passagiere\":>10} {\"Fahrten\":>12} {\"Anteil\":>8} {\"Umsatz\":>14} {\"Ã˜ Betrag\":>10} {\"Ã˜ Distanz\":>10}')\n",
        "print('-' * 70)\n",
        "\n",
        "total_trips = sum(row.trips for row in passenger_stats)\n",
        "for row in passenger_stats:\n",
        "    pct = (row.trips / total_trips) * 100\n",
        "    print(f'{int(row.passenger_count):>10} {row.trips:>12,} {pct:>7.1f}% ${row.revenue:>12,.0f} ${row.avg_amount:>9.2f} {row.avg_distance:>9.2f}mi')\n",
        "\n",
        "print(f'\\nâ±ï¸  Query-Zeit: {query_time:.3f}s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Aggregation 5: Top Fahrten\n",
        "# =============================================================================\n",
        "\n",
        "print('ðŸ† TOP 10 LAENGSTE FAHRTEN')\n",
        "print('=' * 60)\n",
        "\n",
        "df.select(\n",
        "    F.date_format('tpep_pickup_datetime', 'yyyy-MM-dd HH:mm').alias('Pickup'),\n",
        "    F.round('trip_distance', 1).alias('Meilen'),\n",
        "    F.round('fare_amount', 2).alias('Fahrpreis'),\n",
        "    F.round('tip_amount', 2).alias('Trinkgeld'),\n",
        "    F.round('total_amount', 2).alias('Gesamt')\n",
        ") \\\n",
        ".orderBy(F.desc('Meilen')) \\\n",
        ".show(10, truncate=False)\n",
        "\n",
        "print('\\nðŸ’° TOP 10 TEUERSTE FAHRTEN')\n",
        "print('=' * 60)\n",
        "\n",
        "df.select(\n",
        "    F.date_format('tpep_pickup_datetime', 'yyyy-MM-dd HH:mm').alias('Pickup'),\n",
        "    F.round('trip_distance', 1).alias('Meilen'),\n",
        "    F.round('fare_amount', 2).alias('Fahrpreis'),\n",
        "    F.round('tip_amount', 2).alias('Trinkgeld'),\n",
        "    F.round('total_amount', 2).alias('Gesamt')\n",
        ") \\\n",
        ".orderBy(F.desc('Gesamt')) \\\n",
        ".show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Zusammenfassung: Verarbeitungsstatistiken\n",
        "# =============================================================================\n",
        "\n",
        "print('\\n' + '=' * 60)\n",
        "print('ðŸ“Š ZUSAMMENFASSUNG - VERARBEITUNGSSTATISTIKEN')\n",
        "print('=' * 60)\n",
        "\n",
        "final_count = df.count()\n",
        "\n",
        "print(f'''\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  CLUSTER KONFIGURATION                                     â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚  Spark Master:         node1.cloud.local:7077              â”‚\n",
        "â”‚  Worker Nodes:         node1, node2, node3, node4          â”‚\n",
        "â”‚  Executors:            {sc.defaultParallelism:>3}                                   â”‚\n",
        "â”‚  Partitionen:          {df.rdd.getNumPartitions():>3}                                   â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚  DATENVERARBEITUNG                                         â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚  Verarbeitete Zeilen:  {final_count:>12,}                     â”‚\n",
        "â”‚  Datenformat:          Apache Parquet                      â”‚\n",
        "â”‚  Verteilung:           Lokal auf allen Worker-Nodes        â”‚\n",
        "â”‚  Data Locality:        âœ“ Aktiviert                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "''')\n",
        "\n",
        "print('ðŸŽ‰ Analyse abgeschlossen!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Spark Session beenden\n",
        "# =============================================================================\n",
        "\n",
        "spark.stop()\n",
        "print('âœ… Spark Session beendet.')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
