{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lambda Architecture: Spark + Solr\n",
        "\n",
        "Dieses Notebook demonstriert die **Lambda-Architektur** fÃ¼r Echtzeit-Dashboards:\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    Lambda Architecture                           â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                  â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚  â”‚   Parquet    â”‚ â”€â”€â”€â–º â”‚    Spark     â”‚ â”€â”€â”€â–º â”‚    Solr      â”‚   â”‚\n",
        "â”‚  â”‚  (Raw Data)  â”‚      â”‚   (Batch)    â”‚      â”‚  (Serving)   â”‚   â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚                                                      â”‚          â”‚\n",
        "â”‚   Millionen Zeilen      Aggregation,                 â–¼          â”‚\n",
        "â”‚   Minuten-Ladezeit      Vorberechnung          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\n",
        "â”‚                                                â”‚Dashboard â”‚     â”‚\n",
        "â”‚                                                â”‚  <50ms   â”‚     â”‚\n",
        "â”‚                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "## Warum diese Architektur?\n",
        "\n",
        "| Layer | Technologie | Aufgabe | Latenz |\n",
        "|-------|-------------|---------|--------|\n",
        "| **Batch** | Spark + Parquet | Rohdaten verarbeiten, aggregieren | Minuten |\n",
        "| **Serving** | Solr Cloud | Voraggregierte Daten bereitstellen | <50ms |\n",
        "| **Speed** | (Optional) Kafka + Spark Streaming | Echtzeit-Updates | Sekunden |\n",
        "\n",
        "**Kernidee:** Spark macht die schwere Arbeit (einmal), Solr liefert die Ergebnisse (tausendmal schnell)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Konfiguration\n",
        "# =============================================================================\n",
        "\n",
        "# Spark Cluster\n",
        "SPARK_MASTER = 'spark://node0.cloud.local:7077'\n",
        "\n",
        "# Solr Cloud\n",
        "SOLR_NODES = ['node1', 'node2', 'node3', 'node4']\n",
        "SOLR_PORT = 8983\n",
        "ZK_HOSTS = 'node1:2181,node2:2181,node3:2181'\n",
        "\n",
        "# Collections\n",
        "COLLECTION_HOURLY = 'nyc-taxi-hourly'      # Aggregation pro Stunde\n",
        "COLLECTION_ZONES = 'nyc-taxi-zones'        # Aggregation pro Zone\n",
        "COLLECTION_DAILY = 'nyc-taxi-daily'        # Aggregation pro Tag\n",
        "\n",
        "# Daten\n",
        "PARQUET_FILES = [\n",
        "    ('yellow_tripdata_2023-01.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet'),\n",
        "    ('yellow_tripdata_2023-02.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet'),\n",
        "]\n",
        "DATA_DIR = '/data/spark/datasets/nyc-taxi'\n",
        "\n",
        "print('âœ… Konfiguration geladen')\n",
        "print(f'   Spark Master: {SPARK_MASTER}')\n",
        "print(f'   ZooKeeper:    {ZK_HOSTS}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schritt 1: Solr Collections erstellen\n",
        "\n",
        "Wir erstellen drei Collections fÃ¼r verschiedene Aggregationsebenen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def create_collection(name, num_shards=4, replication_factor=1):\n",
        "    \"\"\"Erstellt eine Solr Collection fÃ¼r aggregierte Daten.\"\"\"\n",
        "    url = f'http://node1:{SOLR_PORT}/solr/admin/collections'\n",
        "    params = {\n",
        "        'action': 'CREATE',\n",
        "        'name': name,\n",
        "        'numShards': num_shards,\n",
        "        'replicationFactor': replication_factor,\n",
        "        'collection.configName': '_default'\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        resp = requests.get(url, params=params, timeout=30)\n",
        "        if resp.status_code == 200:\n",
        "            result = resp.json()\n",
        "            if 'success' in result or result.get('responseHeader', {}).get('status') == 0:\n",
        "                print(f'âœ… Collection {name} erstellt')\n",
        "                return True\n",
        "        # Collection existiert bereits?\n",
        "        if 'already exists' in resp.text:\n",
        "            print(f'â„¹ï¸  Collection {name} existiert bereits')\n",
        "            return True\n",
        "        print(f'âš ï¸  Collection {name}: {resp.text[:200]}')\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f'âŒ Fehler bei {name}: {e}')\n",
        "        return False\n",
        "\n",
        "# Collections erstellen\n",
        "print('ğŸ“¦ Erstelle Solr Collections fÃ¼r Aggregationen...\\n')\n",
        "create_collection(COLLECTION_HOURLY, num_shards=4)\n",
        "create_collection(COLLECTION_ZONES, num_shards=4)\n",
        "create_collection(COLLECTION_DAILY, num_shards=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schritt 2: Parquet-Daten herunterladen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "for filename, url in PARQUET_FILES:\n",
        "    filepath = os.path.join(DATA_DIR, filename)\n",
        "    if os.path.exists(filepath):\n",
        "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "        print(f'âœ… {filename} existiert ({size_mb:.1f} MB)')\n",
        "    else:\n",
        "        print(f'â¬‡ï¸  Lade {filename}...')\n",
        "        urllib.request.urlretrieve(url, filepath)\n",
        "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "        print(f'âœ… {filename} heruntergeladen ({size_mb:.1f} MB)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schritt 3: Spark Session starten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "import time\n",
        "\n",
        "print('ğŸš€ Starte Spark Session...')\n",
        "start_time = time.time()\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('Lambda Architecture - Batch Layer') \\\n",
        "    .master(SPARK_MASTER) \\\n",
        "    .config('spark.executor.cores', '2') \\\n",
        "    .config('spark.executor.memory', '6g') \\\n",
        "    .config('spark.driver.memory', '4g') \\\n",
        "    .config('spark.sql.shuffle.partitions', '48') \\\n",
        "    .config('spark.sql.adaptive.enabled', 'true') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f'âœ… Spark Session gestartet in {time.time() - start_time:.1f}s')\n",
        "print(f'   Version: {spark.version}')\n",
        "print(f'   Master:  {spark.sparkContext.master}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schritt 4: Rohdaten laden (Batch Layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('ğŸ“– Lade Parquet-Daten...')\n",
        "start_time = time.time()\n",
        "\n",
        "# Alle Parquet-Dateien laden\n",
        "df_raw = spark.read.parquet(f'{DATA_DIR}/*.parquet')\n",
        "\n",
        "# Cache fÃ¼r wiederholte Verwendung\n",
        "df_raw.cache()\n",
        "\n",
        "# ZÃ¤hlen triggert das Laden\n",
        "total_rows = df_raw.count()\n",
        "load_time = time.time() - start_time\n",
        "\n",
        "print(f'\\n{\"=\" * 60}')\n",
        "print(f'ğŸ“Š ROHDATEN GELADEN')\n",
        "print(f'{\"=\" * 60}')\n",
        "print(f'DatensÃ¤tze:   {total_rows:,}')\n",
        "print(f'Ladezeit:     {load_time:.1f}s')\n",
        "print(f'Throughput:   {total_rows / load_time:,.0f} Zeilen/s')\n",
        "print(f'{\"=\" * 60}')\n",
        "\n",
        "# Schema anzeigen\n",
        "print('\\nSchema:')\n",
        "df_raw.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schritt 5: Aggregationen berechnen (Batch Processing)\n",
        "\n",
        "Jetzt berechnen wir die Aggregationen, die das Dashboard braucht:\n",
        "\n",
        "1. **StÃ¼ndliche Statistiken**: Fahrten, Umsatz, Durchschnittswerte pro Stunde\n",
        "2. **Zonen-Statistiken**: Hotspots, beliebteste Pickup/Dropoff Zonen\n",
        "3. **TÃ¤gliche Ãœbersicht**: Tagestrends"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('âš™ï¸  Berechne Aggregationen...\\n')\n",
        "\n",
        "# Daten vorbereiten\n",
        "df = df_raw.withColumn('pickup_date', F.to_date('tpep_pickup_datetime')) \\\n",
        "           .withColumn('pickup_hour', F.hour('tpep_pickup_datetime')) \\\n",
        "           .withColumn('pickup_dayofweek', F.dayofweek('tpep_pickup_datetime')) \\\n",
        "           .withColumn('trip_duration_min', \n",
        "                       (F.unix_timestamp('tpep_dropoff_datetime') - \n",
        "                        F.unix_timestamp('tpep_pickup_datetime')) / 60)\n",
        "\n",
        "# ============================================================\n",
        "# Aggregation 1: StÃ¼ndliche Statistiken\n",
        "# ============================================================\n",
        "print('ğŸ“Š Aggregation 1: StÃ¼ndliche Statistiken...')\n",
        "start = time.time()\n",
        "\n",
        "df_hourly = df.groupBy('pickup_date', 'pickup_hour') \\\n",
        "    .agg(\n",
        "        F.count('*').alias('trip_count'),\n",
        "        F.sum('total_amount').alias('total_revenue'),\n",
        "        F.avg('total_amount').alias('avg_fare'),\n",
        "        F.avg('trip_distance').alias('avg_distance'),\n",
        "        F.avg('tip_amount').alias('avg_tip'),\n",
        "        F.avg('trip_duration_min').alias('avg_duration_min'),\n",
        "        F.sum('passenger_count').alias('total_passengers')\n",
        "    ) \\\n",
        "    .withColumn('id', F.concat_ws('_', \n",
        "        F.date_format('pickup_date', 'yyyyMMdd'), \n",
        "        F.lpad('pickup_hour', 2, '0'))) \\\n",
        "    .orderBy('pickup_date', 'pickup_hour')\n",
        "\n",
        "hourly_count = df_hourly.count()\n",
        "print(f'   âœ… {hourly_count:,} stÃ¼ndliche Datenpunkte in {time.time() - start:.1f}s')\n",
        "\n",
        "# ============================================================\n",
        "# Aggregation 2: Zonen-Statistiken\n",
        "# ============================================================\n",
        "print('ğŸ“Š Aggregation 2: Zonen-Statistiken...')\n",
        "start = time.time()\n",
        "\n",
        "df_zones = df.groupBy('PULocationID', 'DOLocationID') \\\n",
        "    .agg(\n",
        "        F.count('*').alias('trip_count'),\n",
        "        F.sum('total_amount').alias('total_revenue'),\n",
        "        F.avg('total_amount').alias('avg_fare'),\n",
        "        F.avg('trip_distance').alias('avg_distance'),\n",
        "        F.avg('tip_amount').alias('avg_tip')\n",
        "    ) \\\n",
        "    .withColumn('id', F.concat_ws('_', 'PULocationID', 'DOLocationID')) \\\n",
        "    .filter(F.col('trip_count') >= 10)  # Nur relevante Routen\n",
        "\n",
        "zones_count = df_zones.count()\n",
        "print(f'   âœ… {zones_count:,} Zonen-Kombinationen in {time.time() - start:.1f}s')\n",
        "\n",
        "# ============================================================\n",
        "# Aggregation 3: TÃ¤gliche Ãœbersicht\n",
        "# ============================================================\n",
        "print('ğŸ“Š Aggregation 3: TÃ¤gliche Ãœbersicht...')\n",
        "start = time.time()\n",
        "\n",
        "df_daily = df.groupBy('pickup_date', 'pickup_dayofweek') \\\n",
        "    .agg(\n",
        "        F.count('*').alias('trip_count'),\n",
        "        F.sum('total_amount').alias('total_revenue'),\n",
        "        F.avg('total_amount').alias('avg_fare'),\n",
        "        F.avg('trip_distance').alias('avg_distance'),\n",
        "        F.sum('passenger_count').alias('total_passengers'),\n",
        "        F.avg('tip_amount').alias('avg_tip'),\n",
        "        F.max('total_amount').alias('max_fare'),\n",
        "        F.max('trip_distance').alias('max_distance')\n",
        "    ) \\\n",
        "    .withColumn('id', F.date_format('pickup_date', 'yyyyMMdd')) \\\n",
        "    .withColumn('day_name', F.date_format('pickup_date', 'EEEE')) \\\n",
        "    .orderBy('pickup_date')\n",
        "\n",
        "daily_count = df_daily.count()\n",
        "print(f'   âœ… {daily_count:,} Tages-Datenpunkte in {time.time() - start:.1f}s')\n",
        "\n",
        "print(f'\\n{\"=\" * 60}')\n",
        "print(f'ğŸ“ˆ AGGREGATIONEN BERECHNET')\n",
        "print(f'{\"=\" * 60}')\n",
        "print(f'StÃ¼ndlich:  {hourly_count:,} Datenpunkte')\n",
        "print(f'Zonen:      {zones_count:,} Routen')\n",
        "print(f'TÃ¤glich:    {daily_count:,} Datenpunkte')\n",
        "print(f'{\"=\" * 60}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schritt 6: Aggregationen nach Solr pushen (Serving Layer)\n",
        "\n",
        "Jetzt schreiben wir die vorberechneten Aggregationen nach Solr.\n",
        "Das Dashboard kann dann mit **<50ms Latenz** darauf zugreifen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pysolr\n",
        "from kazoo.client import KazooClient\n",
        "\n",
        "def push_to_solr(df, collection_name, batch_size=1000):\n",
        "    \"\"\"\n",
        "    Pushed einen Spark DataFrame nach Solr.\n",
        "    Konvertiert via Pandas fÃ¼r einfache Serialisierung.\n",
        "    \"\"\"\n",
        "    print(f'\\nğŸ“¤ Pushe nach {collection_name}...')\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Spark DF zu Pandas (fÃ¼r kleine aggregierte Daten OK)\n",
        "    pdf = df.toPandas()\n",
        "    \n",
        "    # Datum-Spalten zu String konvertieren fÃ¼r JSON\n",
        "    for col in pdf.columns:\n",
        "        if pdf[col].dtype == 'datetime64[ns]':\n",
        "            pdf[col] = pdf[col].dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "        elif str(pdf[col].dtype).startswith('date'):\n",
        "            pdf[col] = pdf[col].astype(str)\n",
        "    \n",
        "    # NaN durch None ersetzen\n",
        "    pdf = pdf.where(pdf.notnull(), None)\n",
        "    \n",
        "    # Zu Liste von Dicts konvertieren\n",
        "    docs = pdf.to_dict(orient='records')\n",
        "    \n",
        "    # Solr Verbindung\n",
        "    zk = pysolr.ZooKeeper(ZK_HOSTS)\n",
        "    solr = pysolr.SolrCloud(zk, collection_name, timeout=60)\n",
        "    \n",
        "    # Alte Daten lÃ¶schen\n",
        "    solr.delete(q='*:*')\n",
        "    \n",
        "    # In Batches hochladen\n",
        "    total = len(docs)\n",
        "    for i in range(0, total, batch_size):\n",
        "        batch = docs[i:i + batch_size]\n",
        "        solr.add(batch)\n",
        "        if (i + batch_size) % 5000 == 0 or i + batch_size >= total:\n",
        "            print(f'   Fortschritt: {min(i + batch_size, total):,} / {total:,}')\n",
        "    \n",
        "    # Commit\n",
        "    solr.commit()\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    print(f'   âœ… {total:,} Dokumente in {elapsed:.1f}s gepusht')\n",
        "    print(f'   Throughput: {total / elapsed:,.0f} Docs/s')\n",
        "    \n",
        "    return total\n",
        "\n",
        "# Alle Aggregationen pushen\n",
        "print('=' * 60)\n",
        "print('ğŸ“¤ PUSH TO SOLR (Serving Layer)')\n",
        "print('=' * 60)\n",
        "\n",
        "push_to_solr(df_hourly, COLLECTION_HOURLY)\n",
        "push_to_solr(df_zones, COLLECTION_ZONES)\n",
        "push_to_solr(df_daily, COLLECTION_DAILY)\n",
        "\n",
        "print('\\nâœ… Alle Aggregationen nach Solr gepusht!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schritt 7: Dashboard-Queries testen (Serving Layer)\n",
        "\n",
        "Jetzt testen wir, wie schnell Solr die Daten liefert.\n",
        "Diese Queries wÃ¼rden vom Dashboard genutzt werden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def solr_query(collection, query='*:*', rows=10, **kwargs):\n",
        "    \"\"\"FÃ¼hrt eine Solr Query aus und misst die Zeit.\"\"\"\n",
        "    zk = pysolr.ZooKeeper(ZK_HOSTS)\n",
        "    solr = pysolr.SolrCloud(zk, collection, timeout=30)\n",
        "    \n",
        "    start = time.time()\n",
        "    results = solr.search(query, rows=rows, **kwargs)\n",
        "    elapsed_ms = (time.time() - start) * 1000\n",
        "    \n",
        "    return results, elapsed_ms\n",
        "\n",
        "print('=' * 60)\n",
        "print('ğŸ” DASHBOARD QUERIES (Serving Layer Latenz)')\n",
        "print('=' * 60)\n",
        "\n",
        "# Query 1: StÃ¼ndliche Daten fÃ¼r einen Tag\n",
        "print('\\nğŸ“Š Query 1: StÃ¼ndliche Statistiken fÃ¼r einen Tag')\n",
        "results, ms = solr_query(COLLECTION_HOURLY, 'pickup_date:2023-01-15', rows=24,\n",
        "                         sort='pickup_hour asc')\n",
        "print(f'   Ergebnisse: {results.hits}')\n",
        "print(f'   Latenz:     {ms:.1f}ms âœ…')\n",
        "\n",
        "# Query 2: Top 10 Routen nach Umsatz\n",
        "print('\\nğŸ“Š Query 2: Top 10 Routen nach Umsatz')\n",
        "results, ms = solr_query(COLLECTION_ZONES, '*:*', rows=10,\n",
        "                         sort='total_revenue desc')\n",
        "print(f'   Ergebnisse: {results.hits}')\n",
        "print(f'   Latenz:     {ms:.1f}ms âœ…')\n",
        "for doc in list(results)[:5]:\n",
        "    print(f\"   Route {doc.get('PULocationID')} â†’ {doc.get('DOLocationID')}: ${doc.get('total_revenue', 0):,.0f}\")\n",
        "\n",
        "# Query 3: WochenÃ¼bersicht\n",
        "print('\\nğŸ“Š Query 3: WochenÃ¼bersicht (7 Tage)')\n",
        "results, ms = solr_query(COLLECTION_DAILY, 'pickup_date:[2023-01-09 TO 2023-01-15]', rows=7,\n",
        "                         sort='pickup_date asc')\n",
        "print(f'   Ergebnisse: {results.hits}')\n",
        "print(f'   Latenz:     {ms:.1f}ms âœ…')\n",
        "\n",
        "# Query 4: Facettierung - Fahrten nach Wochentag\n",
        "print('\\nğŸ“Š Query 4: Fahrten nach Wochentag (Facetten)')\n",
        "zk = pysolr.ZooKeeper(ZK_HOSTS)\n",
        "solr = pysolr.SolrCloud(zk, COLLECTION_DAILY, timeout=30)\n",
        "start = time.time()\n",
        "results = solr.search('*:*', rows=0, **{\n",
        "    'facet': 'true',\n",
        "    'facet.field': 'day_name',\n",
        "    'facet.mincount': 1\n",
        "})\n",
        "ms = (time.time() - start) * 1000\n",
        "print(f'   Latenz: {ms:.1f}ms âœ…')\n",
        "if hasattr(results, 'facets') and 'facet_fields' in results.facets:\n",
        "    facets = results.facets['facet_fields'].get('day_name', [])\n",
        "    for i in range(0, len(facets), 2):\n",
        "        print(f\"   {facets[i]}: {facets[i+1]} Tage\")\n",
        "\n",
        "print('\\n' + '=' * 60)\n",
        "print('âœ… Alle Queries unter 100ms - Dashboard-tauglich!')\n",
        "print('=' * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schritt 8: Visualisierung der Aggregationen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# TÃ¤gliche Daten aus Solr laden\n",
        "zk = pysolr.ZooKeeper(ZK_HOSTS)\n",
        "solr = pysolr.SolrCloud(zk, COLLECTION_DAILY, timeout=30)\n",
        "results = solr.search('*:*', rows=100, sort='pickup_date asc')\n",
        "\n",
        "# Zu DataFrame\n",
        "daily_data = pd.DataFrame([dict(doc) for doc in results.docs])\n",
        "\n",
        "# Visualisierung\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: Fahrten pro Tag\n",
        "ax = axes[0, 0]\n",
        "ax.bar(range(len(daily_data)), daily_data['trip_count'], color='steelblue', alpha=0.7)\n",
        "ax.set_title('Fahrten pro Tag', fontsize=12, fontweight='bold')\n",
        "ax.set_xlabel('Tag')\n",
        "ax.set_ylabel('Anzahl Fahrten')\n",
        "ax.ticklabel_format(style='plain', axis='y')\n",
        "\n",
        "# Plot 2: Umsatz pro Tag\n",
        "ax = axes[0, 1]\n",
        "ax.plot(range(len(daily_data)), daily_data['total_revenue'], 'g-o', markersize=4)\n",
        "ax.fill_between(range(len(daily_data)), daily_data['total_revenue'], alpha=0.3, color='green')\n",
        "ax.set_title('Umsatz pro Tag ($)', fontsize=12, fontweight='bold')\n",
        "ax.set_xlabel('Tag')\n",
        "ax.set_ylabel('Umsatz ($)')\n",
        "ax.ticklabel_format(style='plain', axis='y')\n",
        "\n",
        "# Plot 3: Durchschnittlicher Fahrpreis\n",
        "ax = axes[1, 0]\n",
        "ax.bar(range(len(daily_data)), daily_data['avg_fare'], color='orange', alpha=0.7)\n",
        "ax.axhline(y=daily_data['avg_fare'].mean(), color='red', linestyle='--', label='Durchschnitt')\n",
        "ax.set_title('Durchschnittlicher Fahrpreis ($)', fontsize=12, fontweight='bold')\n",
        "ax.set_xlabel('Tag')\n",
        "ax.set_ylabel('Ã˜ Fahrpreis ($)')\n",
        "ax.legend()\n",
        "\n",
        "# Plot 4: Passagiere pro Tag\n",
        "ax = axes[1, 1]\n",
        "ax.bar(range(len(daily_data)), daily_data['total_passengers'], color='purple', alpha=0.7)\n",
        "ax.set_title('Passagiere pro Tag', fontsize=12, fontweight='bold')\n",
        "ax.set_xlabel('Tag')\n",
        "ax.set_ylabel('Anzahl Passagiere')\n",
        "ax.ticklabel_format(style='plain', axis='y')\n",
        "\n",
        "plt.suptitle('NYC Taxi Dashboard - Daten aus Solr Serving Layer', \n",
        "             fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\nâœ… Dashboard-Visualisierung - Daten kommen direkt aus Solr!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Zusammenfassung: Lambda Architecture Performance\n",
        "\n",
        "| Phase | Technologie | Dauer | HÃ¤ufigkeit |\n",
        "|-------|-------------|-------|------------|\n",
        "| **Batch** | Spark + Parquet | Minuten | 1x pro Stunde/Tag |\n",
        "| **Serving** | Solr Query | <50ms | 1000x pro Sekunde mÃ¶glich |\n",
        "\n",
        "### Vorteile dieser Architektur:\n",
        "\n",
        "1. **Trennung von Concerns**: Spark fÃ¼r schwere Berechnungen, Solr fÃ¼r schnelle Abfragen\n",
        "2. **Skalierbarkeit**: Spark skaliert fÃ¼r Batch, Solr skaliert fÃ¼r Queries\n",
        "3. **Einfachheit**: Kein komplexer Spark-Solr Connector nÃ¶tig\n",
        "4. **FlexibilitÃ¤t**: Aggregationen kÃ¶nnen jederzeit neu berechnet werden\n",
        "\n",
        "### NÃ¤chste Schritte fÃ¼r Produktion:\n",
        "\n",
        "- **Scheduling**: Spark Job regelmÃ¤ÃŸig via Cron/Airflow ausfÃ¼hren\n",
        "- **Speed Layer**: Kafka + Spark Streaming fÃ¼r Echtzeit-Updates\n",
        "- **Dashboard**: Grafana mit Solr-Datasource oder eigene Web-App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AufrÃ¤umen\n",
        "spark.stop()\n",
        "print('âœ… Spark Session beendet')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
