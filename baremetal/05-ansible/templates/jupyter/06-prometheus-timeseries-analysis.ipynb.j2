{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prometheus Timeseries Analyse mit Solr + Spark\n",
        "\n",
        "Dieses Notebook demonstriert die **Analyse von Prometheus Monitoring-Daten** aus Solr Cloud:\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ              Prometheus Timeseries Architecture                      ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                                      ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ\n",
        "‚îÇ  ‚îÇ  Prometheus  ‚îÇ ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ   Importer   ‚îÇ ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ    Solr      ‚îÇ       ‚îÇ\n",
        "‚îÇ  ‚îÇ  (Metriken)  ‚îÇ      ‚îÇ  (Mapping)   ‚îÇ      ‚îÇ (prometheus) ‚îÇ       ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ\n",
        "‚îÇ                                                      ‚îÇ              ‚îÇ\n",
        "‚îÇ                                               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ\n",
        "‚îÇ                                               ‚îÇ             ‚îÇ       ‚îÇ\n",
        "‚îÇ                                               ‚ñº             ‚ñº       ‚îÇ\n",
        "‚îÇ                                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
        "‚îÇ                                        ‚îÇ Metadaten‚îÇ  ‚îÇ  BASE64   ‚îÇ  ‚îÇ\n",
        "‚îÇ                                        ‚îÇ(Host,    ‚îÇ  ‚îÇ Timeseries‚îÇ  ‚îÇ\n",
        "‚îÇ                                        ‚îÇ Process) ‚îÇ  ‚îÇ   Data    ‚îÇ  ‚îÇ\n",
        "‚îÇ                                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
        "‚îÇ                                               ‚îÇ             ‚îÇ       ‚îÇ\n",
        "‚îÇ                                               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ\n",
        "‚îÇ                                                      ‚ñº              ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ\n",
        "‚îÇ  ‚îÇ    Spark     ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ ‚îÇ Parallel     ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ ‚îÇ   Filter     ‚îÇ       ‚îÇ\n",
        "‚îÇ  ‚îÇ    (RDD)     ‚îÇ      ‚îÇ Shard-Load   ‚îÇ      ‚îÇ  (Host,Proc) ‚îÇ       ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ\n",
        "‚îÇ                                                                      ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "## Datenformat in Solr\n",
        "\n",
        "Jedes Dokument enth√§lt:\n",
        "- **Metadaten**: `ts_hostGroup`, `ts_host`, `ts_process`, `ts_metricGroup`, `ts_metricName`\n",
        "- **Zeitreihen**: `ts_data` (BASE64-kodierter String mit Timestamp/Value-Paaren)\n",
        "- **Zeitraum**: `ts_start`, `ts_end`, `ts_data_amountValues`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Konfiguration\n",
        "# =============================================================================\n",
        "import os\n",
        "import time\n",
        "import base64\n",
        "import struct\n",
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "# Spark Cluster\n",
        "SPARK_MASTER = 'spark://node0.cloud.local:7077'\n",
        "\n",
        "# Solr Cloud\n",
        "SOLR_HOST = 'node1'\n",
        "SOLR_PORT = 8983\n",
        "SOLR_NODES = ['node1', 'node2', 'node3', 'node4']\n",
        "ZK_HOSTS = 'node1:2181,node2:2181,node3:2181'\n",
        "\n",
        "# Prometheus Daten in der EKG Collection (gefiltert via project_name)\n",
        "COLLECTION = 'ekg'\n",
        "PROJECT_NAME = 'Prometheus'\n",
        "\n",
        "print('‚úÖ Konfiguration geladen')\n",
        "print(f'   Spark Master: {SPARK_MASTER}')\n",
        "print(f'   Solr:         http://{SOLR_HOST}:{SOLR_PORT}/solr/{COLLECTION}')\n",
        "print(f'   Solr Nodes:   {SOLR_NODES}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Teil 1: Datenformat analysieren\n",
        "\n",
        "Zuerst schauen wir uns die Struktur der Prometheus-Dokumente in Solr an."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def solr_query(query, fields='*', rows=10, collection=COLLECTION):\n",
        "    \"\"\"F√ºhrt eine Solr-Query aus und gibt die Dokumente zur√ºck.\"\"\"\n",
        "    url = f'http://{SOLR_HOST}:{SOLR_PORT}/solr/{collection}/select'\n",
        "    params = {\n",
        "        'q': query,\n",
        "        'fl': fields,\n",
        "        'rows': rows,\n",
        "        'wt': 'json'\n",
        "    }\n",
        "    resp = requests.get(url, params=params, timeout=30)\n",
        "    resp.raise_for_status()\n",
        "    result = resp.json()\n",
        "    return result['response']['docs'], result['response']['numFound']\n",
        "\n",
        "def solr_facets(field, query='*:*', collection=COLLECTION, limit=50):\n",
        "    \"\"\"Holt Facetten-Werte f√ºr ein Feld.\"\"\"\n",
        "    url = f'http://{SOLR_HOST}:{SOLR_PORT}/solr/{collection}/select'\n",
        "    params = {\n",
        "        'q': query,\n",
        "        'rows': 0,\n",
        "        'facet': 'true',\n",
        "        'facet.field': field,\n",
        "        'facet.limit': limit,\n",
        "        'facet.mincount': 1,\n",
        "        'wt': 'json'\n",
        "    }\n",
        "    resp = requests.get(url, params=params, timeout=30)\n",
        "    resp.raise_for_status()\n",
        "    result = resp.json()\n",
        "    facet_data = result.get('facet_counts', {}).get('facet_fields', {}).get(field, [])\n",
        "    # Solr gibt [value, count, value, count, ...] zur√ºck\n",
        "    return [(facet_data[i], facet_data[i+1]) for i in range(0, len(facet_data), 2)]\n",
        "\n",
        "# Gesamtanzahl der Prometheus-Dokumente\n",
        "PROMETHEUS_BASE_QUERY = f'project_name:{PROJECT_NAME}'\n",
        "_, total = solr_query(PROMETHEUS_BASE_QUERY, rows=0)\n",
        "print(f'üìä Prometheus Daten in EKG Collection: {total:,} Dokumente')\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# √úbersicht: Welche Hosts, Prozesse und Metrik-Gruppen gibt es?\n",
        "print('=' * 70)\n",
        "print('üìä √úBERSICHT: Verf√ºgbare Dimensionen (nur Prometheus Daten)')\n",
        "print('=' * 70)\n",
        "\n",
        "print('\\nüñ•Ô∏è  Hosts:')\n",
        "for host, count in solr_facets('ts_host', query=PROMETHEUS_BASE_QUERY):\n",
        "    print(f'   {host}: {count:,} Zeitreihen')\n",
        "\n",
        "print('\\n‚öôÔ∏è  Prozesse:')\n",
        "for process, count in solr_facets('ts_process', query=PROMETHEUS_BASE_QUERY):\n",
        "    print(f'   {process}: {count:,} Zeitreihen')\n",
        "\n",
        "print('\\nüìà Metrik-Gruppen:')\n",
        "for group, count in solr_facets('ts_metricGroup', query=PROMETHEUS_BASE_QUERY):\n",
        "    print(f'   {group}: {count:,} Zeitreihen')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Beispiel-Dokument anzeigen (ohne ts_data f√ºr √úbersicht)\n",
        "print('=' * 70)\n",
        "print('üìÑ BEISPIEL-DOKUMENT (Metadaten)')\n",
        "print('=' * 70)\n",
        "\n",
        "docs, _ = solr_query(PROMETHEUS_BASE_QUERY, fields='id,ts_hostGroup,ts_host,ts_process,ts_metricGroup,ts_metricName,ts_start,ts_end,ts_data_amountValues', rows=1)\n",
        "if docs:\n",
        "    doc = docs[0]\n",
        "    for key, value in doc.items():\n",
        "        print(f'   {key}: {value}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Teil 2: BASE64 Timeseries-Daten dekodieren\n",
        "\n",
        "Das `ts_data` Feld enth√§lt BASE64-kodierte Zeitreihen-Daten.\n",
        "Format: Paare von (Value: Double 8 Bytes, Timestamp: Long 8 Bytes, Big-Endian)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode_timeseries(base64_data):\n",
        "    \"\"\"\n",
        "    Dekodiert BASE64-kodierte Timeseries-Daten.\n",
        "    \n",
        "    Format: Paare von (value: double 8 Bytes, timestamp_ms: long 8 Bytes)\n",
        "    Jedes Paar = 16 Bytes (Double + Long, Big-Endian)\n",
        "    \n",
        "    Returns: Liste von (datetime, value) Tupeln\n",
        "    \"\"\"\n",
        "    if not base64_data:\n",
        "        return []\n",
        "    \n",
        "    # BASE64 dekodieren\n",
        "    raw_bytes = base64.b64decode(base64_data)\n",
        "    \n",
        "    # Paare extrahieren (je 16 Bytes = 1 Double + 1 Long)\n",
        "    num_pairs = len(raw_bytes) // 16\n",
        "    datapoints = []\n",
        "    \n",
        "    for i in range(num_pairs):\n",
        "        offset = i * 16\n",
        "        # Big-Endian: >d = double, >q = signed long (8 bytes)\n",
        "        value = struct.unpack('>d', raw_bytes[offset:offset+8])[0]\n",
        "        timestamp_ms = struct.unpack('>q', raw_bytes[offset+8:offset+16])[0]\n",
        "        \n",
        "        # Timestamp von Millisekunden zu datetime\n",
        "        try:\n",
        "            dt = datetime.fromtimestamp(timestamp_ms / 1000.0)\n",
        "            datapoints.append((dt, value))\n",
        "        except (ValueError, OSError):\n",
        "            # Ung√ºltiger Timestamp, √ºberspringen\n",
        "            pass\n",
        "    \n",
        "    return datapoints\n",
        "\n",
        "print('‚úÖ Timeseries-Dekoder definiert')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test: Ein Dokument laden und dekodieren\n",
        "print('=' * 70)\n",
        "print('üî¨ TEST: Timeseries dekodieren')\n",
        "print('=' * 70)\n",
        "\n",
        "docs, _ = solr_query('ts_metricName:metrics_jvm_memory_pools_bytes', fields='*', rows=1)\n",
        "if docs:\n",
        "    doc = docs[0]\n",
        "    print(f'\\nüìÑ Dokument:')\n",
        "    print(f'   Host:        {doc.get(\"ts_host\")}')\n",
        "    print(f'   Prozess:     {doc.get(\"ts_process\")}')\n",
        "    print(f'   Metrik:      {doc.get(\"ts_metricName\")}')\n",
        "    print(f'   Zeitraum:    {doc.get(\"ts_start\")} bis {doc.get(\"ts_end\")}')\n",
        "    print(f'   Datenpunkte: {doc.get(\"ts_data_amountValues\")}')\n",
        "    \n",
        "    # Dekodieren\n",
        "    ts_data = doc.get('ts_data', '')\n",
        "    datapoints = decode_timeseries(ts_data)\n",
        "    \n",
        "    print(f'\\nüìä Dekodierte Datenpunkte: {len(datapoints)}')\n",
        "    print('\\n   Erste 10 Werte:')\n",
        "    for dt, value in datapoints[:10]:\n",
        "        print(f'   {dt.strftime(\"%Y-%m-%d %H:%M:%S\")} ‚Üí {value:,.0f} bytes')\n",
        "    \n",
        "    print('\\n   Letzte 5 Werte:')\n",
        "    for dt, value in datapoints[-5:]:\n",
        "        print(f'   {dt.strftime(\"%Y-%m-%d %H:%M:%S\")} ‚Üí {value:,.0f} bytes')\n",
        "else:\n",
        "    print('‚ö†Ô∏è  Keine Dokumente gefunden')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Teil 3: Paralleles Laden von Solr nach Spark\n",
        "\n",
        "Wie in Notebook 5 demonstriert, laden wir die Daten **parallel von allen Shards**.\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Paralleles Laden via /export: Jeder Executor ‚Üí Lokaler Shard    ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                             ‚îÇ\n",
        "‚îÇ  Executor 1 ‚îÄ‚îÄ‚ñ∫ node1:8983/solr/shard1/export ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ\n",
        "‚îÇ  Executor 2 ‚îÄ‚îÄ‚ñ∫ node2:8983/solr/shard2/export ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚ñ∫   ‚îÇ\n",
        "‚îÇ  Executor 3 ‚îÄ‚îÄ‚ñ∫ node3:8983/solr/shard3/export ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  RDD ‚îÇ\n",
        "‚îÇ  Executor 4 ‚îÄ‚îÄ‚ñ∫ node4:8983/solr/shard4/export ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ\n",
        "‚îÇ                                                             ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "print('üöÄ Starte Spark Session...')\n",
        "\n",
        "# 4 Executors √ó 4 Cores = 16 parallele Tasks\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('Prometheus Timeseries Analysis') \\\n",
        "    .master(SPARK_MASTER) \\\n",
        "    .config('spark.executor.cores', '4') \\\n",
        "    .config('spark.executor.memory', '6g') \\\n",
        "    .config('spark.executor.instances', '4') \\\n",
        "    .config('spark.driver.memory', '4g') \\\n",
        "    .config('spark.sql.shuffle.partitions', '16') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "print(f'‚úÖ Spark Session gestartet')\n",
        "print(f'   Master: {sc.master}')\n",
        "print(f'   Executors: 4 √ó 4 Cores = 16 parallele Tasks')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_shard_info(collection):\n",
        "    \"\"\"\n",
        "    Holt Shard-Informationen aus dem Solr CLUSTERSTATUS API.\n",
        "    Gibt Liste von {node, core, shard_name} zur√ºck f√ºr paralleles Laden.\n",
        "    \"\"\"\n",
        "    url = f'http://{SOLR_HOST}:{SOLR_PORT}/solr/admin/collections'\n",
        "    params = {'action': 'CLUSTERSTATUS', 'collection': collection, 'wt': 'json'}\n",
        "    \n",
        "    resp = requests.get(url, params=params, timeout=10)\n",
        "    resp.raise_for_status()\n",
        "    data = resp.json()\n",
        "    \n",
        "    shard_info = []\n",
        "    shards = data.get('cluster', {}).get('collections', {}).get(collection, {}).get('shards', {})\n",
        "    \n",
        "    for shard_name, shard_data in shards.items():\n",
        "        replicas = shard_data.get('replicas', {})\n",
        "        for replica_name, replica_data in replicas.items():\n",
        "            if replica_data.get('state') == 'active':\n",
        "                node_name = replica_data.get('node_name', '')\n",
        "                node = node_name.split(':')[0]\n",
        "                core = replica_data.get('core')\n",
        "                \n",
        "                shard_info.append({\n",
        "                    'node': node,\n",
        "                    'core': core,\n",
        "                    'shard_name': shard_name\n",
        "                })\n",
        "                break\n",
        "    \n",
        "    return shard_info\n",
        "\n",
        "# Shard-Info anzeigen\n",
        "shard_info = get_shard_info(COLLECTION)\n",
        "print(f'üìä Shards in Collection \"{COLLECTION}\":')\n",
        "for s in shard_info:\n",
        "    print(f'   {s[\"shard_name\"]}: {s[\"node\"]} ‚Üí {s[\"core\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Worker-Funktion f√ºr Executors - MUSS zuerst definiert werden!\n",
        "def load_and_decode_shard(task):\n",
        "    \"\"\"\n",
        "    L√§dt ALLE Dokumente von einem Shard via /select und dekodiert die Timeseries.\n",
        "    Diese Funktion l√§uft auf den Executors!\n",
        "    \n",
        "    Verwendet /select statt /export, da die BASE64-Strings zu gro√ü f√ºr /export sind.\n",
        "    \n",
        "    task enth√§lt: node, core, query, num_docs\n",
        "    \"\"\"\n",
        "    import requests\n",
        "    import base64\n",
        "    import struct\n",
        "    \n",
        "    node = task['node']\n",
        "    core = task['core']\n",
        "    q = task['query']\n",
        "    num_docs = task.get('num_docs', 10000)  # Anzahl Dokumente in diesem Shard\n",
        "    \n",
        "    fields = 'id,ts_hostGroup,ts_host,ts_process,ts_metricGroup,ts_metricName,ts_start,ts_end,ts_data_amountValues,ts_data'\n",
        "    \n",
        "    # /select mit rows=num_docs und distrib=false (nur lokaler Shard)\n",
        "    url = f'http://{node}:8983/solr/{core}/select'\n",
        "    params = {'q': q, 'fl': fields, 'rows': num_docs, 'distrib': 'false', 'wt': 'json'}\n",
        "    \n",
        "    results = []\n",
        "    try:\n",
        "        response = requests.get(url, params=params, timeout=300)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        docs = data.get('response', {}).get('docs', [])\n",
        "        \n",
        "        for doc in docs:\n",
        "            metadata = {\n",
        "                'id': doc.get('id'),\n",
        "                'hostGroup': doc.get('ts_hostGroup'),\n",
        "                'host': doc.get('ts_host'),\n",
        "                'process': doc.get('ts_process'),\n",
        "                'metricGroup': doc.get('ts_metricGroup'),\n",
        "                'metricName': doc.get('ts_metricName'),\n",
        "                'numValues': doc.get('ts_data_amountValues', 0)\n",
        "            }\n",
        "            \n",
        "            ts_data = doc.get('ts_data', '')\n",
        "            datapoints = []\n",
        "            \n",
        "            if ts_data:\n",
        "                try:\n",
        "                    raw_bytes = base64.b64decode(ts_data)\n",
        "                    num_pairs = len(raw_bytes) // 16\n",
        "                    for i in range(num_pairs):\n",
        "                        offset = i * 16\n",
        "                        value = struct.unpack('>d', raw_bytes[offset:offset+8])[0]\n",
        "                        timestamp_ms = struct.unpack('>q', raw_bytes[offset+8:offset+16])[0]\n",
        "                        datapoints.append((timestamp_ms, value))\n",
        "                except Exception:\n",
        "                    pass\n",
        "            \n",
        "            results.append((metadata, datapoints))\n",
        "    except Exception as e:\n",
        "        results.append(({'error': str(e), 'task': task}, []))\n",
        "    \n",
        "    return results\n",
        "\n",
        "def load_prometheus_parallel(spark, query, collection=COLLECTION):\n",
        "    \"\"\"\n",
        "    L√§dt Prometheus-Dokumente PARALLEL aus allen Solr Shards via /select.\n",
        "    \n",
        "    Verwendet /select statt /export, da die BASE64-Strings zu gro√ü f√ºr /export sind.\n",
        "    Ein Task pro Shard.\n",
        "    Dekodiert die BASE64-Timeseries (String-Feld) inline auf den Executors.\n",
        "    Gibt ein RDD von (metadata, datapoints) Tupeln zur√ºck.\n",
        "    \"\"\"\n",
        "    print(f'\\nüì• PARALLELES Laden von Prometheus-Daten via /select...')\n",
        "    print(f'   Query: {query}')\n",
        "    \n",
        "    start = time.time()\n",
        "    \n",
        "    # SparkContext aus Session holen\n",
        "    sc = spark.sparkContext\n",
        "    print(f'   SparkContext: {sc.master}')\n",
        "    \n",
        "    # 1. Shard-Info holen\n",
        "    shard_info = get_shard_info(collection)\n",
        "    num_shards = len(shard_info)\n",
        "    print(f'   Shards: {num_shards}')\n",
        "    for s in shard_info:\n",
        "        print(f\"      {s['shard_name']}: {s['node']} ‚Üí {s['core']}\")\n",
        "    \n",
        "    if num_shards == 0:\n",
        "        print('   ‚ö†Ô∏è  Keine Shards gefunden!')\n",
        "        return None\n",
        "    \n",
        "    # 2. Tasks erstellen - ein Task pro Shard\n",
        "    print(f'   üìä Erstelle Tasks f√ºr /select (1 pro Shard)...')\n",
        "    tasks = []\n",
        "    total_docs = 0\n",
        "    \n",
        "    for shard in shard_info:\n",
        "        # Count f√ºr diesen Shard abfragen (distrib=false f√ºr lokalen Shard)\n",
        "        url = f\"http://{shard['node']}:8983/solr/{shard['core']}/select\"\n",
        "        params = {'q': query, 'rows': 0, 'distrib': 'false', 'wt': 'json'}\n",
        "        resp = requests.get(url, params=params, timeout=30)\n",
        "        shard_count = resp.json().get('response', {}).get('numFound', 0)\n",
        "        total_docs += shard_count\n",
        "        \n",
        "        # Ein Task pro Shard mit num_docs f√ºr /select rows Parameter\n",
        "        tasks.append({\n",
        "            'node': shard['node'],\n",
        "            'core': shard['core'],\n",
        "            'shard_name': shard['shard_name'],\n",
        "            'query': query,\n",
        "            'num_docs': shard_count,\n",
        "            'task_id': f\"{shard['shard_name']}_select\"\n",
        "        })\n",
        "        \n",
        "        print(f\"      {shard['shard_name']}: {shard_count:,} Dokumente\")\n",
        "    \n",
        "    print(f'   Gesamt: {total_docs:,} Dokumente')\n",
        "    \n",
        "    if total_docs == 0:\n",
        "        print('   ‚ö†Ô∏è  Keine Daten gefunden')\n",
        "        return None\n",
        "    \n",
        "    # 3. RDD mit allen Tasks erstellen (1 Task pro Shard)\n",
        "    num_tasks = len(tasks)\n",
        "    print(f'   üì¶ Erstelle RDD mit {num_tasks} Tasks ({num_shards} Shards)...')\n",
        "    task_rdd = sc.parallelize(tasks, numSlices=num_tasks)\n",
        "    actual_partitions = task_rdd.getNumPartitions()\n",
        "    print(f'   ‚úì RDD hat {actual_partitions} Partitionen')\n",
        "    \n",
        "    # 4. Parallel von allen Shards laden via /select und dekodieren\n",
        "    print(f'   üöÄ Starte {num_tasks} parallele /select + Decode Tasks...')\n",
        "    docs_rdd = task_rdd.flatMap(load_and_decode_shard)\n",
        "    \n",
        "    # 5. Timeseries-Segmente zusammenf√ºhren\n",
        "    # Eine Metrik kann auf mehrere Solr-Dokumente verteilt sein (zeitliche Segmente)\n",
        "    print(f'   üîó F√ºhre zeitliche Segmente zusammen (reduceByKey)...')\n",
        "    \n",
        "    def to_key_value(item):\n",
        "        metadata, datapoints = item\n",
        "        key = (\n",
        "            metadata.get('host', ''),\n",
        "            metadata.get('process', ''),\n",
        "            metadata.get('metricGroup', ''),\n",
        "            metadata.get('metricName', '')\n",
        "        )\n",
        "        return (key, (metadata, datapoints))\n",
        "    \n",
        "    def merge_segments(a, b):\n",
        "        # Metadata vom ersten nehmen, datapoints zusammenf√ºhren\n",
        "        metadata_a, dp_a = a\n",
        "        metadata_b, dp_b = b\n",
        "        return (metadata_a, dp_a + dp_b)\n",
        "    \n",
        "    def sort_and_flatten(item):\n",
        "        key, (metadata, datapoints) = item\n",
        "        # Ung√ºltige Werte entfernen:\n",
        "        # - NaN (IEEE 754, Prometheus 'stale' marker)\n",
        "        # - Inf/-Inf (ung√ºltige Berechnungen)\n",
        "        import math\n",
        "        cleaned_dp = [(ts, val) for ts, val in datapoints \n",
        "                      if not math.isnan(val) and not math.isinf(val)]\n",
        "        # Nach Timestamp sortieren\n",
        "        sorted_dp = sorted(cleaned_dp, key=lambda x: x[0])\n",
        "        # numValues aktualisieren\n",
        "        metadata['numValues'] = len(sorted_dp)\n",
        "        return (metadata, sorted_dp)\n",
        "    \n",
        "    merged_rdd = docs_rdd \\\n",
        "        .map(to_key_value) \\\n",
        "        .reduceByKey(merge_segments) \\\n",
        "        .map(sort_and_flatten)\n",
        "    \n",
        "    # Cache f√ºr mehrfache Verwendung\n",
        "    merged_rdd.cache()\n",
        "    \n",
        "    # Materialisieren durch count()\n",
        "    print(f'   ‚è≥ Warte auf Ergebnisse...')\n",
        "    doc_count = docs_rdd.count()\n",
        "    merged_count = merged_rdd.count()\n",
        "    \n",
        "    elapsed = time.time() - start\n",
        "    print(f'   ‚úÖ {doc_count:,} Solr-Dokumente ‚Üí {merged_count:,} Zeitreihen (nach Merge) in {elapsed:.1f}s')\n",
        "    \n",
        "    return merged_rdd\n",
        "\n",
        "print('‚úÖ Paralleler Loader definiert')\n",
        "print('   Verwendet /select Handler (1 Request pro Shard)')\n",
        "print('   F√ºhrt Timeseries-Segmente automatisch zusammen (reduceByKey)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Teil 3a: Test mit kleinem Datensatz - Laden vom Driver\n",
        "\n",
        "Bevor wir parallel laden, testen wir erst mal **vom Driver aus** mit einem kleinen Datensatz:\n",
        "- **PROCESS**: `zookeeper`\n",
        "- Ca. **1.600 Dokumente** (Timeseries-Segmente)\n",
        "\n",
        "Das hilft uns zu verifizieren, dass die Solr-Query und das Dekodieren funktioniert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TEST: Kleiner Datensatz - Laden vom DRIVER (nicht parallel)\n",
        "# =============================================================================\n",
        "# Zuerst laden wir einen kleinen Datensatz direkt vom Driver, um zu testen\n",
        "# ob Query und Dekodierung funktionieren.\n",
        "# =============================================================================\n",
        "print('=' * 70)\n",
        "print('üì• TEST: Laden vom Driver (single-threaded)')\n",
        "print('=' * 70)\n",
        "\n",
        "# Test-Filter: ALLE ZooKeeper Metriken (ca. 1.663 Dokumente)\n",
        "TEST_PROCESS = 'zookeeper'\n",
        "\n",
        "# Solr Query - WICHTIG: project_name filtert auf Prometheus Daten in der ekg Collection\n",
        "TEST_FILTER = f'project_name:{PROJECT_NAME} AND ts_process:{TEST_PROCESS}'\n",
        "print(f'   Filter: {TEST_FILTER}')\n",
        "\n",
        "# Erst mal z√§hlen wie viele Dokumente es gibt\n",
        "test_docs, test_count = solr_query(TEST_FILTER, rows=0)\n",
        "print(f'   Gefundene Dokumente: {test_count}')\n",
        "\n",
        "if test_count > 0:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Alle Dokumente laden (vom Driver) - mit /select\n",
        "    fields = 'id,ts_hostGroup,ts_host,ts_process,ts_metricGroup,ts_metricName,ts_start,ts_end,ts_data_amountValues,ts_data'\n",
        "    url = f'http://{SOLR_HOST}:{SOLR_PORT}/solr/{COLLECTION}/select'\n",
        "    params = {\n",
        "        'q': TEST_FILTER,\n",
        "        'fl': fields,\n",
        "        'rows': test_count,\n",
        "        'wt': 'json'\n",
        "    }\n",
        "    \n",
        "    print(f'\\n   üîÑ Lade Daten von {url}...')\n",
        "    resp = requests.get(url, params=params, timeout=300)\n",
        "    resp.raise_for_status()\n",
        "    \n",
        "    data = resp.json()\n",
        "    test_docs_loaded = data.get('response', {}).get('docs', [])\n",
        "    load_time = time.time() - start_time\n",
        "    print(f'   ‚úÖ {len(test_docs_loaded)} Dokumente geladen in {load_time:.2f}s')\n",
        "    \n",
        "    # Debug: Zeige Struktur des ersten Dokuments\n",
        "    if test_docs_loaded:\n",
        "        print(f'\\n   üîç Debug - Felder im ersten Dokument:')\n",
        "        for key in test_docs_loaded[0].keys():\n",
        "            val = test_docs_loaded[0][key]\n",
        "            if key == 'ts_data':\n",
        "                print(f'      {key}: (BASE64, {len(str(val))} Zeichen)')\n",
        "            else:\n",
        "                print(f'      {key}: {val}')\n",
        "    \n",
        "    # Dekodieren\n",
        "    print(f'\\n   üîÑ Dekodiere Timeseries-Daten...')\n",
        "    decode_start = time.time()\n",
        "    \n",
        "    driver_results = []\n",
        "    total_datapoints = 0\n",
        "    \n",
        "    for doc in test_docs_loaded:\n",
        "        metadata = {\n",
        "            'id': doc.get('id'),\n",
        "            'hostGroup': doc.get('ts_hostGroup'),\n",
        "            'host': doc.get('ts_host'),\n",
        "            'process': doc.get('ts_process'),\n",
        "            'metricGroup': doc.get('ts_metricGroup'),\n",
        "            'metricName': doc.get('ts_metricName'),\n",
        "            'numValues': doc.get('ts_data_amountValues', 0)\n",
        "        }\n",
        "        \n",
        "        datapoints = decode_timeseries(doc.get('ts_data', ''))\n",
        "        total_datapoints += len(datapoints)\n",
        "        driver_results.append((metadata, datapoints))\n",
        "    \n",
        "    decode_time = time.time() - decode_start\n",
        "    total_time = time.time() - start_time\n",
        "    \n",
        "    print(f'   ‚úÖ {len(driver_results)} Zeitreihen dekodiert in {decode_time:.2f}s')\n",
        "    print(f'   ‚úÖ Gesamt: {total_datapoints:,} Datenpunkte in {total_time:.2f}s')\n",
        "    \n",
        "    # Stichprobe anzeigen\n",
        "    print('\\nüìÑ Stichprobe (erste 3 Zeitreihen):')\n",
        "    for metadata, datapoints in driver_results[:3]:\n",
        "        print(f'\\n   Host: {metadata.get(\"host\", \"N/A\")}')\n",
        "        print(f'   Prozess: {metadata.get(\"process\", \"N/A\")}')\n",
        "        print(f'   Metrik: {metadata.get(\"metricGroup\", \"N/A\")}/{metadata.get(\"metricName\", \"N/A\")}')\n",
        "        print(f'   Datenpunkte: {len(datapoints)}')\n",
        "        if datapoints:\n",
        "            first_ts, first_val = datapoints[0]\n",
        "            last_ts, last_val = datapoints[-1]\n",
        "            # decode_timeseries gibt (datetime, value) zur√ºck\n",
        "            print(f'   Erster Wert: {first_ts.strftime(\"%Y-%m-%d %H:%M\")} ‚Üí {first_val:.4f}')\n",
        "            print(f'   Letzter Wert: {last_ts.strftime(\"%Y-%m-%d %H:%M\")} ‚Üí {last_val:.4f}')\n",
        "else:\n",
        "    print('‚ö†Ô∏è  Keine Daten gefunden f√ºr diesen Filter!')\n",
        "    driver_results = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Teil 3b: Paralleles Laden desselben Datensatzes im Cluster\n",
        "\n",
        "Jetzt wiederholen wir das Laden **parallel von allen Shards** im Spark-Cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TEST: Gleicher Datensatz - jetzt PARALLEL im Cluster\n",
        "# =============================================================================\n",
        "print('=' * 70)\n",
        "print('üì• TEST: Paralleles Laden im Spark-Cluster')\n",
        "print('=' * 70)\n",
        "\n",
        "print(f'   Filter (gleich wie oben): {TEST_FILTER}')\n",
        "\n",
        "# Paralleles Laden mit der definierten Funktion\n",
        "prometheus_rdd = load_prometheus_parallel(spark, TEST_FILTER)\n",
        "\n",
        "if prometheus_rdd:\n",
        "    # Cache ist schon in load_prometheus_parallel() gesetzt\n",
        "    print('\\n‚úÖ Daten gecacht - alle folgenden Analysen nutzen diesen Cache!')\n",
        "    \n",
        "    # Vergleich mit Driver-Ergebnissen\n",
        "    cluster_count = prometheus_rdd.count()\n",
        "    print(f'\\nüìä Vergleich:')\n",
        "    print(f'   Driver:  {len(driver_results)} Zeitreihen')\n",
        "    print(f'   Cluster: {cluster_count} Zeitreihen')\n",
        "    \n",
        "    # Stichprobe anzeigen\n",
        "    print('\\nüìÑ Stichprobe (erste 3 Zeitreihen):')\n",
        "    for metadata, datapoints in prometheus_rdd.take(3):\n",
        "        print(f'\\n   Host: {metadata.get(\"host\", \"N/A\")}')\n",
        "        print(f'   Prozess: {metadata.get(\"process\", \"N/A\")}')\n",
        "        print(f'   Metrik: {metadata.get(\"metricGroup\", \"N/A\")}/{metadata.get(\"metricName\", \"N/A\")}')\n",
        "        print(f'   Datenpunkte: {len(datapoints)}')\n",
        "        if datapoints:\n",
        "            first_ts, first_val = datapoints[0]\n",
        "            last_ts, last_val = datapoints[-1]\n",
        "            # load_and_decode_shard gibt (timestamp_ms, value) zur√ºck\n",
        "            print(f'   Erster Wert: {datetime.fromtimestamp(first_ts/1000).strftime(\"%Y-%m-%d %H:%M\")} ‚Üí {first_val:.4f}')\n",
        "            print(f'   Letzter Wert: {datetime.fromtimestamp(last_ts/1000).strftime(\"%Y-%m-%d %H:%M\")} ‚Üí {last_val:.4f}')\n",
        "else:\n",
        "    print('‚ö†Ô∏è  Keine Daten geladen - nachfolgende Analysen werden nicht funktionieren!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Filter f√ºr weitere Analysen setzen\n",
        "# =============================================================================\n",
        "# Nach erfolgreichem Test kann hier ein gr√∂√üerer Filter gesetzt werden\n",
        "# =============================================================================\n",
        "print('=' * 70)\n",
        "print('üìù Aktueller Filter f√ºr alle Analysen')\n",
        "print('=' * 70)\n",
        "\n",
        "PROMETHEUS_FILTER = TEST_FILTER\n",
        "print(f'   Filter: {PROMETHEUS_FILTER}')\n",
        "print('\\n   ‚ÑπÔ∏è  Um mehr Daten zu laden, √§ndern Sie den Filter und f√ºhren Sie')\n",
        "print('      die vorherige Zelle erneut aus.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Teil 4: Auswertungen mit Spark\n",
        "\n",
        "Jetzt f√ºhren wir verschiedene Analysen auf den Prometheus-Daten durch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Auswertung 1: Statistiken pro Host und Metrik-Gruppe\n",
        "# =============================================================================\n",
        "# Verwendet das gecachte prometheus_rdd - KEIN erneutes Laden!\n",
        "# =============================================================================\n",
        "print('=' * 70)\n",
        "print('üìä AUSWERTUNG 1: √úbersicht pro Host und Metrik-Gruppe')\n",
        "print('=' * 70)\n",
        "\n",
        "if prometheus_rdd:\n",
        "    # Aggregation: (host, metricGroup) ‚Üí (count, total_datapoints)\n",
        "    stats_rdd = prometheus_rdd.map(lambda x: (\n",
        "        (x[0].get('host', 'N/A'), x[0].get('metricGroup', 'N/A')),\n",
        "        (1, len(x[1]))  # (1 Zeitreihe, Anzahl Datenpunkte)\n",
        "    )).reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
        "    \n",
        "    print('\\nüìä Statistik pro Host und Metrik-Gruppe:')\n",
        "    print(f'{\"Host\":<25} {\"Metrik-Gruppe\":<20} {\"Zeitreihen\":>12} {\"Datenpunkte\":>15}')\n",
        "    print('-' * 75)\n",
        "    \n",
        "    for (host, group), (series_count, datapoint_count) in sorted(stats_rdd.collect()):\n",
        "        print(f'{host:<25} {group:<20} {series_count:>12,} {datapoint_count:>15,}')\n",
        "else:\n",
        "    print('‚ö†Ô∏è  Keine Daten verf√ºgbar - bitte zuerst Zelle mit \"LADEN\" ausf√ºhren!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Auswertung 2: Zeitreihen-Statistiken (Min, Max, Avg, Stddev)\n",
        "# =============================================================================\n",
        "# Berechnet Statistiken f√ºr alle Metriken im gecachten RDD\n",
        "# =============================================================================\n",
        "print('=' * 70)\n",
        "print('üìä AUSWERTUNG 2: Statistiken pro Metrik')\n",
        "print('=' * 70)\n",
        "\n",
        "if prometheus_rdd:\n",
        "    import math\n",
        "    \n",
        "    def fmt_num(val):\n",
        "        \"\"\"Formatiert gro√üe Zahlen lesbar: 20542120681 ‚Üí '20.54 G'\"\"\"\n",
        "        abs_val = abs(val)\n",
        "        sign = '-' if val < 0 else ''\n",
        "        if abs_val >= 1e12:\n",
        "            return f'{sign}{abs_val/1e12:.2f} T'\n",
        "        elif abs_val >= 1e9:\n",
        "            return f'{sign}{abs_val/1e9:.2f} G'\n",
        "        elif abs_val >= 1e6:\n",
        "            return f'{sign}{abs_val/1e6:.2f} M'\n",
        "        elif abs_val >= 1e3:\n",
        "            return f'{sign}{abs_val/1e3:.2f} K'\n",
        "        elif abs_val >= 1 or abs_val == 0:\n",
        "            return f'{val:.2f}'\n",
        "        else:\n",
        "            return f'{val:.4f}'\n",
        "    \n",
        "    def compute_stats(item):\n",
        "        \"\"\"Berechnet Statistiken f√ºr eine Zeitreihe.\"\"\"\n",
        "        metadata, datapoints = item\n",
        "        \n",
        "        if not datapoints:\n",
        "            return None\n",
        "        \n",
        "        values = [dp[1] for dp in datapoints]\n",
        "        n = len(values)\n",
        "        \n",
        "        min_val = min(values)\n",
        "        max_val = max(values)\n",
        "        avg_val = sum(values) / n\n",
        "        \n",
        "        # Standardabweichung\n",
        "        variance = sum((v - avg_val) ** 2 for v in values) / n\n",
        "        stddev = math.sqrt(variance)\n",
        "        \n",
        "        return (\n",
        "            metadata.get('host', 'N/A'),\n",
        "            metadata.get('process', 'N/A'),\n",
        "            metadata.get('metricGroup', 'N/A'),\n",
        "            metadata.get('metricName', 'N/A'),\n",
        "            n,\n",
        "            min_val,\n",
        "            max_val,\n",
        "            avg_val,\n",
        "            stddev\n",
        "        )\n",
        "    \n",
        "    stats = prometheus_rdd.map(compute_stats).filter(lambda x: x is not None).collect()\n",
        "    \n",
        "    print(f'\\n{\"Host\":<20} {\"Prozess\":<15} {\"Gruppe\":<12} {\"Metrik\":<25} {\"N\":>6} {\"Min\":>12} {\"Max\":>12} {\"Avg\":>12}')\n",
        "    print('-' * 125)\n",
        "    \n",
        "    for host, process, group, metric, n, min_v, max_v, avg_v, std_v in sorted(stats)[:30]:\n",
        "        host_short = host.replace('.cloud.local', '')\n",
        "        proc_short = (process[:12] + '...') if len(process) > 15 else process\n",
        "        print(f'{host_short:<20} {proc_short:<15} {group:<12} {metric:<25} {n:>6,} {fmt_num(min_v):>12} {fmt_num(max_v):>12} {fmt_num(avg_v):>12}')\n",
        "    \n",
        "    if len(stats) > 30:\n",
        "        print(f'\\n   ... und {len(stats) - 30} weitere Zeitreihen')\n",
        "else:\n",
        "    print('‚ö†Ô∏è  Keine Daten verf√ºgbar - bitte zuerst Zelle mit \"LADEN\" ausf√ºhren!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Auswertung 3: Zeitreihen zu DataFrame (f√ºr komplexere Analysen)\n",
        "# =============================================================================\n",
        "# Konvertiert das gecachte RDD in einen DataFrame f√ºr SQL-Analysen\n",
        "# =============================================================================\n",
        "print('=' * 70)\n",
        "print('üìä AUSWERTUNG 3: Zeitreihen als Spark DataFrame')\n",
        "print('=' * 70)\n",
        "\n",
        "def flatten_timeseries(item):\n",
        "    \"\"\"Flacht Zeitreihe zu einzelnen Datenpunkten ab.\"\"\"\n",
        "    metadata, datapoints = item\n",
        "    \n",
        "    for ts_ms, value in datapoints:\n",
        "        yield {\n",
        "            'host': metadata.get('host', 'N/A'),\n",
        "            'process': metadata.get('process', 'N/A'),\n",
        "            'metricGroup': metadata.get('metricGroup', 'N/A'),\n",
        "            'metricName': metadata.get('metricName', 'N/A'),\n",
        "            'timestamp_ms': ts_ms,\n",
        "            'value': value\n",
        "        }\n",
        "\n",
        "if prometheus_rdd:\n",
        "    # Flatten zu einzelnen Datenpunkten\n",
        "    flat_rdd = prometheus_rdd.flatMap(flatten_timeseries)\n",
        "    \n",
        "    # Schema definieren\n",
        "    schema = StructType([\n",
        "        StructField('host', StringType(), True),\n",
        "        StructField('process', StringType(), True),\n",
        "        StructField('metricGroup', StringType(), True),\n",
        "        StructField('metricName', StringType(), True),\n",
        "        StructField('timestamp_ms', LongType(), True),\n",
        "        StructField('value', DoubleType(), True)\n",
        "    ])\n",
        "    \n",
        "    # DataFrame erstellen (NaN/Inf-Werte wurden bereits beim Laden gefiltert)\n",
        "    df = spark.createDataFrame(flat_rdd, schema)\n",
        "    \n",
        "    # Timestamp konvertieren\n",
        "    df = df.withColumn('timestamp', (F.col('timestamp_ms') / 1000).cast('timestamp'))\n",
        "    \n",
        "    # Cache f√ºr mehrfache Verwendung in folgenden Analysen\n",
        "    df.cache()\n",
        "    \n",
        "    df_count = df.count()\n",
        "    print(f'\\n‚úÖ DataFrame erstellt und gecacht: {df_count:,} Datenpunkte')\n",
        "    print(f'   ‚ÑπÔ∏è  NaN/Inf-Werte wurden bereits beim Laden gefiltert')\n",
        "    df.printSchema()\n",
        "    \n",
        "    # Stichprobe\n",
        "    if df_count > 0:\n",
        "        print('\\nüìä Stichprobe:')\n",
        "        df.select('host', 'metricGroup', 'metricName', 'timestamp', 'value').show(10, truncate=False)\n",
        "else:\n",
        "    print('‚ö†Ô∏è  Keine Daten verf√ºgbar - bitte zuerst Zelle mit \"LADEN\" ausf√ºhren!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Auswertung 4: Aggregation pro Stunde\n",
        "# =============================================================================\n",
        "# Verwendet den gecachten DataFrame 'df' - KEIN erneutes Laden!\n",
        "# =============================================================================\n",
        "print('=' * 70)\n",
        "print('üìä AUSWERTUNG 4: Durchschnittswerte pro Stunde')\n",
        "print('=' * 70)\n",
        "\n",
        "if 'df' in dir() and df is not None:\n",
        "    # Stunde extrahieren und aggregieren\n",
        "    hourly_stats = df.withColumn('hour', F.hour('timestamp')) \\\n",
        "        .groupBy('host', 'metricGroup', 'hour') \\\n",
        "        .agg(\n",
        "            F.count('*').alias('count'),\n",
        "            F.avg('value').alias('avg_value'),\n",
        "            F.min('value').alias('min_value'),\n",
        "            F.max('value').alias('max_value')\n",
        "        ) \\\n",
        "        .orderBy('host', 'metricGroup', 'hour')\n",
        "    \n",
        "    print('\\nüìä St√ºndliche Statistik pro Host und Metrik-Gruppe:')\n",
        "    hourly_stats.show(30, truncate=False)\n",
        "else:\n",
        "    print('‚ö†Ô∏è  Kein DataFrame verf√ºgbar - bitte zuerst Auswertung 3 ausf√ºhren!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Auswertung 5: Vergleich zwischen Hosts\n",
        "# =============================================================================\n",
        "# Verwendet den gecachten DataFrame 'df' - KEIN erneutes Laden!\n",
        "# =============================================================================\n",
        "print('=' * 70)\n",
        "print('üìä AUSWERTUNG 5: Host-Vergleich')\n",
        "print('=' * 70)\n",
        "\n",
        "if 'df' in dir() and df is not None:\n",
        "    host_comparison = df.groupBy('host', 'metricGroup') \\\n",
        "        .agg(\n",
        "            F.count('*').alias('datapoints'),\n",
        "            F.avg('value').alias('avg'),\n",
        "            F.stddev('value').alias('stddev'),\n",
        "            F.percentile_approx('value', 0.5).alias('median'),\n",
        "            F.percentile_approx('value', 0.95).alias('p95'),\n",
        "            F.percentile_approx('value', 0.99).alias('p99')\n",
        "        ) \\\n",
        "        .orderBy('metricGroup', 'host')\n",
        "    \n",
        "    print('\\nüìä Host-Vergleich (Solr-Metriken):')\n",
        "    host_comparison.show(50, truncate=False)\n",
        "else:\n",
        "    print('‚ö†Ô∏è  Kein DataFrame verf√ºgbar - bitte zuerst Auswertung 3 ausf√ºhren!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Auswertung 6: Anomalie-Erkennung (einfach: Werte > 3 Standardabweichungen)\n",
        "# =============================================================================\n",
        "# Verwendet den gecachten DataFrame 'df' - KEIN erneutes Laden!\n",
        "# =============================================================================\n",
        "print('=' * 70)\n",
        "print('üìä AUSWERTUNG 6: Anomalie-Erkennung')\n",
        "print('=' * 70)\n",
        "\n",
        "if 'df' in dir() and df is not None:\n",
        "    from pyspark.sql import Window\n",
        "    \n",
        "    # Statistiken pro Metrik berechnen\n",
        "    stats_window = Window.partitionBy('host', 'metricGroup', 'metricName')\n",
        "    \n",
        "    df_with_stats = df.withColumn('mean', F.avg('value').over(stats_window)) \\\n",
        "                      .withColumn('stddev', F.stddev('value').over(stats_window))\n",
        "    \n",
        "    # Z-Score berechnen\n",
        "    df_with_zscore = df_with_stats.withColumn(\n",
        "        'z_score',\n",
        "        F.when(F.col('stddev') > 0, \n",
        "               F.abs(F.col('value') - F.col('mean')) / F.col('stddev'))\n",
        "         .otherwise(0)\n",
        "    )\n",
        "    \n",
        "    # Anomalien finden (Z-Score > 3)\n",
        "    anomalies = df_with_zscore.filter(F.col('z_score') > 3) \\\n",
        "        .select('host', 'metricGroup', 'metricName', 'timestamp', 'value', 'mean', 'z_score') \\\n",
        "        .orderBy(F.desc('z_score'))\n",
        "    \n",
        "    anomaly_count = anomalies.count()\n",
        "    print(f'\\n‚ö†Ô∏è  Gefundene Anomalien (Z-Score > 3): {anomaly_count}')\n",
        "    \n",
        "    if anomaly_count > 0:\n",
        "        print('\\nüìä Top 20 Anomalien:')\n",
        "        anomalies.show(20, truncate=False)\n",
        "else:\n",
        "    print('‚ö†Ô∏è  Kein DataFrame verf√ºgbar - bitte zuerst Auswertung 3 ausf√ºhren!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Visualisierung: Matplotlib\n",
        "# =============================================================================\n",
        "# Verwendet den gecachten DataFrame 'df' - KEIN erneutes Laden!\n",
        "# =============================================================================\n",
        "print('=' * 70)\n",
        "print('üìä VISUALISIERUNG: Metrik √ºber Zeit')\n",
        "print('=' * 70)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "if 'df' in dir() and df is not None:\n",
        "    # Zeige verf√ºgbare Metriken\n",
        "    print('\\nüìä Verf√ºgbare Metriken (Top 10 nach Datenpunkten):')\n",
        "    metrics = df.groupBy('metricGroup', 'metricName') \\\n",
        "        .count() \\\n",
        "        .orderBy(F.desc('count')) \\\n",
        "        .limit(10) \\\n",
        "        .collect()\n",
        "    \n",
        "    for i, m in enumerate(metrics):\n",
        "        print(f'   {i+1}. {m.metricGroup} / {m.metricName} ({m[\"count\"]:,} Punkte)')\n",
        "    \n",
        "    # Bevorzugt interessante Metriken plotten (mit Variation)\n",
        "    # Fallback: erste verf√ºgbare Metrik\n",
        "    if metrics:\n",
        "        # Suche nach interessanten Prometheus-Metriken (mit Variation)\n",
        "        plot_group = None\n",
        "        plot_metric = None\n",
        "        \n",
        "        # Priorisierte Metriknamen (zeigen typischerweise Variation)\n",
        "        preferred_patterns = [\n",
        "            'cpu_seconds_total',             # CPU Nutzung (steigt kontinuierlich)\n",
        "            'requests_total',                # Request Counter\n",
        "            'jvm_memory_pools_bytes',        # JVM Memory Pools\n",
        "            'query_median_ms',               # Query Latenz\n",
        "            'searcher_documents',            # Dokument-Anzahl\n",
        "        ]\n",
        "        \n",
        "        for pattern in preferred_patterns:\n",
        "            for m in metrics:\n",
        "                if pattern in m.metricName:\n",
        "                    plot_group = m.metricGroup\n",
        "                    plot_metric = m.metricName\n",
        "                    break\n",
        "            if plot_metric:\n",
        "                break\n",
        "        \n",
        "        # Fallback: erste Metrik\n",
        "        if not plot_metric:\n",
        "            plot_group = metrics[0].metricGroup\n",
        "            plot_metric = metrics[0].metricName\n",
        "        \n",
        "        print(f'\\nüìà Plotte: {plot_group} / {plot_metric}')\n",
        "        \n",
        "        # Daten f√ºr genau diese Metrik sammeln (alle Prozesse)\n",
        "        plot_data = df.filter(\n",
        "            (F.col('metricGroup') == plot_group) & \n",
        "            (F.col('metricName') == plot_metric)\n",
        "        ) \\\n",
        "            .select('host', 'process', 'timestamp', 'value') \\\n",
        "            .orderBy('timestamp') \\\n",
        "            .toPandas()\n",
        "        \n",
        "        if not plot_data.empty:\n",
        "            # Gruppiere nach Prozess f√ºr separate Linien\n",
        "            fig, ax = plt.subplots(figsize=(14, 6))\n",
        "            \n",
        "            # Wenn viele Prozesse, nur die ersten 5 zeigen\n",
        "            processes = plot_data['process'].unique()[:5]\n",
        "            \n",
        "            for proc in processes:\n",
        "                proc_data = plot_data[plot_data['process'] == proc].sort_values('timestamp')\n",
        "                label = proc[:20] + '...' if len(proc) > 20 else proc\n",
        "                ax.plot(proc_data['timestamp'], proc_data['value'], \n",
        "                       label=label, alpha=0.7, linewidth=1)\n",
        "            \n",
        "            ax.set_xlabel('Zeit')\n",
        "            ax.set_ylabel('Wert')\n",
        "            ax.set_title(f'{plot_group} / {plot_metric}')\n",
        "            ax.legend(loc='upper right', fontsize='small')\n",
        "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))\n",
        "            ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "            print(f'\\n   üìä {len(plot_data):,} Datenpunkte geplottet')\n",
        "            print(f'   üìä Zeitraum: {plot_data[\"timestamp\"].min()} bis {plot_data[\"timestamp\"].max()}')\n",
        "        else:\n",
        "            print('‚ö†Ô∏è  Keine Daten f√ºr Plot verf√ºgbar')\n",
        "else:\n",
        "    print('‚ö†Ô∏è  Kein DataFrame verf√ºgbar - bitte zuerst Auswertung 3 ausf√ºhren!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Aufr√§umen\n",
        "# =============================================================================\n",
        "print('=' * 70)\n",
        "print('üßπ Session beenden')\n",
        "print('=' * 70)\n",
        "\n",
        "# Cache-Info anzeigen\n",
        "print('\\nüìä Cache-Status:')\n",
        "if 'prometheus_rdd' in dir() and prometheus_rdd is not None:\n",
        "    print(f'   prometheus_rdd: gecacht (Solr-Metriken)')\n",
        "if 'df' in dir() and df is not None:\n",
        "    print(f'   df (DataFrame): gecacht')\n",
        "\n",
        "# Optional: Caches freigeben\n",
        "# prometheus_rdd.unpersist()\n",
        "# df.unpersist()\n",
        "\n",
        "# Optional: Spark Session stoppen\n",
        "# spark.stop()\n",
        "# print('‚úÖ Spark Session beendet')\n",
        "\n",
        "print('\\n‚úÖ Notebook abgeschlossen!')\n",
        "print('\\nüìù Zusammenfassung:')\n",
        "print(f'   - Filter: {PROMETHEUS_FILTER}')\n",
        "print('   - Daten EINMALIG parallel von allen Solr Shards via /export geladen')\n",
        "print('   - BASE64-kodierte Timeseries (String-Feld mit docValues) dekodiert')\n",
        "print('   - Daten gecacht (prometheus_rdd, df)')\n",
        "print('   - Alle Analysen nutzen gecachte Daten (kein Neuladen!)')\n",
        "print('   - Durchgef√ºhrte Auswertungen:')\n",
        "print('     ‚Ä¢ Statistiken pro Host/Metrik-Gruppe')\n",
        "print('     ‚Ä¢ Zeitreihen-Statistiken (Min, Max, Avg, StdDev)')\n",
        "print('     ‚Ä¢ DataFrame f√ºr komplexere SQL-Analysen')\n",
        "print('     ‚Ä¢ St√ºndliche Aggregation')\n",
        "print('     ‚Ä¢ Host-Vergleich mit Perzentilen')\n",
        "print('     ‚Ä¢ Anomalie-Erkennung (Z-Score)')\n",
        "print('     ‚Ä¢ Visualisierung')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
